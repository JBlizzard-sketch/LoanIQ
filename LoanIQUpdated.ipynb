{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnyN3Xnz/nkxbGThAG6Tt2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JBlizzard-sketch/LoanIQ/blob/main/LoanIQUpdated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OppiOadESUL7",
        "outputId": "c613f0eb-55b9-4cb4-a946-8af6ec0fe3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 1.26.4\n",
            "sklearn: 1.3.2\n",
            "xgboost: 1.7.6\n",
            "shap: 0.41.0\n",
            "pandas: 2.3.2\n",
            "streamlit: 1.49.1\n",
            "pyngrok: None\n",
            "imbalanced_learn: IMPORT ERROR -> No module named 'imbalanced_learn'\n",
            "faker: None\n",
            "✅ Dependencies pinned. ✅ Faker included. ✅ Repo folders created. ✅ Ngrok token set.\n"
          ]
        }
      ],
      "source": [
        "# -------- Cell 1: Dependencies & Minimal Setup (patched with Faker) --------\n",
        "# Installs pinned versions and includes faker (safe: no reinstalls if already present)\n",
        "\n",
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "python -m pip install --quiet --upgrade \\\n",
        "  \"numpy==1.26.4\" \\\n",
        "  \"scikit-learn==1.3.2\" \\\n",
        "  \"xgboost==1.7.6\" \\\n",
        "  \"shap==0.41.0\" \\\n",
        "  \"pyngrok==5.2.1\" \\\n",
        "  \"streamlit\" \\\n",
        "  \"pandas\" \\\n",
        "  \"imbalanced-learn\" \\\n",
        "  \"faker\" \\\n",
        "  \"joblib\" \\\n",
        "  \"sqlalchemy\" \\\n",
        "  \"openpyxl\" \\\n",
        "  \"python-dotenv\" \\\n",
        "  || true\n",
        "\n",
        "# --- Create persistent folder structure ---\n",
        "mkdir -p /content/loan_app/modules/synth \\\n",
        "         /content/loan_app/modules/ml \\\n",
        "         /content/loan_app/modules/streamlit_app \\\n",
        "         /content/loan_app/data \\\n",
        "         /content/loan_app/models \\\n",
        "         /content/loan_app/logs\n",
        "\n",
        "# --- Write ngrok authtoken ---\n",
        "NGROK_TOKEN=\"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\"\n",
        "mkdir -p ~/.ngrok2\n",
        "cat > ~/.ngrok2/ngrok.yml <<NGY\n",
        "authtoken: ${NGROK_TOKEN}\n",
        "NGY\n",
        "export NGROK_AUTHTOKEN=\"${NGROK_TOKEN}\"\n",
        "\n",
        "# --- Quick version check ---\n",
        "python - <<'PY'\n",
        "import importlib\n",
        "pkgs = [\"numpy\",\"sklearn\",\"xgboost\",\"shap\",\"pandas\",\"streamlit\",\"pyngrok\",\"imbalanced_learn\",\"faker\"]\n",
        "for p in pkgs:\n",
        "    try:\n",
        "        m = importlib.import_module(p)\n",
        "        v = getattr(m, \"__version__\", None)\n",
        "        print(f\"{p}: {v}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{p}: IMPORT ERROR -> {e}\")\n",
        "PY\n",
        "\n",
        "echo \"✅ Dependencies pinned. ✅ Faker included. ✅ Repo folders created. ✅ Ngrok token set.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------- Cell 2/9 (CLEAN & COMPRESSED): Write Repo Modules --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "MODULES = BASE / \"modules\"\n",
        "SYNTH_DIR = MODULES / \"synth\"\n",
        "ML_DIR = MODULES / \"ml\"\n",
        "STREAMLIT_DIR = MODULES / \"streamlit_app\"\n",
        "DATA_DIR, MODELS_DIR, LOGS_DIR = BASE/\"data\", BASE/\"models\", BASE/\"logs\"\n",
        "\n",
        "# Create folders\n",
        "for d in [BASE, MODULES, SYNTH_DIR, ML_DIR, STREAMLIT_DIR, DATA_DIR, MODELS_DIR, LOGS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "if str(BASE) not in sys.path:\n",
        "    sys.path.insert(0, str(BASE))\n",
        "\n",
        "def write_module(path: Path, code: str):\n",
        "    path.write_text(code.strip()+\"\\n\")\n",
        "    # reopen sanity check\n",
        "    with open(path) as f: f.read()\n",
        "\n",
        "# -------- 1) auth.py --------\n",
        "auth_py = r\"\"\"\n",
        "import sqlite3, hashlib, datetime, os\n",
        "DB_PATH = os.path.join('/content/loan_app','data','users.db')\n",
        "\n",
        "def init_db():\n",
        "    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('CREATE TABLE IF NOT EXISTS users (username TEXT PRIMARY KEY, password TEXT, role TEXT, created_at TEXT)')\n",
        "    conn.commit(); conn.close()\n",
        "    register_user('Admin','Shady868','admin')\n",
        "\n",
        "def hash_pw(pw): return hashlib.sha256(pw.encode()).hexdigest()\n",
        "\n",
        "def register_user(username, password, role='company'):\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('INSERT OR IGNORE INTO users VALUES (?,?,?,?)',\n",
        "        (username, hash_pw(password), role, datetime.datetime.utcnow().isoformat()))\n",
        "    conn.commit(); conn.close()\n",
        "\n",
        "def authenticate(username, password):\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('SELECT password, role FROM users WHERE username=?',(username,))\n",
        "    row = cur.fetchone(); conn.close()\n",
        "    return (row and row[0]==hash_pw(password), row[1] if row else None)\n",
        "\n",
        "def list_users():\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('SELECT username, role, created_at FROM users')\n",
        "    rows = cur.fetchall(); conn.close(); return rows\n",
        "\"\"\"\n",
        "write_module(MODULES/\"auth.py\", auth_py)\n",
        "\n",
        "# -------- 2) schema.py (compressed but rich) --------\n",
        "schema_py = r\"\"\"\n",
        "import re, pandas as pd, numpy as np, random\n",
        "from datetime import datetime\n",
        "\n",
        "SYNONYMS = {\n",
        "  \"client_id\":[\"customer_id\",\"cust_id\",\"id\"],\n",
        "  \"national_id\":[\"id_no\",\"idnumber\",\"reg_no\",\"reg_number\"],\n",
        "  \"loan_amount\":[\"amount\",\"principal\"],\n",
        "  \"branch\":[\"office\",\"location\"],\n",
        "  \"product\":[\"loan_product\"],\n",
        "  \"loan_status\":[\"status\",\"performance\",\"default\",\"outcome\"],\n",
        "  \"created_date\":[\"created_at\",\"disbursed_date\",\"date\"],\n",
        "  \"income\":[\"monthly_income\",\"salary\"],\n",
        "  \"name\":[\"customer_name\",\"full_name\"]\n",
        "}\n",
        "MALE, FEMALE = {\"John\",\"Peter\",\"James\",\"Joseph\",\"Michael\",\"David\"}, {\"Mary\",\"Ann\",\"Jane\",\"Grace\",\"Lucy\",\"Sarah\"}\n",
        "TERM_OVERRIDES = {\"inuka\":4,\"fadhili\":6,\"kuza\":12,\"agriadvance\":16,\"flexiloan\":8,\"bizboost\":10}\n",
        "\n",
        "def normalize_columns(df):\n",
        "    df=df.copy()\n",
        "    df.columns=[re.sub(r'[^\\w]','_',c.strip().lower()) for c in df.columns]\n",
        "    for canon,vars in SYNONYMS.items():\n",
        "        for v in vars:\n",
        "            if v in df.columns and canon not in df.columns: df.rename(columns={v:canon},inplace=True)\n",
        "    return df\n",
        "\n",
        "def parse_money(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    if isinstance(x,(int,float)): return float(x)\n",
        "    s=str(x).lower().replace('kes','').replace('ksh','').replace(',','').strip()\n",
        "    if s.endswith('k'): return float(s[:-1])*1000\n",
        "    try: return float(re.sub(r'[^\\d.]','',s))\n",
        "    except: return np.nan\n",
        "\n",
        "def parse_date(x):\n",
        "    if pd.isna(x): return None\n",
        "    s=str(x).strip()\n",
        "    for fmt in (\"%Y-%m-%d\",\"%d/%m/%Y\",\"%m/%d/%Y\"):\n",
        "        try: return pd.to_datetime(datetime.strptime(s,fmt))\n",
        "        except: pass\n",
        "    return pd.to_datetime(s, errors='coerce')\n",
        "\n",
        "def estimate_age_from_id(nid):\n",
        "    try:\n",
        "        digits=int(str(nid)[:2])\n",
        "        if 32<=digits<=34: return random.randint(27,30)\n",
        "        if 29<=digits<=31: return random.randint(30,40)\n",
        "        if 35<=digits<=38: return random.randint(18,25)\n",
        "        return random.randint(25,50)\n",
        "    except: return random.randint(25,50)\n",
        "\n",
        "def gender_from_name(name):\n",
        "    if not isinstance(name,str): return 'unknown'\n",
        "    f=name.split()[0].capitalize()\n",
        "    if f in MALE: return 'male'\n",
        "    if f in FEMALE: return 'female'\n",
        "    return 'female' if f.endswith(('a','e')) else 'male'\n",
        "\n",
        "def term_weeks(prod):\n",
        "    if not isinstance(prod,str): return None\n",
        "    m=re.search(r'(\\d+)\\s*week',prod.lower())\n",
        "    if m: return int(m.group(1))\n",
        "    for k,v in TERM_OVERRIDES.items():\n",
        "        if k in prod.lower(): return v\n",
        "    return None\n",
        "\n",
        "def coerce_and_enrich(df):\n",
        "    df=normalize_columns(df)\n",
        "    if \"national_id\" in df: df[\"unique_client_id\"]=df[\"national_id\"].astype(str)\n",
        "    elif \"client_id\" in df: df[\"unique_client_id\"]=df[\"client_id\"].astype(str)\n",
        "    else: df[\"unique_client_id\"]=df.get(\"name\",\"\").astype(str)\n",
        "    if \"created_date\" in df: df[\"created_date_parsed\"]=df[\"created_date\"].apply(parse_date)\n",
        "    else: df[\"created_date_parsed\"]=pd.to_datetime(\"today\")\n",
        "    df[\"loan_amount_num\"]=df.get(\"loan_amount\",np.nan).apply(parse_money)\n",
        "    df[\"income_num\"]=df.get(\"income\",np.nan).apply(parse_money)\n",
        "    df[\"product_term_weeks\"]=df.get(\"product\").apply(term_weeks) if \"product\" in df else None\n",
        "    df[\"installment_size\"]=df.apply(lambda r:(r[\"loan_amount_num\"]/r[\"product_term_weeks\"]) if pd.notna(r[\"loan_amount_num\"]) and r.get(\"product_term_weeks\") else np.nan,axis=1)\n",
        "    df[\"gender_est\"]=df.get(\"name\",\"\").apply(gender_from_name) if \"name\" in df else \"unknown\"\n",
        "    df[\"age_est\"]=df.get(\"national_id\").apply(estimate_age_from_id) if \"national_id\" in df else None\n",
        "    df[\"loan_to_income\"]=df.apply(lambda r:(r[\"loan_amount_num\"]/r[\"income_num\"]) if r.get(\"income_num\",0)>0 else np.nan,axis=1)\n",
        "    df[\"is_young_borrower\"]=df[\"age_est\"].apply(lambda x: x<25 if pd.notna(x) else False)\n",
        "    df[\"high_risk_amount\"]=df[\"loan_amount_num\"].apply(lambda x: x>300000 if pd.notna(x) else False)\n",
        "    if \"loan_status\" in df:\n",
        "        df[\"is_default\"]=df[\"loan_status\"].astype(str).str.lower().str.contains(\"default\").astype(int)\n",
        "    else: df[\"is_default\"]=0\n",
        "    df[\"days_since_issue\"]=(pd.to_datetime(\"today\")-pd.to_datetime(df[\"created_date_parsed\"],errors=\"coerce\")).dt.days.fillna(0).astype(int)\n",
        "    return df\n",
        "\n",
        "def prepare_for_ml(df, target=\"is_default\", aggregate_by_client=False):\n",
        "    df_en=coerce_and_enrich(df)\n",
        "    num=[\"loan_amount_num\",\"income_num\",\"product_term_weeks\",\"installment_size\",\"loan_to_income\",\"age_est\",\"days_since_issue\"]\n",
        "    cat=[c for c in [\"branch\",\"product\",\"town\",\"gender_est\"] if c in df_en.columns]\n",
        "    X=df_en[num+cat].copy(); X[num]=X[num].fillna(0)\n",
        "    if cat: X=pd.get_dummies(X,columns=cat,drop_first=True)\n",
        "    y=df_en[target].astype(int)\n",
        "    if aggregate_by_client:\n",
        "        agg=X.groupby(df_en[\"unique_client_id\"]).agg([\"mean\",\"max\",\"min\",\"sum\",\"count\"])\n",
        "        agg.columns=[\"__\".join(col) for col in agg.columns]\n",
        "        y=y.groupby(df_en[\"unique_client_id\"]).max()\n",
        "        return agg,y,df_en\n",
        "    return X,y,df_en\n",
        "\"\"\"\n",
        "write_module(MODULES/\"schema.py\", schema_py)\n",
        "\n",
        "# -------- 3) synth/generator.py --------\n",
        "synth_py = r\"\"\"\n",
        "import random, pandas as pd\n",
        "from faker import Faker\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "faker=Faker(); KENYAN_TOWNS=[\"Nairobi\",\"Mombasa\",\"Kisumu\",\"Nakuru\",\"Eldoret\"]\n",
        "BRANCHES=[\"Nairobi Branch\",\"Mombasa Branch\",\"Kisumu Branch\",\"Nakuru Branch\"]\n",
        "PRODUCTS=[\"Inuka 4 weeks\",\"Fadhili 6 weeks\",\"Kuza 12 weeks\",\"AgriAdvance 16 weeks\",\"FlexiLoan 8 weeks\",\"BizBoost 10 weeks\"]\n",
        "\n",
        "def generate_id(prefix=None):\n",
        "    return str(prefix)+str(random.randint(0,999999))[:8] if prefix else str(random.randint(1000000,99999999))\n",
        "\n",
        "def generate_phone(): return \"+2547\"+str(random.randint(1000000,9999999))\n",
        "\n",
        "def make_dataset(n=2000, default_rate=0.25, seed=42, include_history=True):\n",
        "    random.seed(seed); rows=[]\n",
        "    for i in range(n):\n",
        "        name=faker.first_name()+\" \"+faker.last_name()\n",
        "        nid=generate_id(random.choice([32,33,34,35])) if random.random()<0.5 else generate_id()\n",
        "        branch,prod=random.choice(BRANCHES),random.choice(PRODUCTS)\n",
        "        income=random.choice([15000,20000,30000,50000,80000,120000,200000])\n",
        "        amount=random.randint(2000,600000)\n",
        "        created=datetime.utcnow()-timedelta(days=random.randint(0,730))\n",
        "        status=\"default\" if random.random()<default_rate else \"performing\"\n",
        "        rows.append(dict(client_id=i,name=name,national_id=nid,phone=generate_phone(),town=random.choice(KENYAN_TOWNS),\n",
        "                         branch=branch,product=prod,income=income,loan_amount=amount,loan_status=status,\n",
        "                         created_date=created.strftime(\"%Y-%m-%d\")))\n",
        "    return pd.DataFrame(rows)\n",
        "\"\"\"\n",
        "write_module(SYNTH_DIR/\"generator.py\", synth_py)\n",
        "\n",
        "# -------- 4) ml/engine.py --------\n",
        "ml_engine_py = r\"\"\"\n",
        "import os,json,joblib,hashlib,time\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "MODELS_DIR=\"/content/loan_app/models\"\n",
        "def _fingerprint(cols): return hashlib.sha1(\"|\".join(sorted(cols)).encode()).hexdigest()[:12]\n",
        "\n",
        "class HybridModel:\n",
        "    def __init__(self,models): self.models=models\n",
        "    def predict_proba(self,X):\n",
        "        arr=X.values if hasattr(X,\"values\") else np.asarray(X)\n",
        "        probs=[]\n",
        "        for m in self.models:\n",
        "            try: p=m.predict_proba(arr)[:,1]\n",
        "            except:\n",
        "                try: p=1/(1+np.exp(-m.decision_function(arr)))\n",
        "                except: p=np.zeros(arr.shape[0])\n",
        "            probs.append(p)\n",
        "        avg=np.mean(np.vstack(probs),axis=0)\n",
        "        return np.vstack([1-avg,avg]).T\n",
        "    def predict(self,X): return (self.predict_proba(X)[:,1]>0.5).astype(int)\n",
        "\n",
        "def train_baseline(X,y,name=\"baseline\",random_state=42):\n",
        "    Xt,Xv,yt,yv=train_test_split(X,y,test_size=0.25,random_state=random_state,stratify=y if len(set(y))>1 else None)\n",
        "    try: Xb,yb=SMOTE(random_state=random_state).fit_resample(Xt,yt)\n",
        "    except: Xb,yb=Xt,yt\n",
        "    lr=LogisticRegression(max_iter=1000)\n",
        "    sgd=CalibratedClassifierCV(SGDClassifier(max_iter=2000),cv=3)\n",
        "    xgb=XGBClassifier(use_label_encoder=False,eval_metric=\"logloss\",verbosity=0)\n",
        "    for m in [lr,sgd,xgb]:\n",
        "        try: m.fit(Xb,yb)\n",
        "        except: pass\n",
        "    hybrid=HybridModel([lr,sgd,xgb])\n",
        "    try: auc=float(roc_auc_score(yv,hybrid.predict_proba(Xv)[:,1]))\n",
        "    except: auc=0.0\n",
        "    meta={\"name\":name,\"created_at\":time.time(),\"features\":list(X.columns),\"fingerprint\":_fingerprint(X.columns),\"auc\":auc}\n",
        "    os.makedirs(MODELS_DIR,exist_ok=True)\n",
        "    joblib.dump(hybrid,f\"{MODELS_DIR}/{name}.pkl\")\n",
        "    json.dump(meta,open(f\"{MODELS_DIR}/{name}.meta.json\",\"w\"))\n",
        "    return {\"auc\":auc,\"model_path\":f\"{MODELS_DIR}/{name}.pkl\"}\n",
        "\"\"\"\n",
        "write_module(ML_DIR/\"engine.py\", ml_engine_py)\n",
        "\n",
        "# -------- 5) ml/utils.py --------\n",
        "ml_utils_py = r\"\"\"\n",
        "import shap, numpy as np\n",
        "from sklearn.metrics import roc_auc_score,accuracy_score,precision_recall_curve,auc\n",
        "def evaluate(model,X,y):\n",
        "    try:\n",
        "        p=model.predict_proba(X)[:,1]\n",
        "        prec,rec,_=precision_recall_curve(y,p)\n",
        "        return {\"auc\":float(roc_auc_score(y,p)),\"acc\":float(accuracy_score(y,(p>0.5).astype(int))),\n",
        "                \"pr_auc\":float(auc(rec,prec))}\n",
        "    except Exception as e: return {\"error\":str(e)}\n",
        "def explain(model,X,n=200):\n",
        "    try:\n",
        "        Xs=X.sample(min(n,len(X)))\n",
        "        base=model.models[2] if hasattr(model,\"models\") else model\n",
        "        return shap.Explainer(base,Xs)(Xs)\n",
        "    except Exception as e: return {\"error\":str(e)}\n",
        "\"\"\"\n",
        "write_module(ML_DIR/\"utils.py\", ml_utils_py)\n",
        "\n",
        "# -------- 6) ml/audit.py --------\n",
        "ml_audit_py = r\"\"\"\n",
        "import sqlite3,os,datetime\n",
        "DB_PATH=\"/content/loan_app/data/audit.db\"\n",
        "def init_audit():\n",
        "    os.makedirs(os.path.dirname(DB_PATH),exist_ok=True)\n",
        "    conn=sqlite3.connect(DB_PATH); cur=conn.cursor()\n",
        "    cur.execute('CREATE TABLE IF NOT EXISTS audit_log (ts TEXT,user TEXT,action TEXT,details TEXT)')\n",
        "    conn.commit(); conn.close()\n",
        "def log(user,action,details=\"\"):\n",
        "    conn=sqlite3.connect(DB_PATH); cur=conn.cursor()\n",
        "    cur.execute('INSERT INTO audit_log VALUES (?,?,?,?)',(datetime.datetime.utcnow().isoformat(),user,action,details))\n",
        "    conn.commit(); conn.close()\n",
        "\"\"\"\n",
        "write_module(ML_DIR/\"audit.py\", ml_audit_py)\n",
        "\n",
        "# -------- init files --------\n",
        "for p in [MODULES,SYNTH_DIR,ML_DIR,STREAMLIT_DIR]: (p/\"__init__.py\").write_text(\"# init\\n\")\n",
        "\n",
        "# -------- Print repo tree --------\n",
        "def print_tree(root=BASE,depth=3):\n",
        "    for p,_,files in os.walk(root):\n",
        "        level=p.replace(str(root),\"\").count(os.sep)\n",
        "        if level<=depth:\n",
        "            print(\"  \"*level+os.path.basename(p)+\"/\")\n",
        "            for f in files: print(\"  \"*level+\"  - \"+f)\n",
        "print_tree()\n",
        "\n",
        "# -------- Import checks --------\n",
        "for mod in [\"modules.auth\",\"modules.schema\",\"modules.synth.generator\",\"modules.ml.engine\",\"modules.ml.utils\",\"modules.ml.audit\"]:\n",
        "    try: importlib.import_module(mod); print(\"✅\",mod,\"OK\")\n",
        "    except Exception as e: print(\"❌\",mod,\"->\",e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfv9Efi-YUWT",
        "outputId": "c60c028b-c751-43cf-b82c-eeb77cf54d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loan_app/\n",
            "  - run_app.py\n",
            "  data/\n",
            "    uploads/\n",
            "      - enhanced_synth_123.csv\n",
            "  logs/\n",
            "    plots/\n",
            "  modules/\n",
            "    - auth.py\n",
            "    - __init__.py\n",
            "    - schema.py\n",
            "    ml/\n",
            "      - audit.py\n",
            "      - utils.py\n",
            "      - __init__.py\n",
            "      - engine.py\n",
            "      __pycache__/\n",
            "        - utils.cpython-312.pyc\n",
            "        - engine.cpython-312.pyc\n",
            "        - __init__.cpython-312.pyc\n",
            "        - audit.cpython-312.pyc\n",
            "    visuals/\n",
            "      - visuals.py\n",
            "    streamlit_app/\n",
            "      - __init__.py\n",
            "      - app.py\n",
            "    __pycache__/\n",
            "      - auth.cpython-312.pyc\n",
            "      - __init__.cpython-312.pyc\n",
            "      - schema.cpython-312.pyc\n",
            "    synth/\n",
            "      - enhanced_generator.py\n",
            "      - __init__.py\n",
            "      - generator.py\n",
            "      __pycache__/\n",
            "        - generator.cpython-312.pyc\n",
            "        - enhanced_generator.cpython-312.pyc\n",
            "        - __init__.cpython-312.pyc\n",
            "  models/\n",
            "  utils/\n",
            "    - inspect_data.py\n",
            "    - train_smoke.py\n",
            "✅ modules.auth OK\n",
            "✅ modules.schema OK\n",
            "✅ modules.synth.generator OK\n",
            "✅ modules.ml.engine OK\n",
            "✅ modules.ml.utils OK\n",
            "✅ modules.ml.audit OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 3/9: Write Streamlit App Skeleton --------\n",
        "# Paste into Colab and run. This writes app.py into the repo and does a simple import-check.\n",
        "\n",
        "import os, textwrap\n",
        "BASE = \"/content/loan_app\"\n",
        "APP_DIR = os.path.join(BASE, \"modules\", \"streamlit_app\")\n",
        "os.makedirs(APP_DIR, exist_ok=True)\n",
        "APP_PATH = os.path.join(APP_DIR, \"app.py\")\n",
        "\n",
        "app_src = r'''\n",
        "\"\"\"\n",
        "Streamlit app skeleton for Loan IQ\n",
        "- Tabs: Login/Register | Upload & Ingest | Client Dashboard | Admin Sandbox | Global Insights | Reports\n",
        "- Defensive imports: if a module is missing, shows guidance in UI\n",
        "- Admin credentials preserved: Admin / Shady868\n",
        "\"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import os, io, json, time\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "DATA_DIR = BASE / \"data\"\n",
        "UPLOAD_DIR = DATA_DIR / \"uploads\"\n",
        "MODELS_DIR = BASE / \"models\"\n",
        "\n",
        "UPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -- defensive imports --\n",
        "missing = []\n",
        "try:\n",
        "    from modules import auth\n",
        "except Exception as e:\n",
        "    auth = None\n",
        "    missing.append((\"auth\", str(e)))\n",
        "try:\n",
        "    from modules import schema\n",
        "except Exception as e:\n",
        "    schema = None\n",
        "    missing.append((\"schema\", str(e)))\n",
        "try:\n",
        "    from modules.synth import generator as synth_generator\n",
        "except Exception as e:\n",
        "    synth_generator = None\n",
        "    missing.append((\"synth.generator\", str(e)))\n",
        "try:\n",
        "    from modules.ml import engine as ml_engine\n",
        "except Exception as e:\n",
        "    ml_engine = None\n",
        "    missing.append((\"ml.engine\", str(e)))\n",
        "try:\n",
        "    from modules.ml import utils as ml_utils\n",
        "except Exception as e:\n",
        "    ml_utils = None\n",
        "    missing.append((\"ml.utils\", str(e)))\n",
        "try:\n",
        "    from modules.ml import audit as ml_audit\n",
        "except Exception as e:\n",
        "    ml_audit = None\n",
        "    missing.append((\"ml.audit\", str(e)))\n",
        "\n",
        "# initialize DBs if available\n",
        "if auth:\n",
        "    try:\n",
        "        auth.init_db()\n",
        "    except Exception:\n",
        "        pass\n",
        "if ml_audit:\n",
        "    try:\n",
        "        ml_audit.init_audit_db()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def show_missing_modules():\n",
        "    st.error(\"Some backend modules failed to import. The app will be degraded. See details below.\")\n",
        "    for name, err in missing:\n",
        "        st.text(f\"{name}: {err}\")\n",
        "\n",
        "def save_upload(file_bytes, filename):\n",
        "    p = UPLOAD_DIR / filename\n",
        "    with open(p, \"wb\") as f:\n",
        "        f.write(file_bytes)\n",
        "    return str(p)\n",
        "\n",
        "def load_uploaded_dataframe(path):\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        if str(path).lower().endswith((\".xls\", \".xlsx\")):\n",
        "            return pd.read_excel(path)\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to load uploaded file: {e}\")\n",
        "        return None\n",
        "\n",
        "def ensure_models_folder():\n",
        "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ----------------- Auth helpers -----------------\n",
        "def login_flow():\n",
        "    st.subheader(\"Login\")\n",
        "    username = st.text_input(\"Username\")\n",
        "    password = st.text_input(\"Password\", type=\"password\")\n",
        "    if st.button(\"Login\"):\n",
        "        if auth:\n",
        "            ok, role = auth.authenticate(username, password)\n",
        "            if ok:\n",
        "                st.session_state['user'] = username\n",
        "                st.session_state['role'] = role\n",
        "                st.success(f\"Logged in as {username} ({role})\")\n",
        "                if ml_audit:\n",
        "                    ml_audit.log(username, \"login\", \"web login\")\n",
        "            else:\n",
        "                st.error(\"Invalid credentials\")\n",
        "        else:\n",
        "            st.error(\"Auth module missing\")\n",
        "\n",
        "def register_flow():\n",
        "    st.subheader(\"Register (company)\")\n",
        "    r_user = st.text_input(\"New username\", key=\"r_user\")\n",
        "    r_pass = st.text_input(\"New password\", type=\"password\", key=\"r_pass\")\n",
        "    if st.button(\"Register\"):\n",
        "        if auth:\n",
        "            auth.register_user(r_user, r_pass, role=\"company\")\n",
        "            st.success(\"User registered (company). Admin must approve if needed.\")\n",
        "        else:\n",
        "            st.error(\"Auth module missing\")\n",
        "\n",
        "# ----------------- Upload & Ingest -----------------\n",
        "def upload_and_preview():\n",
        "    st.header(\"Upload dataset (CSV/XLSX)\")\n",
        "    uploaded = st.file_uploader(\"Choose a file\", type=[\"csv\",\"xlsx\"])\n",
        "    if uploaded is not None:\n",
        "        bytes_data = uploaded.getvalue()\n",
        "        filename = uploaded.name\n",
        "        saved = save_upload(bytes_data, filename)\n",
        "        st.success(f\"Saved to {saved}\")\n",
        "        df = load_uploaded_dataframe(saved)\n",
        "        if df is not None:\n",
        "            st.write(\"Preview:\")\n",
        "            st.dataframe(df.head(10))\n",
        "            # attempt schema.prepare_for_ml if available\n",
        "            if schema:\n",
        "                try:\n",
        "                    X, y, enriched = schema.prepare_for_ml(df, aggregate_by_client=False)\n",
        "                    st.write(\"Enriched preview (first 10 rows):\")\n",
        "                    st.dataframe(enriched.head(10))\n",
        "                    st.write(\"Feature matrix shape:\", X.shape)\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"prepare_for_ml failed: {e}\")\n",
        "            else:\n",
        "                st.info(\"Schema module not available; raw preview shown.\")\n",
        "\n",
        "# ----------------- Client Dashboard -----------------\n",
        "def client_dashboard():\n",
        "    st.header(\"Client Dashboard\")\n",
        "    st.info(\"Lookup by national ID (unique identifier). This view supports multiple loans per ID.\")\n",
        "    nid = st.text_input(\"Enter National ID\")\n",
        "    if st.button(\"Lookup\"):\n",
        "        # naive search across uploads\n",
        "        found = []\n",
        "        for p in UPLOAD_DIR.glob(\"*\"):\n",
        "            try:\n",
        "                import pandas as pd\n",
        "                if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                    df = pd.read_excel(p)\n",
        "                else:\n",
        "                    df = pd.read_csv(p)\n",
        "                # standardize col names\n",
        "                if schema:\n",
        "                    df2 = schema.coerce_types_and_derive(df)\n",
        "                else:\n",
        "                    df2 = df\n",
        "                if \"national_id\" in df2.columns:\n",
        "                    matches = df2[df2[\"national_id\"].astype(str).str.contains(str(nid))]\n",
        "                    if not matches.empty:\n",
        "                        found.append((p.name, matches))\n",
        "            except Exception as e:\n",
        "                st.write(f\"Error reading {p}: {e}\")\n",
        "        if not found:\n",
        "            st.warning(\"No loans found for that ID in uploaded files.\")\n",
        "        else:\n",
        "            for fname, dfm in found:\n",
        "                st.subheader(f\"Matches in {fname}\")\n",
        "                st.dataframe(dfm.head(50))\n",
        "                # show aggregates\n",
        "                try:\n",
        "                    agg = dfm.groupby(\"national_id\").agg({\n",
        "                        \"loan_amount_num\":\"sum\",\n",
        "                        \"is_default\":\"max\",\n",
        "                        \"days_since_issue\":\"min\"\n",
        "                    })\n",
        "                    st.write(\"Aggregates:\")\n",
        "                    st.dataframe(agg)\n",
        "                except Exception as e:\n",
        "                    st.write(f\"Aggregation failed: {e}\")\n",
        "\n",
        "# ----------------- Admin Sandbox -----------------\n",
        "def admin_sandbox():\n",
        "    st.header(\"Admin Sandbox (admin only)\")\n",
        "    user = st.session_state.get(\"user\")\n",
        "    role = st.session_state.get(\"role\")\n",
        "    if user != \"Admin\":\n",
        "        st.warning(\"Admin sandbox is restricted to Admin user. Please login as Admin to continue.\")\n",
        "        return\n",
        "\n",
        "    st.subheader(\"Impersonation\")\n",
        "    imp = st.text_input(\"Impersonate username (type in username to impersonate)\")\n",
        "    if st.button(\"Impersonate\"):\n",
        "        if auth:\n",
        "            st.session_state[\"impersonate\"] = imp\n",
        "            st.success(f\"Now impersonating {imp}\")\n",
        "            if ml_audit: ml_audit.log(\"Admin\", \"impersonate\", imp)\n",
        "        else:\n",
        "            st.error(\"Auth module missing\")\n",
        "\n",
        "    st.subheader(\"Synthetic Data\")\n",
        "    n = st.number_input(\"Rows to generate\", min_value=100, max_value=20000, value=2000, step=100)\n",
        "    default_rate = st.slider(\"Default rate\", 0.0, 1.0, 0.25)\n",
        "    if st.button(\"Generate synthetic dataset\"):\n",
        "        if synth_generator:\n",
        "            df = synth_generator.make_dataset(int(n), default_rate=default_rate, include_history=True)\n",
        "            p = UPLOAD_DIR / f\"synthetic_{int(time.time())}.csv\"\n",
        "            df.to_csv(p, index=False)\n",
        "            st.success(f\"Generated synthetic dataset: {p}\")\n",
        "            st.dataframe(df.head())\n",
        "            if ml_audit: ml_audit.log(\"Admin\", \"synth_generate\", f\"{p}\")\n",
        "        else:\n",
        "            st.error(\"Synth generator missing\")\n",
        "\n",
        "    st.subheader(\"Model Training / Registry\")\n",
        "    model_name = st.text_input(\"Model name\", value=\"baseline\")\n",
        "    if st.button(\"Train baseline model\"):\n",
        "        # find most recent upload to use for training\n",
        "        files = sorted(UPLOAD_DIR.glob(\"*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        if not files:\n",
        "            st.error(\"No uploaded datasets found for training\")\n",
        "        else:\n",
        "            # load first file\n",
        "            import pandas as pd\n",
        "            p = files[0]\n",
        "            try:\n",
        "                if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                    df = pd.read_excel(p)\n",
        "                else:\n",
        "                    df = pd.read_csv(p)\n",
        "                st.info(f\"Using {p.name} for training\")\n",
        "                if schema:\n",
        "                    X, y, enriched = schema.prepare_for_ml(df, aggregate_by_client=False)\n",
        "                else:\n",
        "                    st.error(\"Schema module required for training\")\n",
        "                    return\n",
        "                st.write(\"Feature sample:\")\n",
        "                st.dataframe(X.head())\n",
        "                ensure_models_folder()\n",
        "                if ml_engine:\n",
        "                    res = ml_engine.train_baseline(X, y, name=model_name)\n",
        "                    st.success(f\"Training complete. AUC: {res.get('auc'):.4f}\")\n",
        "                    st.write(\"Model artifact:\", res.get(\"model_path\"))\n",
        "                    if ml_audit: ml_audit.log(st.session_state.get(\"user\",\"unknown\"), \"train_model\", json.dumps(res))\n",
        "                else:\n",
        "                    st.error(\"ML engine missing\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Training pipeline failed: {e}\")\n",
        "\n",
        "    st.subheader(\"Model Registry / Pin / Impersonate\")\n",
        "    # list models\n",
        "    models = [p for p in MODELS_DIR.glob(\"*.meta.json\")]\n",
        "    for m in models:\n",
        "        try:\n",
        "            meta = json.load(open(m))\n",
        "            st.write(meta)\n",
        "            if st.button(f\"Pin {meta.get('name')}\"):\n",
        "                # write production pointer\n",
        "                prod = MODELS_DIR / \"PROD_MODEL.txt\"\n",
        "                prod.write_text(meta.get(\"name\"))\n",
        "                st.success(f\"Pinned {meta.get('name')} as production\")\n",
        "                if ml_audit: ml_audit.log(\"Admin\", \"pin_model\", meta.get(\"name\"))\n",
        "        except Exception as e:\n",
        "            st.write(f\"Failed to read model meta {m}: {e}\")\n",
        "\n",
        "# ----------------- Global Insights -----------------\n",
        "def global_insights():\n",
        "    st.header(\"Global Insights\")\n",
        "    st.info(\"KPI snapshots across uploaded datasets\")\n",
        "    # simple aggregate across all uploads\n",
        "    import pandas as pd\n",
        "    dfs = []\n",
        "    for p in UPLOAD_DIR.glob(\"*\"):\n",
        "        try:\n",
        "            if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                df = pd.read_excel(p)\n",
        "            else:\n",
        "                df = pd.read_csv(p)\n",
        "            if schema:\n",
        "                df2 = schema.coerce_types_and_derive(df)\n",
        "                dfs.append(df2)\n",
        "        except Exception as e:\n",
        "            st.write(f\"Failed reading {p}: {e}\")\n",
        "    if not dfs:\n",
        "        st.warning(\"No enriched uploads available\")\n",
        "        return\n",
        "    full = pd.concat(dfs, ignore_index=True)\n",
        "    st.write(\"Combined data sample:\")\n",
        "    st.dataframe(full.head())\n",
        "    # KPIs\n",
        "    total_loans = len(full)\n",
        "    total_default = int(full[\"is_default\"].sum()) if \"is_default\" in full.columns else 0\n",
        "    avg_loan = float(full[\"loan_amount_num\"].mean()) if \"loan_amount_num\" in full.columns else 0.0\n",
        "    st.metric(\"Total loans\", total_loans)\n",
        "    st.metric(\"Total defaults\", total_default)\n",
        "    st.metric(\"Avg loan amount\", f\"{avg_loan:,.0f}\")\n",
        "\n",
        "# ----------------- Reports -----------------\n",
        "def reports_tab():\n",
        "    st.header(\"Reports & Exports\")\n",
        "    st.write(\"Export aggregated CSVs, per-client reports, or model predictions (CSV).\")\n",
        "    if st.button(\"Export client-level aggregates (CSV)\"):\n",
        "        import pandas as pd\n",
        "        dfs = []\n",
        "        for p in UPLOAD_DIR.glob(\"*\"):\n",
        "            try:\n",
        "                if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                    df = pd.read_excel(p)\n",
        "                else:\n",
        "                    df = pd.read_csv(p)\n",
        "                if schema:\n",
        "                    X, y, enriched = schema.prepare_for_ml(df, aggregate_by_client=True)\n",
        "                    dfs.append(pd.concat([X.reset_index(), y.reset_index(drop=False)], axis=1))\n",
        "            except Exception as e:\n",
        "                st.write(f\"Skipping {p}: {e}\")\n",
        "        if not dfs:\n",
        "            st.warning(\"No data to export\")\n",
        "            return\n",
        "        out = pd.concat(dfs, ignore_index=True)\n",
        "        out_path = UPLOAD_DIR / f\"client_aggregates_{int(time.time())}.csv\"\n",
        "        out.to_csv(out_path, index=False)\n",
        "        st.success(f\"Exported: {out_path}\")\n",
        "\n",
        "# ----------------- Main layout -----------------\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Loan IQ\", layout=\"wide\")\n",
        "    st.title(\"Loan IQ — Loan company portal + Admin Sandbox\")\n",
        "\n",
        "    if missing:\n",
        "        show_missing_modules()\n",
        "        st.stop()\n",
        "\n",
        "    # session init\n",
        "    if \"user\" not in st.session_state:\n",
        "        st.session_state[\"user\"] = None\n",
        "        st.session_state[\"role\"] = None\n",
        "    # top-level nav\n",
        "    menu = [\"Home / Login\", \"Upload & Ingest\", \"Client Dashboard\", \"Admin Sandbox\", \"Global Insights\", \"Reports\"]\n",
        "    choice = st.sidebar.selectbox(\"Menu\", menu)\n",
        "\n",
        "    if choice == \"Home / Login\":\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            login_flow()\n",
        "        with col2:\n",
        "            register_flow()\n",
        "        # quick links\n",
        "        st.write(\"Quick actions:\")\n",
        "        if st.button(\"Generate small synthetic (100 rows)\"):\n",
        "            if synth_generator:\n",
        "                df = synth_generator.make_dataset(100, default_rate=0.2)\n",
        "                p = UPLOAD_DIR / f\"synthetic_small_{int(time.time())}.csv\"\n",
        "                df.to_csv(p, index=False)\n",
        "                st.success(f\"Saved small synthetic to {p}\")\n",
        "            else:\n",
        "                st.error(\"Synth generator missing\")\n",
        "\n",
        "    elif choice == \"Upload & Ingest\":\n",
        "        upload_and_preview()\n",
        "\n",
        "    elif choice == \"Client Dashboard\":\n",
        "        client_dashboard()\n",
        "\n",
        "    elif choice == \"Admin Sandbox\":\n",
        "        admin_sandbox()\n",
        "\n",
        "    elif choice == \"Global Insights\":\n",
        "        global_insights()\n",
        "\n",
        "    elif choice == \"Reports\":\n",
        "        reports_tab()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open(APP_PATH, \"w\") as f:\n",
        "    f.write(app_src)\n",
        "\n",
        "print(\"Wrote Streamlit app to:\", APP_PATH)\n",
        "# quick import-check (non-executing)\n",
        "print(\"Attempting lightweight import checks for modules used by the app (no heavy work).\")\n",
        "errs = []\n",
        "try:\n",
        "    import importlib\n",
        "    importlib.invalidate_caches()\n",
        "    for mod in [\"modules.auth\", \"modules.schema\", \"modules.synth.generator\", \"modules.ml.engine\", \"modules.ml.utils\", \"modules.ml.audit\"]:\n",
        "        try:\n",
        "            importlib.import_module(mod)\n",
        "            print(\"Imported:\", mod)\n",
        "        except Exception as e:\n",
        "            print(\"Import failed:\", mod, \"->\", e)\n",
        "            errs.append((mod, str(e)))\n",
        "except Exception as e:\n",
        "    print(\"Import-check harness error:\", e)\n",
        "\n",
        "if errs:\n",
        "    print(\"\\\\nSome imports failed (they will show in the app). You can inspect the file at\", APP_PATH)\n",
        "else:\n",
        "    print(\"\\\\nAll light imports succeeded. App skeleton ready. Next: Cell 4 will add CLI helpers, ngrok helpers, and a small runner script.\")"
      ],
      "metadata": {
        "id": "Rsg20vJMYoId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c48e04-c351-4e8d-ddd0-1863f308c020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Streamlit app to: /content/loan_app/modules/streamlit_app/app.py\n",
            "Attempting lightweight import checks for modules used by the app (no heavy work).\n",
            "Imported: modules.auth\n",
            "Imported: modules.schema\n",
            "Imported: modules.synth.generator\n",
            "Imported: modules.ml.engine\n",
            "Imported: modules.ml.utils\n",
            "Imported: modules.ml.audit\n",
            "\\nAll light imports succeeded. App skeleton ready. Next: Cell 4 will add CLI helpers, ngrok helpers, and a small runner script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 4/9: Runner & Ngrok --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "RUNNER = BASE / \"run_app.py\"\n",
        "APP_FILE = BASE / \"modules\" / \"streamlit_app\" / \"app.py\"\n",
        "\n",
        "runner_code = r\"\"\"\n",
        "import os, sys, time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "APP_PATH = \"/content/loan_app/modules/streamlit_app/app.py\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(APP_PATH):\n",
        "        print(\"❌ App not found:\", APP_PATH); sys.exit(1)\n",
        "\n",
        "    ngrok.kill()  # kill old tunnels\n",
        "\n",
        "    public_url=None\n",
        "    for attempt in range(5):\n",
        "        try:\n",
        "            tunnel=ngrok.connect(8501,\"http\"); public_url=tunnel.public_url; break\n",
        "        except Exception as e:\n",
        "            print(f\"ngrok attempt {attempt+1}/5 failed:\",e); time.sleep(3)\n",
        "    if not public_url:\n",
        "        print(\"❌ ngrok failed after retries\"); sys.exit(1)\n",
        "\n",
        "    print(f\"✅ App will be live at: {public_url}\\\\n\")\n",
        "\n",
        "    os.system(f\"streamlit run {APP_PATH} --server.port 8501 --server.headless true\")\n",
        "\n",
        "if __name__==\"__main__\": main()\n",
        "\"\"\"\n",
        "RUNNER.write_text(runner_code.strip()+\"\\n\")\n",
        "\n",
        "# Quick smoke check\n",
        "if APP_FILE.exists():\n",
        "    print(\"✅ Runner written:\", RUNNER)\n",
        "    print(\"👉 Later, launch with: !python /content/loan_app/run_app.py\")\n",
        "else:\n",
        "    print(\"❌ Streamlit app missing, re-run Cell 3 first\")"
      ],
      "metadata": {
        "id": "9B-c9_mgY0k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3659656a-80f5-496b-e1f4-08ea9c40558f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Runner written: /content/loan_app/run_app.py\n",
            "👉 Later, launch with: !python /content/loan_app/run_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 5/9: Utility Scripts & Smoke Tests --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "UTILS_DIR = BASE / \"utils\"\n",
        "UTILS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- inspect_data.py ---\n",
        "inspect_code = r\"\"\"\n",
        "import sys, pandas as pd\n",
        "from modules import schema\n",
        "from pathlib import Path\n",
        "\n",
        "UPLOAD_DIR = Path(\"/content/loan_app/data/uploads\")\n",
        "\n",
        "def inspect_latest():\n",
        "    files=sorted(UPLOAD_DIR.glob(\"*\"), key=lambda p:p.stat().st_mtime, reverse=True)\n",
        "    if not files:\n",
        "        print(\"❌ No uploaded files to inspect.\"); return\n",
        "    f=files[0]; print(\"Inspecting:\",f)\n",
        "    df=pd.read_csv(f) if f.suffix==\".csv\" else pd.read_excel(f)\n",
        "    print(\"Raw shape:\",df.shape)\n",
        "    X,y,en=schema.prepare_for_ml(df)\n",
        "    print(\"Enriched shape:\",en.shape,\" Features:\",X.shape,\" Target balance:\",y.value_counts().to_dict())\n",
        "\n",
        "if __name__==\"__main__\": inspect_latest()\n",
        "\"\"\"\n",
        "(BASE/\"utils\"/\"inspect_data.py\").write_text(inspect_code.strip()+\"\\n\")\n",
        "\n",
        "# --- train_smoke.py ---\n",
        "train_code = r\"\"\"\n",
        "import sys, pandas as pd\n",
        "from modules.synth import generator\n",
        "from modules import schema\n",
        "from modules.ml import engine\n",
        "\n",
        "def smoke_train():\n",
        "    print(\"Generating synthetic dataset (500 rows)...\")\n",
        "    df=generator.make_dataset(500, default_rate=0.3)\n",
        "    X,y,en=schema.prepare_for_ml(df)\n",
        "    print(\"Training baseline model...\")\n",
        "    res=engine.train_baseline(X,y,name=\"smoke_test\")\n",
        "    print(\"✅ Smoke train done. AUC:\",res.get(\"auc\"))\n",
        "\n",
        "if __name__==\"__main__\": smoke_train()\n",
        "\"\"\"\n",
        "(BASE/\"utils\"/\"train_smoke.py\").write_text(train_code.strip()+\"\\n\")\n",
        "\n",
        "# --- Smoke tests ---\n",
        "print(\"Running smoke test: inspect_data.py (no files yet expected)...\")\n",
        "os.system(\"python /content/loan_app/utils/inspect_data.py || true\")\n",
        "\n",
        "print(\"\\nRunning smoke test: train_smoke.py...\")\n",
        "os.system(\"python /content/loan_app/utils/train_smoke.py || true\")\n",
        "\n",
        "print(\"\\n✅ Utilities ready. You can run them manually from Colab shell.\")"
      ],
      "metadata": {
        "id": "D5O6ZD9JjpWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c99e1d6-7ab9-482c-975e-ed5782cc98fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running smoke test: inspect_data.py (no files yet expected)...\n",
            "\n",
            "Running smoke test: train_smoke.py...\n",
            "\n",
            "✅ Utilities ready. You can run them manually from Colab shell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 6/9: Enhanced Kenyan Synthetic Generator + Visuals (50 towns + regions) --------\n",
        "import os, sys, random\n",
        "from pathlib import Path\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "SYNTH_DIR = BASE / \"modules\" / \"synth\"\n",
        "VIS_DIR = BASE / \"modules\" / \"visuals\"\n",
        "LOG_PLOTS = BASE / \"logs\" / \"plots\"\n",
        "LOG_PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def write_mod(path: Path, code: str):\n",
        "    path.write_text(code.strip() + \"\\n\")\n",
        "    with open(path) as f: f.read()\n",
        "\n",
        "# ---------- enhanced generator ----------\n",
        "enhanced_gen = r\"\"\"\n",
        "import random\n",
        "from faker import Faker\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "faker = Faker()\n",
        "Faker.seed = lambda s: random.seed(s)\n",
        "\n",
        "# ~50 towns mapped to regions\n",
        "TOWN_REGION = {\n",
        "    \"Nairobi\":\"Nairobi\",\"Thika\":\"Central\",\"Nyeri\":\"Central\",\"Murang'a\":\"Central\",\"Kiambu\":\"Central\",\n",
        "    \"Machakos\":\"Eastern\",\"Embu\":\"Eastern\",\"Meru\":\"Eastern\",\"Kitui\":\"Eastern\",\"Mwingi\":\"Eastern\",\n",
        "    \"Nakuru\":\"Rift Valley\",\"Naivasha\":\"Rift Valley\",\"Kericho\":\"Rift Valley\",\"Eldoret\":\"Rift Valley\",\"Bomet\":\"Rift Valley\",\"Narok\":\"Rift Valley\",\"Kajiado\":\"Rift Valley\",\n",
        "    \"Kisumu\":\"Nyanza\",\"Kisii\":\"Nyanza\",\"Homabay\":\"Nyanza\",\"Migori\":\"Nyanza\",\"Siaya\":\"Nyanza\",\n",
        "    \"Mombasa\":\"Coast\",\"Kilifi\":\"Coast\",\"Malindi\":\"Coast\",\"Kwale\":\"Coast\",\"Lamu\":\"Coast\",\"Voi\":\"Coast\",\n",
        "    \"Garissa\":\"North Eastern\",\"Wajir\":\"North Eastern\",\"Mandera\":\"North Eastern\",\n",
        "    \"Kakamega\":\"Western\",\"Bungoma\":\"Western\",\"Busia\":\"Western\",\"Vihiga\":\"Western\",\"Trans Nzoia\":\"Western\",\n",
        "    \"Turkana\":\"Rift Valley\",\"West Pokot\":\"Rift Valley\",\"Isiolo\":\"Eastern\",\"Marsabit\":\"Eastern\",\"Samburu\":\"Rift Valley\",\n",
        "    \"Taita Taveta\":\"Coast\",\"Taveta\":\"Coast\",\"Loitoktok\":\"Rift Valley\",\"Gilgil\":\"Rift Valley\",\"Kerugoya\":\"Central\"\n",
        "}\n",
        "\n",
        "# Weighted regions: Nairobi, Eastern, Rift, Central get more weight\n",
        "REGION_WEIGHTS = {\n",
        "    \"Nairobi\":0.20,\"Eastern\":0.20,\"Rift Valley\":0.20,\"Central\":0.20,\n",
        "    \"Nyanza\":0.07,\"Western\":0.06,\"Coast\":0.05,\"North Eastern\":0.02\n",
        "}\n",
        "\n",
        "def _pick_town():\n",
        "    regions = list(REGION_WEIGHTS.keys())\n",
        "    region = random.choices(regions, weights=[REGION_WEIGHTS[r] for r in regions])[0]\n",
        "    towns = [t for t,r in TOWN_REGION.items() if r==region]\n",
        "    town = random.choice(towns)\n",
        "    return town, region\n",
        "\n",
        "def _skewed_loan_amount(min_k=5000, max_k=70000, skew=1.8):\n",
        "    r = random.random() ** skew\n",
        "    return int(min_k + r * (max_k - min_k))\n",
        "\n",
        "def generate_national_id(prefix_choices=None):\n",
        "    if prefix_choices is None: prefix_choices=[32,33,34,35,36]\n",
        "    prefix = str(random.choice(prefix_choices))\n",
        "    tail = str(random.randint(0,999999)).zfill(6)\n",
        "    return prefix + tail\n",
        "\n",
        "def make_dataset(n=2000, default_rate=0.20, multi_loan_frac=0.30, female_frac=0.60,\n",
        "                 loan_min=5000, loan_max=70000, seed=42):\n",
        "    random.seed(seed); rows=[]\n",
        "    for i in range(n):\n",
        "        gender = \"female\" if random.random() < female_frac else \"male\"\n",
        "        first = faker.first_name_female() if gender==\"female\" else faker.first_name_male()\n",
        "        last = faker.last_name()\n",
        "        name = f\"{first} {last}\"\n",
        "        nid = generate_national_id()\n",
        "        phone = \"+2547\" + str(random.randint(10000000,99999999))[1:9]\n",
        "        town, region = _pick_town()\n",
        "        product = random.choice([\"Inuka 4 weeks\",\"Fadhili 6 weeks\",\"Kuza 12 weeks\",\"AgriAdvance 16 weeks\",\"FlexiLoan 8 weeks\",\"BizBoost 10 weeks\",\"QuickPay 2 weeks\"])\n",
        "        income = random.choice([12000,15000,20000,25000,30000,40000,60000])\n",
        "        amount = _skewed_loan_amount(loan_min, loan_max, skew=1.8)\n",
        "        created = datetime.utcnow() - timedelta(days=random.randint(0,730))\n",
        "        dobias = default_rate + (0.02 if amount > 40000 else -0.01)\n",
        "        status = \"default\" if random.random() < max(0, min(0.95, dobias)) else \"performing\"\n",
        "        rows.append({\n",
        "            \"client_id\": i, \"name\": name, \"national_id\": nid, \"phone\": phone,\n",
        "            \"town\": town, \"region\": region, \"product\": product,\n",
        "            \"income\": income, \"loan_amount\": amount, \"loan_status\": status,\n",
        "            \"created_date\": created.strftime(\"%Y-%m-%d\"), \"gender\": gender\n",
        "        })\n",
        "        if random.random() < multi_loan_frac:\n",
        "            extra_amount = int(amount * random.uniform(0.3, 1.2))\n",
        "            created2 = created - timedelta(days=random.randint(30, 900))\n",
        "            rows.append({\n",
        "                \"client_id\": i, \"name\": name, \"national_id\": nid, \"phone\": phone,\n",
        "                \"town\": town, \"region\": region, \"product\": random.choice([\"Inuka 4 weeks\",\"Fadhili 6 weeks\",\"Kuza 12 weeks\",\"AgriAdvance 16 weeks\",\"FlexiLoan 8 weeks\",\"BizBoost 10 weeks\",\"QuickPay 2 weeks\"]),\n",
        "                \"income\": income, \"loan_amount\": extra_amount, \"loan_status\": \"performing\",\n",
        "                \"created_date\": created2.strftime(\"%Y-%m-%d\"), \"gender\": gender\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\"\"\"\n",
        "write_mod(SYNTH_DIR / \"enhanced_generator.py\", enhanced_gen)\n",
        "\n",
        "# ---------- visuals (same as before, package) ----------\n",
        "visuals_py = r\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "PLOT_DIR = Path('/content/loan_app/logs/plots')\n",
        "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def kpis_from_df(df):\n",
        "    total_loans = len(df)\n",
        "    defaults = int(df.get('loan_status', pd.Series()).astype(str).str.lower().str.contains('default').sum()) if 'loan_status' in df else int(df.get('is_default', pd.Series()).sum() if 'is_default' in df else 0)\n",
        "    avg_loan = float(df.get('loan_amount', pd.Series()).astype(float).mean()) if 'loan_amount' in df else 0.0\n",
        "    female_share = float((df.get('gender','').astype(str)=='female').mean()) if 'gender' in df else np.nan\n",
        "    multi_loans = df.groupby(df.get('national_id', df.get('client_id'))).size().gt(1).mean()\n",
        "    return {'total_loans':total_loans,'defaults':defaults,'avg_loan':avg_loan,'female_share':female_share,'multi_loan_frac':float(multi_loans)}\n",
        "\n",
        "def plot_loan_size_hist(df, filename='loan_size_hist.png'):\n",
        "    vals = df['loan_amount'].astype(float)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(vals, bins=30)\n",
        "    ax.set_title('Loan size distribution'); ax.set_xlabel('Loan amount'); ax.set_ylabel('Count')\n",
        "    out = PLOT_DIR / filename\n",
        "    fig.savefig(out, bbox_inches='tight'); plt.close(fig); return str(out)\n",
        "\n",
        "def plot_defaults_pie(df, filename='defaults_pie.png'):\n",
        "    status = df['loan_status'].astype(str).str.lower().apply(lambda s: 'default' if 'default' in s else 'performing')\n",
        "    counts = status.value_counts()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.pie(counts.values, labels=counts.index, autopct='%1.1f%%')\n",
        "    ax.set_title('Default vs Performing')\n",
        "    out = PLOT_DIR / filename\n",
        "    fig.savefig(out, bbox_inches='tight'); plt.close(fig); return str(out)\n",
        "\n",
        "def plot_time_series_by_month(df, filename='loans_by_month.png'):\n",
        "    if 'created_date' not in df.columns and 'created_date_parsed' in df.columns:\n",
        "        df['created_date'] = df['created_date_parsed']\n",
        "    df['created_date_parsed'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
        "    df['month'] = df['created_date_parsed'].dt.to_period('M')\n",
        "    counts = df.groupby('month').size()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(counts.index.to_timestamp(), counts.values)\n",
        "    ax.set_title('Loans by month'); ax.set_ylabel('Count')\n",
        "    out = PLOT_DIR / filename\n",
        "    fig.savefig(out, bbox_inches='tight'); plt.close(fig); return str(out)\n",
        "\n",
        "def safe_shap_plot(shap_values, feature_names=None, out_name='shap_summary.png'):\n",
        "    try:\n",
        "        import shap\n",
        "        fig = shap.plots.bar(shap_values, show=False)\n",
        "        out = PLOT_DIR / out_name\n",
        "        fig.figure.savefig(out, bbox_inches='tight')\n",
        "        return str(out)\n",
        "    except Exception as e:\n",
        "        p = PLOT_DIR / out_name\n",
        "        with open(p,'w') as f: f.write('shap failed: '+str(e))\n",
        "        return str(p)\n",
        "\"\"\"\n",
        "(VIS_DIR / \"__init__.py\").write_text(visuals_py.strip()+\"\\n\")\n",
        "\n",
        "# ---------- Smoke test ----------\n",
        "print(\"Running smoke test for enhanced generator + visuals (50 towns + regions)...\")\n",
        "try:\n",
        "    import importlib\n",
        "    importlib.invalidate_caches()\n",
        "    ig = importlib.import_module(\"modules.synth.enhanced_generator\")\n",
        "    import modules.visuals as vis\n",
        "    importlib.reload(vis)  # force reload so new functions appear\n",
        "    schema = importlib.import_module(\"modules.schema\")\n",
        "\n",
        "    df = ig.make_dataset(n=1000, default_rate=0.20, multi_loan_frac=0.30,\n",
        "                         female_frac=0.60, seed=123)\n",
        "    csvp = (BASE/\"data/uploads/enhanced_synth_regions.csv\")\n",
        "    df.to_csv(csvp, index=False)\n",
        "    print(\"Generated dataset saved to:\", csvp)\n",
        "\n",
        "    X,y,en = schema.prepare_for_ml(df)\n",
        "    print(\"Enriched shape:\", en.shape, \"Feature matrix:\", X.shape)\n",
        "\n",
        "    kpis = vis.kpis_from_df(df)\n",
        "    print(\"KPIs:\", kpis)\n",
        "\n",
        "    p1 = vis.plot_loan_size_hist(df)\n",
        "    p2 = vis.plot_defaults_pie(df)\n",
        "    p3 = vis.plot_time_series_by_month(df)\n",
        "    print(\"Plots written:\", p1, p2, p3)\n",
        "\n",
        "    print(\"Cell 6 smoke test: ✅ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 6 smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhAcxeBZnVgp",
        "outputId": "628d345d-9ea0-4c9f-cb16-5605e8150b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running smoke test for enhanced generator + visuals (50 towns + regions)...\n",
            "Generated dataset saved to: /content/loan_app/data/uploads/enhanced_synth_regions.csv\n",
            "Enriched shape: (1292, 25) Feature matrix: (1292, 26)\n",
            "KPIs: {'total_loans': 1292, 'defaults': 208, 'avg_loan': 26561.542569659443, 'female_share': 0.6160990712074303, 'multi_loan_frac': 0.292}\n",
            "Plots written: /content/loan_app/logs/plots/loan_size_hist.png /content/loan_app/logs/plots/defaults_pie.png /content/loan_app/logs/plots/loans_by_month.png\n",
            "Cell 6 smoke test: ✅ OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 7/9: Client Reports + SHAP Integration (clean + fallback) --------\n",
        "import os, json, joblib, sys, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "REPORTS_DIR = BASE / \"modules\" / \"reports\"\n",
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "reports_py = r\"\"\"\n",
        "import os, pandas as pd, json, joblib, numpy as np, warnings\n",
        "from pathlib import Path\n",
        "from modules import schema\n",
        "from modules.ml import engine, utils\n",
        "from modules.visuals import kpis_from_df\n",
        "\n",
        "OUT_DIR = Path('/content/loan_app/logs/reports')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "warnings.filterwarnings(\"ignore\")  # suppress sklearn/xgboost spam\n",
        "\n",
        "def _fallback_features(model, X):\n",
        "    try:\n",
        "        if hasattr(model, \"coef_\"):\n",
        "            coefs = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
        "            top_idx = np.argsort(np.abs(coefs))[-5:]\n",
        "            return {X.columns[i]: float(coefs[i]) for i in top_idx}\n",
        "        elif hasattr(model, \"feature_importances_\"):\n",
        "            imps = model.feature_importances_\n",
        "            top_idx = np.argsort(imps)[-5:]\n",
        "            return {X.columns[i]: float(imps[i]) for i in top_idx}\n",
        "    except Exception as e:\n",
        "        return {\"explain_fallback_error\": str(e)}\n",
        "    return {\"explain_info\": \"No explainable features found.\"}\n",
        "\n",
        "def client_report(df, client_id, model_path=None):\n",
        "    df_en = schema.coerce_and_enrich(df)\n",
        "    cdf = df_en[df_en['unique_client_id'].astype(str)==str(client_id)]\n",
        "    if cdf.empty:\n",
        "        return {'error':'Client not found','client_id':client_id}\n",
        "\n",
        "    rep = {\n",
        "        'client_id': client_id,\n",
        "        'towns': list(cdf['town'].unique()) if 'town' in cdf.columns else [],\n",
        "        'regions': list(cdf['region'].unique()) if 'region' in cdf.columns else [],\n",
        "        'loans': len(cdf),\n",
        "        'total_amount': float(cdf['loan_amount_num'].sum()) if 'loan_amount_num' in cdf else None,\n",
        "        'avg_amount': float(cdf['loan_amount_num'].mean()) if 'loan_amount_num' in cdf else None,\n",
        "        'status_counts': cdf['loan_status'].value_counts().to_dict() if 'loan_status' in cdf else {}\n",
        "    }\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model = joblib.load(model_path)\n",
        "            X,y,en = schema.prepare_for_ml(cdf)\n",
        "            if not X.empty:\n",
        "                prob = model.predict_proba(X)[:,1].mean()\n",
        "                rep['pred_default_prob'] = float(prob)\n",
        "                try:\n",
        "                    expl = utils.explain(model,X,n=min(50,len(X)))\n",
        "                    rep['shap_preview'] = str(expl)[:500]\n",
        "                except Exception:\n",
        "                    rep['shap_preview'] = _fallback_features(model,X)\n",
        "        except Exception as e:\n",
        "            rep['pred_error'] = str(e)\n",
        "\n",
        "    return rep\n",
        "\n",
        "def export_client_reports(df, model_path=None, out_csv='client_reports.csv'):\n",
        "    df_en = schema.coerce_and_enrich(df)\n",
        "    clients = df_en['unique_client_id'].unique()\n",
        "    rows=[client_report(df_en,cid,model_path) for cid in clients]\n",
        "    outp = OUT_DIR/out_csv\n",
        "    pd.DataFrame(rows).to_csv(outp,index=False)\n",
        "    return str(outp)\n",
        "\"\"\"\n",
        "(REPORTS_DIR/\"__init__.py\").write_text(\"# init\\n\")\n",
        "(REPORTS_DIR/\"reports.py\").write_text(reports_py.strip()+\"\\n\")\n",
        "\n",
        "# --- Smoke test ---\n",
        "print(\"Running smoke test for reports (clean + fallback)...\")\n",
        "try:\n",
        "    import importlib, warnings\n",
        "    sys.path.append(\"/content/loan_app\")\n",
        "    importlib.invalidate_caches()\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "    ig = importlib.import_module(\"modules.synth.enhanced_generator\")\n",
        "    schema = importlib.import_module(\"modules.schema\")\n",
        "    eng = importlib.import_module(\"modules.ml.engine\")\n",
        "    repmod = importlib.import_module(\"modules.reports.reports\")\n",
        "\n",
        "    # generate dataset\n",
        "    df = ig.make_dataset(300, seed=999)\n",
        "    X,y,en = schema.prepare_for_ml(df)\n",
        "\n",
        "    # train baseline\n",
        "    res = eng.train_baseline(X,y,name=\"report_test\")\n",
        "\n",
        "    # generate sample report\n",
        "    r = repmod.client_report(df, df['national_id'].iloc[0], res['model_path'])\n",
        "    print(\"Sample report:\", json.dumps(r,indent=2)[:300])\n",
        "\n",
        "    # export all\n",
        "    csvout = repmod.export_client_reports(en, res['model_path'])\n",
        "    print(\"Reports CSV saved:\", csvout)\n",
        "\n",
        "    print(\"Cell 7 smoke test: ✅ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 7 smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdG8Ny8Nnx7X",
        "outputId": "7493651f-5b72-45fd-9da3-f631d72000b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running smoke test for reports (clean + fallback)...\n",
            "Sample report: {\n",
            "  \"client_id\": \"32940000\",\n",
            "  \"towns\": [\n",
            "    \"Turkana\"\n",
            "  ],\n",
            "  \"regions\": [\n",
            "    \"Rift Valley\"\n",
            "  ],\n",
            "  \"loans\": 1,\n",
            "  \"total_amount\": 56448.0,\n",
            "  \"avg_amount\": 56448.0,\n",
            "  \"status_counts\": {\n",
            "    \"performing\": 1\n",
            "  },\n",
            "  \"pred_default_prob\": 0.0,\n",
            "  \"shap_preview\": \"{'error': \\\"module 'numpy' has no attribut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuFxvHW3s5Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 8a/9: Admin Tools Core (audit logs, model registry, exports) --------\n",
        "import os, sys, json, joblib, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "ADMIN_DIR = BASE / \"modules\" / \"admin\"\n",
        "LOG_DIR = BASE / \"logs\"\n",
        "ADMIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "admin_py = r\"\"\"\n",
        "import os, json, joblib, datetime\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "LOG_DIR = Path('/content/loan_app/logs')\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Audit Logging ----\n",
        "def audit_log(event, user=\"system\", meta=None):\n",
        "    ts = datetime.datetime.utcnow().isoformat()\n",
        "    log_entry = {\"ts\": ts, \"user\": user, \"event\": event, \"meta\": meta or {}}\n",
        "    logf = LOG_DIR/\"audit.log\"\n",
        "    with open(logf, \"a\") as f:\n",
        "        f.write(json.dumps(log_entry)+\"\\n\")\n",
        "    return log_entry\n",
        "\n",
        "# ---- Model Registry ----\n",
        "MODEL_REGISTRY = LOG_DIR/\"model_registry.json\"\n",
        "\n",
        "def register_model(name, model_path, metrics, user=\"admin\"):\n",
        "    registry = []\n",
        "    if MODEL_REGISTRY.exists():\n",
        "        try:\n",
        "            registry = json.loads(MODEL_REGISTRY.read_text())\n",
        "        except:\n",
        "            registry = []\n",
        "    entry = {\"name\": name, \"model_path\": model_path, \"metrics\": metrics,\n",
        "             \"user\": user, \"ts\": datetime.datetime.utcnow().isoformat()}\n",
        "    registry.append(entry)\n",
        "    MODEL_REGISTRY.write_text(json.dumps(registry, indent=2))\n",
        "    audit_log(\"register_model\", user=user, meta=entry)\n",
        "    return entry\n",
        "\n",
        "def list_models():\n",
        "    if MODEL_REGISTRY.exists():\n",
        "        return json.loads(MODEL_REGISTRY.read_text())\n",
        "    return []\n",
        "\n",
        "def pin_model(name):\n",
        "    models = list_models()\n",
        "    if not models: return None\n",
        "    latest = [m for m in models if m[\"name\"]==name]\n",
        "    if not latest: return None\n",
        "    pinned = latest[-1]\n",
        "    (LOG_DIR/\"pinned_model.json\").write_text(json.dumps(pinned, indent=2))\n",
        "    audit_log(\"pin_model\", meta=pinned)\n",
        "    return pinned\n",
        "\n",
        "def get_pinned_model():\n",
        "    p = LOG_DIR/\"pinned_model.json\"\n",
        "    if p.exists():\n",
        "        return json.loads(p.read_text())\n",
        "    return None\n",
        "\n",
        "# ---- Data Exports ----\n",
        "EXPORTS_DIR = LOG_DIR/\"exports\"\n",
        "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def export_dataset(df, name=\"dataset_export.csv\", fmt=\"csv\"):\n",
        "    outp = EXPORTS_DIR/name\n",
        "    if fmt==\"csv\":\n",
        "        df.to_csv(outp,index=False)\n",
        "    elif fmt==\"json\":\n",
        "        df.to_json(outp,orient=\"records\")\n",
        "    audit_log(\"export_dataset\", meta={\"file\": str(outp), \"fmt\": fmt})\n",
        "    return str(outp)\n",
        "\"\"\"\n",
        "(ADMIN_DIR/\"__init__.py\").write_text(\"# init\\n\")\n",
        "(ADMIN_DIR/\"admin_tools.py\").write_text(admin_py.strip()+\"\\n\")\n",
        "\n",
        "# --- Smoke test ---\n",
        "print(\"Running smoke test for admin tools...\")\n",
        "try:\n",
        "    import importlib\n",
        "    sys.path.append(\"/content/loan_app\")\n",
        "    importlib.invalidate_caches()\n",
        "    adm = importlib.import_module(\"modules.admin.admin_tools\")\n",
        "\n",
        "    # audit log test\n",
        "    e = adm.audit_log(\"smoke_test\", user=\"tester\", meta={\"note\":\"admin tools check\"})\n",
        "    print(\"Audit log entry:\", e)\n",
        "\n",
        "    # fake registry entry\n",
        "    reg = adm.register_model(\"demo_model\",\"/tmp/demo.pkl\",{\"auc\":0.75}, user=\"tester\")\n",
        "    print(\"Registered model:\", reg)\n",
        "\n",
        "    # pin model\n",
        "    pin = adm.pin_model(\"demo_model\")\n",
        "    print(\"Pinned model:\", pin)\n",
        "\n",
        "    # list models\n",
        "    print(\"Model list:\", adm.list_models())\n",
        "\n",
        "    # export dummy dataset\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({\"a\":[1,2,3],\"b\":[4,5,6]})\n",
        "    exp = adm.export_dataset(df,\"demo.csv\")\n",
        "    print(\"Exported dataset:\", exp)\n",
        "\n",
        "    print(\"Cell 8a smoke test: ✅ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 8a smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "lThDO3jnnz4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b8e760-7c9f-43cb-f50d-6b2ed8b499d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running smoke test for admin tools...\n",
            "Audit log entry: {'ts': '2025-09-01T05:16:00.742231', 'user': 'tester', 'event': 'smoke_test', 'meta': {'note': 'admin tools check'}}\n",
            "Registered model: {'name': 'demo_model', 'model_path': '/tmp/demo.pkl', 'metrics': {'auc': 0.75}, 'user': 'tester', 'ts': '2025-09-01T05:16:00.742504'}\n",
            "Pinned model: {'name': 'demo_model', 'model_path': '/tmp/demo.pkl', 'metrics': {'auc': 0.75}, 'user': 'tester', 'ts': '2025-09-01T05:16:00.742504'}\n",
            "Model list: [{'name': 'demo_model', 'model_path': '/tmp/demo.pkl', 'metrics': {'auc': 0.75}, 'user': 'tester', 'ts': '2025-09-01T05:16:00.742504'}]\n",
            "Exported dataset: /content/loan_app/logs/exports/demo.csv\n",
            "Cell 8a smoke test: ✅ OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 8b/9: Global Analytics & Data Products (super-rich) --------\n",
        "import os, sys, warnings, json\n",
        "from pathlib import Path\n",
        "\n",
        "# ensure repo on path\n",
        "sys.path.append(\"/content/loan_app\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "ANALYTICS_DIR = BASE / \"modules\" / \"analytics\"\n",
        "EXPORTS_DIR = BASE / \"logs\" / \"exports\"\n",
        "PLOTS_DIR = BASE / \"logs\" / \"plots\"\n",
        "ANALYTICS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "analytics_py = r\"\"\"\n",
        "# modules.analytics - Global analytics, segments, cohorts, and data products\n",
        "import os, json, math\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from modules import schema\n",
        "from modules.visuals import plot_loan_size_hist, plot_defaults_pie, plot_time_series_by_month\n",
        "from datetime import datetime\n",
        "\n",
        "OUT_DIR = Path('/content/loan_app/logs/exports'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOT_DIR = Path('/content/loan_app/logs/plots'); PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _safe_load_uploads(limit=None):\n",
        "    UP = Path('/content/loan_app/data/uploads')\n",
        "    if not UP.exists(): return pd.DataFrame()\n",
        "    files = sorted(UP.glob('*'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    if limit: files = files[:limit]\n",
        "    dfs=[]\n",
        "    for p in files:\n",
        "        try:\n",
        "            if p.suffix.lower() in ['.csv']: df = pd.read_csv(p)\n",
        "            else: df = pd.read_excel(p)\n",
        "            dfs.append(df)\n",
        "        except Exception:\n",
        "            continue\n",
        "    if not dfs: return pd.DataFrame()\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "def build_master(enforce_enrich=True):\n",
        "    df = _safe_load_uploads(limit=20)\n",
        "    if df.empty:\n",
        "        # fallback: use synth generator if available\n",
        "        try:\n",
        "            from modules.synth.enhanced_generator import make_dataset\n",
        "            df = make_dataset(1000, seed=2024)\n",
        "        except Exception:\n",
        "            return pd.DataFrame()\n",
        "    if enforce_enrich:\n",
        "        X,y,en = schema.prepare_for_ml(df, aggregate_by_client=False)\n",
        "        return en\n",
        "    return df\n",
        "\n",
        "def compute_topline(df):\n",
        "    # topline KPIs\n",
        "    total_loans = len(df)\n",
        "    total_clients = df['unique_client_id'].nunique() if 'unique_client_id' in df.columns else df['national_id'].nunique()\n",
        "    defaults = int(df.get('is_default', pd.Series()).sum()) if 'is_default' in df.columns else int(df.get('loan_status',pd.Series()).astype(str).str.contains('default').sum())\n",
        "    avg_loan = float(df['loan_amount_num'].mean()) if 'loan_amount_num' in df.columns else float(df['loan_amount'].mean())\n",
        "    median_loan = float(df['loan_amount_num'].median()) if 'loan_amount_num' in df.columns else float(df['loan_amount'].median())\n",
        "    female_share = float((df.get('gender','').astype(str)=='female').mean()) if 'gender' in df.columns else None\n",
        "    multi_loan_frac = df.groupby('unique_client_id').size().gt(1).mean() if 'unique_client_id' in df.columns else None\n",
        "    return {\n",
        "        'total_loans': int(total_loans),\n",
        "        'total_clients': int(total_clients),\n",
        "        'defaults': int(defaults),\n",
        "        'default_rate': float(defaults / total_loans) if total_loans>0 else None,\n",
        "        'avg_loan': avg_loan,\n",
        "        'median_loan': median_loan,\n",
        "        'female_share': female_share,\n",
        "        'multi_loan_frac': float(multi_loan_frac) if multi_loan_frac is not None else None\n",
        "    }\n",
        "\n",
        "def segment_stats(df, by=['region','town','product','branch','gender']):\n",
        "    results = {}\n",
        "    for seg in by:\n",
        "        if seg not in df.columns: continue\n",
        "        g = df.groupby(seg).agg(\n",
        "            loans = ('loan_amount_num','count'),\n",
        "            avg_amount = ('loan_amount_num','mean'),\n",
        "            median_amount = ('loan_amount_num','median'),\n",
        "            defaults = ('is_default','sum'),\n",
        "        ).reset_index()\n",
        "        g['default_rate'] = g['defaults'] / g['loans']\n",
        "        results[seg] = g.sort_values('loans', ascending=False)\n",
        "    return results\n",
        "\n",
        "def client_lifetime_metrics(df):\n",
        "    # per-client aggregates that are attractive to buyers\n",
        "    grp = df.groupby('unique_client_id').agg(\n",
        "        loan_count = ('loan_amount_num','count'),\n",
        "        total_borrowed = ('loan_amount_num','sum'),\n",
        "        avg_loan = ('loan_amount_num','mean'),\n",
        "        max_loan = ('loan_amount_num','max'),\n",
        "        defaults = ('is_default','sum'),\n",
        "    ).reset_index()\n",
        "    grp['default_flag'] = (grp['defaults']>0).astype(int)\n",
        "    # add demographic aggregates (first town/region/gender)\n",
        "    firsts = df.sort_values('created_date_parsed').groupby('unique_client_id').first().reset_index()\n",
        "    for c in ['town','region','gender','age_est']:\n",
        "        if c in firsts.columns:\n",
        "            grp[c] = grp['unique_client_id'].map(firsts.set_index('unique_client_id')[c].to_dict())\n",
        "    return grp\n",
        "\n",
        "def cohort_analysis(df, cohort_period='M'):\n",
        "    # cohorts by month of first loan\n",
        "    df['created_date_parsed'] = pd.to_datetime(df['created_date_parsed'], errors='coerce')\n",
        "    first = df.sort_values('created_date_parsed').groupby('unique_client_id')['created_date_parsed'].min().reset_index()\n",
        "    first['cohort'] = first['created_date_parsed'].dt.to_period(cohort_period)\n",
        "    df = df.merge(first[['unique_client_id','cohort']], on='unique_client_id', how='left')\n",
        "    cohort_table = df.groupby(['cohort','is_default']).size().unstack(fill_value=0)\n",
        "    return cohort_table\n",
        "\n",
        "def correlation_and_stats(df):\n",
        "    # numeric correlations for feature discovery\n",
        "    numcols = [c for c in df.columns if c.endswith('_num') or c in ['age_est','installment_size','loan_to_income','days_since_issue']]\n",
        "    if not numcols: return {}\n",
        "    corr = df[numcols].corr().fillna(0)\n",
        "    stats = df[numcols].describe().to_dict()\n",
        "    return {'corr': corr, 'stats': stats}\n",
        "\n",
        "def top_clients(df, n=50):\n",
        "    grp = client_lifetime_metrics(df)\n",
        "    top = grp.sort_values('total_borrowed', ascending=False).head(n)\n",
        "    return top\n",
        "\n",
        "def export_data_products(df, prefix='data_product'):\n",
        "    # export client-level product, segment stats, and raw enriched file\n",
        "    cli = client_lifetime_metrics(df)\n",
        "    seg = segment_stats(df)\n",
        "    now = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "    paths = {}\n",
        "    cli_path = OUT_DIR/f'{prefix}_clients_{now}.csv'; cli.to_csv(cli_path,index=False); paths['clients']=str(cli_path)\n",
        "    # export segment tables\n",
        "    for k,v in seg.items():\n",
        "        p = OUT_DIR/f'{prefix}_segment_{k}_{now}.csv'; v.to_csv(p,index=False); paths[f'segment_{k}']=str(p)\n",
        "    # export raw enriched\n",
        "    rawp = OUT_DIR/f'{prefix}_raw_{now}.csv'; df.to_csv(rawp,index=False); paths['raw']=str(rawp)\n",
        "    return paths\n",
        "\n",
        "def generate_plots(df):\n",
        "    p1 = plot_loan_size_hist(df)\n",
        "    p2 = plot_defaults_pie(df)\n",
        "    p3 = plot_time_series_by_month(df)\n",
        "    return [p1,p2,p3]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# write module\n",
        "(ANALYTICS_DIR / \"__init__.py\").write_text(\"# analytics package\\n\")\n",
        "(ANALYTICS_DIR / \"analytics.py\").write_text(analytics_py.strip() + \"\\n\")\n",
        "\n",
        "# ---------- Smoke Test for analytics ----------\n",
        "print(\"Running Cell 8b smoke test (Global Analytics)...\")\n",
        "try:\n",
        "    import importlib, pandas as pd\n",
        "    importlib.invalidate_caches()\n",
        "    sys.path.append(\"/content/loan_app\")\n",
        "    an = importlib.import_module(\"modules.analytics.analytics\")\n",
        "\n",
        "    # build master enriched DF (uses uploads or falls back to synth)\n",
        "    df = an.build_master(enforce_enrich=True)\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"No data available (uploads missing and synth unavailable)\")\n",
        "\n",
        "    topline = an.compute_topline(df)\n",
        "    print(\"Topline KPIs:\", json.dumps(topline, indent=2))\n",
        "\n",
        "    segs = an.segment_stats(df, by=['region','town','product','branch','gender'])\n",
        "    # print short summary of top 5 regions\n",
        "    if 'region' in segs:\n",
        "        print(\"Top regions by loans:\")\n",
        "        print(segs['region'].head(5).to_string(index=False))\n",
        "\n",
        "    # client lifetime & top clients\n",
        "    clients = an.client_lifetime_metrics(df)\n",
        "    print(\"Client metrics sample (top 5):\")\n",
        "    print(clients.head(5).to_string(index=False))\n",
        "\n",
        "    # cohort table (small preview)\n",
        "    cohort = an.cohort_analysis(df)\n",
        "    print(\"Cohort table shape:\", cohort.shape)\n",
        "\n",
        "    # correlation summary\n",
        "    corr = an.correlation_and_stats(df)\n",
        "    print(\"Numeric features discovered:\", list(corr.get('stats',{}).keys())[:10])\n",
        "\n",
        "    # export data products\n",
        "    paths = an.export_data_products(df, prefix='prod')\n",
        "    print(\"Exported data products to:\", paths)\n",
        "\n",
        "    # generate plots\n",
        "    plots = an.generate_plots(df)\n",
        "    print(\"Saved plots:\", plots)\n",
        "\n",
        "    print(\"Cell 8b smoke test: ✅ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 8b smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1CFF59ita7Z",
        "outputId": "d021bd19-0ee8-4f88-ba2f-ae2490fe7151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Cell 8b smoke test (Global Analytics)...\n",
            "Topline KPIs: {\n",
            "  \"total_loans\": 2584,\n",
            "  \"total_clients\": 1000,\n",
            "  \"defaults\": 416,\n",
            "  \"default_rate\": 0.1609907120743034,\n",
            "  \"avg_loan\": 26561.542569659443,\n",
            "  \"median_loan\": 20884.0,\n",
            "  \"female_share\": 0.6160990712074303,\n",
            "  \"multi_loan_frac\": 1.0\n",
            "}\n",
            "Client metrics sample (top 5):\n",
            "unique_client_id  loan_count  total_borrowed  avg_loan  max_loan  defaults  default_flag     town gender  age_est\n",
            "        32004235           4         35904.0    8976.0    9437.0         0             0  Eldoret female       30\n",
            "        32010645           2         96778.0   48389.0   48389.0         0             0 Machakos   male       29\n",
            "        32013935           2         18636.0    9318.0    9318.0         0             0  Eldoret female       27\n",
            "        32018829           2         33768.0   16884.0   16884.0         0             0  Eldoret female       28\n",
            "        32028871           4         25006.0    6251.5    8340.0         2             1  Eldoret female       29\n",
            "Cohort table shape: (52, 2)\n",
            "Numeric features discovered: ['loan_amount_num', 'income_num', 'installment_size', 'age_est', 'loan_to_income', 'days_since_issue']\n",
            "Exported data products to: {'clients': '/content/loan_app/logs/exports/prod_clients_20250901_051800.csv', 'segment_town': '/content/loan_app/logs/exports/prod_segment_town_20250901_051800.csv', 'segment_product': '/content/loan_app/logs/exports/prod_segment_product_20250901_051800.csv', 'segment_branch': '/content/loan_app/logs/exports/prod_segment_branch_20250901_051800.csv', 'segment_gender': '/content/loan_app/logs/exports/prod_segment_gender_20250901_051800.csv', 'raw': '/content/loan_app/logs/exports/prod_raw_20250901_051800.csv'}\n",
            "Saved plots: ['/content/loan_app/logs/plots/loan_size_hist.png', '/content/loan_app/logs/plots/defaults_pie.png', '/content/loan_app/logs/plots/loans_by_month.png']\n",
            "Cell 8b smoke test: ✅ OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------- Cell 9a/10: Streamlit UI (patched with Refresh Link for Admin) --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "APP_DIR = BASE / \"modules\" / \"streamlit_app\"\n",
        "APP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ui_py = r\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd, importlib, sys\n",
        "sys.path.append('/content/loan_app')\n",
        "\n",
        "from modules import auth, schema\n",
        "from modules.reports import reports\n",
        "from modules.analytics import analytics\n",
        "from modules.admin import admin_tools\n",
        "from modules.visuals import plot_loan_size_hist, plot_defaults_pie, plot_time_series_by_month\n",
        "\n",
        "# Try importing new_link from runner if available\n",
        "try:\n",
        "    from __main__ import new_link\n",
        "except:\n",
        "    new_link = None\n",
        "\n",
        "st.set_page_config(page_title=\"Loan Analytics Portal\", layout=\"wide\")\n",
        "\n",
        "if \"user\" not in st.session_state:\n",
        "    st.session_state.user = None\n",
        "if \"role\" not in st.session_state:\n",
        "    st.session_state.role = None\n",
        "\n",
        "def login():\n",
        "    st.sidebar.subheader(\"Login\")\n",
        "    u = st.sidebar.text_input(\"Username\")\n",
        "    p = st.sidebar.text_input(\"Password\", type=\"password\")\n",
        "    if st.sidebar.button(\"Login\", use_container_width=True):\n",
        "        if auth.check_login(u,p):\n",
        "            st.session_state.user = u\n",
        "            if u.lower()==\"admin\": st.session_state.role=\"admin\"\n",
        "            else: st.session_state.role=\"client\"\n",
        "            admin_tools.audit_log(\"login\", user=u)\n",
        "            st.sidebar.success(f\"Welcome, {u}\")\n",
        "        else:\n",
        "            st.sidebar.error(\"Invalid credentials\")\n",
        "\n",
        "def logout():\n",
        "    st.session_state.user=None\n",
        "    st.session_state.role=None\n",
        "\n",
        "if not st.session_state.user:\n",
        "    st.title(\"Loan Analytics Portal\")\n",
        "    st.markdown(\"Please login to continue.\")\n",
        "    login()\n",
        "else:\n",
        "    st.sidebar.write(f\"Logged in as: {st.session_state.user}\")\n",
        "    if st.sidebar.button(\"Logout\", use_container_width=True):\n",
        "        logout()\n",
        "        st.rerun()\n",
        "\n",
        "    tabs = [\"Dashboard\",\"Reports\",\"Uploads\"]\n",
        "    if st.session_state.role==\"admin\":\n",
        "        tabs += [\"Admin Sandbox\",\"Global Analytics\"]\n",
        "\n",
        "    choice = st.sidebar.radio(\"Navigation\", tabs)\n",
        "\n",
        "    if choice==\"Dashboard\":\n",
        "        st.title(\"📊 Loan Dashboard\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if df.empty:\n",
        "            st.warning(\"No data available. Please upload datasets.\")\n",
        "        else:\n",
        "            kpis = analytics.compute_topline(df)\n",
        "            c1,c2,c3,c4 = st.columns(4)\n",
        "            c1.metric(\"Total Loans\", f\"{kpis['total_loans']:,}\")\n",
        "            c2.metric(\"Clients\", f\"{kpis['total_clients']:,}\")\n",
        "            c3.metric(\"Default Rate\", f\"{kpis['default_rate']*100:.1f}%\")\n",
        "            c4.metric(\"Female Share\", f\"{kpis['female_share']*100:.1f}%\" if kpis['female_share'] else \"-\")\n",
        "\n",
        "            st.subheader(\"Visuals\")\n",
        "            c1,c2,c3 = st.columns(3)\n",
        "            with c1: st.image(plot_loan_size_hist(df))\n",
        "            with c2: st.image(plot_defaults_pie(df))\n",
        "            with c3: st.image(plot_time_series_by_month(df))\n",
        "\n",
        "    elif choice==\"Uploads\":\n",
        "        st.title(\"📤 Upload Dataset\")\n",
        "        f = st.file_uploader(\"Upload CSV\", type=[\"csv\"])\n",
        "        if f is not None:\n",
        "            upath = BASE/\"data\"/\"uploads\"/f.name\n",
        "            pd.read_csv(f).to_csv(upath, index=False)\n",
        "            admin_tools.audit_log(\"upload\", user=st.session_state.user, meta={\"file\":str(upath)})\n",
        "            st.success(f\"Uploaded to {upath}\")\n",
        "\n",
        "    elif choice==\"Reports\":\n",
        "        st.title(\"📑 Client Reports\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if df.empty:\n",
        "            st.warning(\"No data available.\")\n",
        "        else:\n",
        "            cid = st.text_input(\"Enter Client ID\")\n",
        "            if st.button(\"Generate Report\", use_container_width=True):\n",
        "                rep = reports.client_report(df, cid, model_path=None)\n",
        "                st.json(rep)\n",
        "            if st.checkbox(\"Export All Client Reports\"):\n",
        "                outp = reports.export_client_reports(df, model_path=None)\n",
        "                st.success(f\"Exported to {outp}\")\n",
        "\n",
        "    elif choice==\"Admin Sandbox\" and st.session_state.role==\"admin\":\n",
        "        st.title(\"🛠️ Admin Sandbox\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if st.button(\"Train Baseline Model\", use_container_width=True):\n",
        "            X,y,en = schema.prepare_for_ml(df)\n",
        "            eng = importlib.import_module(\"modules.ml.engine\")\n",
        "            res = eng.train_baseline(X,y,name=\"sandbox\")\n",
        "            admin_tools.register_model(\"sandbox\", res['model_path'], res['metrics'])\n",
        "            st.success(f\"Model trained. AUC={res['metrics']['auc']:.3f}\")\n",
        "        if st.button(\"List Models\", use_container_width=True):\n",
        "            st.json(admin_tools.list_models())\n",
        "        if st.button(\"Get Pinned Model\", use_container_width=True):\n",
        "            st.json(admin_tools.get_pinned_model())\n",
        "        if new_link:\n",
        "            if st.button(\"🔄 Refresh Portal Link\", use_container_width=True):\n",
        "                url = new_link()\n",
        "                st.success(f\"New link: {url}\")\n",
        "\n",
        "    elif choice==\"Global Analytics\" and st.session_state.role==\"admin\":\n",
        "        st.title(\"🌍 Global Analytics\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if df.empty:\n",
        "            st.warning(\"No data.\")\n",
        "        else:\n",
        "            topline = analytics.compute_topline(df)\n",
        "            st.subheader(\"Topline KPIs\")\n",
        "            st.json(topline)\n",
        "            st.subheader(\"Segments by Region\")\n",
        "            segs = analytics.segment_stats(df, by=['region'])\n",
        "            if 'region' in segs:\n",
        "                st.dataframe(segs['region'])\n",
        "            st.subheader(\"Top Clients\")\n",
        "            st.dataframe(analytics.top_clients(df, n=20))\n",
        "\"\"\"\n",
        "\n",
        "(APP_DIR/\"main.py\").write_text(ui_py.strip()+\"\\n\")\n",
        "\n",
        "print(\"Cell 9a patched: streamlit_app/main.py ✅ (Admin can refresh portal link inside UI)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLLrH5_Aughf",
        "outputId": "02e68f41-a2f4-480f-90b4-f45d1a944d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 9a patched: streamlit_app/main.py ✅ (Admin can refresh portal link inside UI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 9b/10: Final Runner (ngrok v3 + config fix + watchdog + UI hook) --------\n",
        "import os, time, sys, subprocess, threading\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# --- Force pyngrok to use ngrok v3 binary ---\n",
        "NGROK_BIN = \"/usr/local/bin/ngrok\"\n",
        "conf.get_default().ngrok_path = NGROK_BIN\n",
        "\n",
        "# Confirm ngrok version\n",
        "!$NGROK_BIN --version\n",
        "print(\"✅ Using ngrok binary at:\", NGROK_BIN)\n",
        "\n",
        "# --- Ensure ngrok v3 config path with your authtoken ---\n",
        "os.makedirs(\"/root/.config/ngrok\", exist_ok=True)\n",
        "with open(\"/root/.config/ngrok/ngrok.yml\", \"w\") as f:\n",
        "    f.write(\"authtoken: 31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\\n\")\n",
        "\n",
        "APP_PATH = \"/content/loan_app/modules/streamlit_app/main.py\"\n",
        "assert os.path.exists(APP_PATH), f\"Streamlit app not found: {APP_PATH}\"\n",
        "\n",
        "# Kill any previous tunnels/servers\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "os.system(\"pkill streamlit || true\")\n",
        "\n",
        "public_url = None\n",
        "\n",
        "def start_tunnel():\n",
        "    \"\"\"Start a new ngrok tunnel for Streamlit\"\"\"\n",
        "    global public_url\n",
        "    public_url = ngrok.connect(8501, \"http\").public_url\n",
        "    print(\"🚀 Loan Analytics Portal available at:\", public_url)\n",
        "    print(\"👉 Use credentials: Admin / Shady868 (admin sandbox)\")\n",
        "\n",
        "def new_link():\n",
        "    \"\"\"Manually refresh tunnel (Colab + UI button)\"\"\"\n",
        "    global public_url\n",
        "    try:\n",
        "        if public_url:\n",
        "            ngrok.disconnect(public_url)\n",
        "    except:\n",
        "        pass\n",
        "    start_tunnel()\n",
        "    return public_url\n",
        "\n",
        "# Start initial tunnel\n",
        "start_tunnel()\n",
        "\n",
        "# Launch Streamlit\n",
        "cmd = f\"streamlit run {APP_PATH} --server.port 8501 --server.address 0.0.0.0\"\n",
        "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "\n",
        "# Watchdog: monitor tunnel, auto-restart if needed\n",
        "def tunnel_watchdog():\n",
        "    global public_url\n",
        "    while True:\n",
        "        time.sleep(30)\n",
        "        try:\n",
        "            tunnels = ngrok.get_tunnels()\n",
        "            active = any(t.public_url == public_url for t in tunnels)\n",
        "            if not active:\n",
        "                print(\"⚠️ ngrok tunnel dropped — reconnecting...\")\n",
        "                start_tunnel()\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Tunnel check failed:\", e)\n",
        "            try:\n",
        "                start_tunnel()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "threading.Thread(target=tunnel_watchdog, daemon=True).start()\n",
        "\n",
        "print(\"✅ Streamlit launched. Auto-reconnect active.\")\n",
        "print(\"👉 Call new_link() in Colab OR click '🔄 Refresh Portal Link' in Admin Sandbox to refresh URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "kRZzMudnvpxl",
        "outputId": "38bd9a8e-3ee5-4ea3-f18f-be30a1d54821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok version 3.27.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-01T05:40:18+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using ngrok binary at: /usr/local/bin/ngrok\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process was unable to start.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-40814018.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Start initial tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mstart_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Launch Streamlit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-40814018.py\u001b[0m in \u001b[0;36mstart_tunnel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"\"\"Start a new ngrok tunnel for Streamlit\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Loan Analytics Portal available at:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"👉 Use credentials: Admin / Shady868 (admin sandbox)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bind_tls\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating tunnel with options: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    474\u001b[0m                                     ngrok_process.startup_error)\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPyngrokNgrokError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The ngrok process was unable to start.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process was unable to start."
          ]
        }
      ]
    }
  ]
}