{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsWndyPhuS8NNFF5qFruhm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JBlizzard-sketch/LoanIQ/blob/main/Copy_of_LoanIQ2Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 🚀 LoanIQ Bootstrap Cell (Run after reset)\n",
        "# ============================================\n",
        "\n",
        "# --- Step 1: Install dependencies ---\n",
        "!pip install faker imbalanced-learn shap xgboost streamlit\n",
        "\n",
        "# --- Step 2: Create folder scaffolding ---\n",
        "import os\n",
        "\n",
        "folders = [\n",
        "    \"modules/bootstrap\", \"modules/core\", \"modules/ingestion\", \"modules/synth\",\n",
        "    \"modules/features\", \"modules/ml\", \"modules/reports\", \"modules/sandbox\", \"modules/api\",\n",
        "    \"pages\", \"tests\", \"data\", \"config\"\n",
        "]\n",
        "for f in folders:\n",
        "    os.makedirs(f, exist_ok=True)\n",
        "\n",
        "# Touch __init__.py files\n",
        "for f in [\"modules\"] + [f\"modules/{d}\" for d in [\"bootstrap\",\"core\",\"ingestion\",\"synth\",\"features\",\"ml\",\"reports\",\"sandbox\",\"api\"]]:\n",
        "    open(os.path.join(f, \"__init__.py\"), \"w\").close()\n",
        "open(\"tests/__init__.py\", \"w\").close()\n",
        "\n",
        "# --- Step 3: Write modules/synth/generators.py ---\n",
        "generators_code = r'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "faker = Faker([\"en_US\"])  # use global dataset, we'll override with Kenyan names manually\n",
        "\n",
        "kenyan_first_names = [\n",
        "    \"Achieng\", \"Atieno\", \"Wanjiku\", \"Njeri\", \"Chebet\", \"Kiptoo\", \"Otieno\", \"Kamau\",\n",
        "    \"Mwangi\", \"Omondi\", \"Kipchoge\", \"Cherono\", \"Mutiso\", \"Nduta\", \"Nyambura\", \"Wairimu\"\n",
        "]\n",
        "kenyan_last_names = [\n",
        "    \"Ouma\", \"Mutua\", \"Koech\", \"Barasa\", \"Kiprotich\", \"Maina\", \"Otieno\", \"Kamau\",\n",
        "    \"Mwangi\", \"Njoroge\", \"Omondi\", \"Chege\", \"Kariuki\", \"Kipkorir\"\n",
        "]\n",
        "towns = [\n",
        "    \"Nairobi\",\"Mombasa\",\"Kisumu\",\"Nakuru\",\"Eldoret\",\"Meru\",\"Nyeri\",\"Machakos\",\"Thika\",\"Kitale\",\n",
        "    \"Kericho\",\"Embu\",\"Garissa\",\"Isiolo\",\"Kilifi\",\"Lamu\",\"Voi\",\"Narok\",\"Naivasha\",\"Kakamega\"\n",
        "]\n",
        "\n",
        "products = {\n",
        "    \"Inuka\":5, \"Kuza\":4, \"Fadhili\":6, \"Imara\":8, \"Boreshwa\":12\n",
        "}\n",
        "loan_types = [\"Normal\",\"Top-up\",\"Emergency\",\"Business\"]\n",
        "statuses = [\"Active\",\"Pending Approval\"]\n",
        "health_states = [\"performing\",\"watch\",\"non-performing\"]\n",
        "\n",
        "def guess_age_from_id(gov_id:str) -> int:\n",
        "    try:\n",
        "        num = int(gov_id[:2])\n",
        "        if num <= 15: return random.randint(55,70)\n",
        "        if num <= 25: return random.randint(40,55)\n",
        "        if num <= 33: return random.randint(28,40)\n",
        "        return random.randint(18,28)\n",
        "    except:\n",
        "        return random.randint(18,60)\n",
        "\n",
        "def generate_clients_loans(n_rows=1000, seed=None, default_rate=0.15, gender_ratio=0.6):\n",
        "    if seed: np.random.seed(seed); random.seed(seed)\n",
        "    rows = []\n",
        "    for i in range(n_rows):\n",
        "        fname = random.choice(kenyan_first_names)\n",
        "        lname = random.choice(kenyan_last_names)\n",
        "        name = f\"{fname} {lname}\"\n",
        "        gov_id = str(random.randint(20000000, 40000000))\n",
        "        age = guess_age_from_id(gov_id)\n",
        "        gender = \"F\" if random.random() < gender_ratio else \"M\"\n",
        "        branch = random.choice(towns)\n",
        "        product = random.choice(list(products.keys()))\n",
        "        product_weeks = products[product]\n",
        "        amount = random.randint(5000, 50000)\n",
        "        loan_type = random.choice(loan_types)\n",
        "        status = random.choice(statuses)\n",
        "        health = \"default\" if random.random() < default_rate else \"performing\"\n",
        "        debt_to_income = round(random.uniform(0.1, 1.5), 2)\n",
        "        created_date = faker.date_between(start_date=\"-2y\", end_date=\"today\")\n",
        "        rows.append([f\"CUST{i:05d}\", name, gov_id, age, gender, branch,\n",
        "                     product, product_weeks, amount, loan_type, status, health,\n",
        "                     debt_to_income, created_date])\n",
        "    return pd.DataFrame(rows, columns=[\n",
        "        \"customer_id\",\"customer_name\",\"gov_id\",\"age\",\"gender\",\"branch\",\n",
        "        \"product\",\"product_weeks\",\"loan_amount\",\"loan_type\",\"status\",\n",
        "        \"default\",\"debt_to_income\",\"created_date\"\n",
        "    ])\n",
        "'''\n",
        "with open(\"modules/synth/generators.py\",\"w\") as f: f.write(generators_code)\n",
        "\n",
        "# --- Step 4: Write modules/ml/engine.py ---\n",
        "engine_code = r'''\n",
        "import os, joblib\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import shap\n",
        "import xgboost as xgb\n",
        "\n",
        "def train_model(df, algo=\"LogReg\", use_smote=True, test_size=0.2, random_state=42):\n",
        "    if \"default\" not in df.columns:\n",
        "        raise ValueError(\"Dataset must contain 'default' column (0/1).\")\n",
        "\n",
        "    X = df.drop(columns=[\"default\",\"customer_id\",\"customer_name\",\"gov_id\",\"created_date\"])\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "    y = df[\"default\"].apply(lambda v: 1 if v!=\"performing\" else 0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,random_state=random_state)\n",
        "\n",
        "    if use_smote:\n",
        "        sm = SMOTE(random_state=random_state)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    if algo==\"LogReg\":\n",
        "        model = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=1000))])\n",
        "    elif algo==\"SGD\":\n",
        "        model = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SGDClassifier(loss=\"log_loss\", max_iter=1000))])\n",
        "    elif algo==\"XGBoost\":\n",
        "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "    elif algo==\"HybridBlend\":\n",
        "        logreg = LogisticRegression(max_iter=1000)\n",
        "        xgbc = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "        logreg.fit(X_train,y_train)\n",
        "        xgbc.fit(X_train,y_train)\n",
        "        def hybrid_predict(X): return (0.5*logreg.predict_proba(X)[:,1] + 0.5*xgbc.predict_proba(X)[:,1])\n",
        "        model = (logreg, xgbc, hybrid_predict)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown algo {algo}\")\n",
        "\n",
        "    if algo!=\"HybridBlend\": model.fit(X_train,y_train)\n",
        "\n",
        "    if algo==\"HybridBlend\":\n",
        "        preds = (model[2](X_test) > 0.5).astype(int)\n",
        "        probs = model[2](X_test)\n",
        "    else:\n",
        "        preds = model.predict(X_test)\n",
        "        probs = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy_score(y_test,preds),\n",
        "        \"Precision\": precision_score(y_test,preds,zero_division=0),\n",
        "        \"Recall\": recall_score(y_test,preds,zero_division=0),\n",
        "        \"AUC\": roc_auc_score(y_test,probs)\n",
        "    }\n",
        "\n",
        "    path = f\"models/{algo}_model.pkl\"\n",
        "    os.makedirs(\"models\",exist_ok=True)\n",
        "    joblib.dump(model, path)\n",
        "\n",
        "    try:\n",
        "        if algo==\"HybridBlend\":\n",
        "            explainer = shap.Explainer(model[1])\n",
        "            shap_values = explainer(X_test)\n",
        "        else:\n",
        "            explainer = shap.Explainer(model, X_test)\n",
        "            shap_values = explainer(X_test)\n",
        "    except Exception:\n",
        "        shap_values = None\n",
        "\n",
        "    return metrics, shap_values, path\n",
        "'''\n",
        "with open(\"modules/ml/engine.py\",\"w\") as f: f.write(engine_code)\n",
        "\n",
        "# --- Step 5: Write Streamlit page placeholders ---\n",
        "for i in range(1,8):\n",
        "    with open(f\"pages/{i:02d}_placeholder.py\",\"w\") as f:\n",
        "        f.write(f\"import streamlit as st\\nst.title('Placeholder Tab {i}')\\n\")\n",
        "\n",
        "# --- Step 6: Confirm setup ---\n",
        "print(\"✅ LoanIQ environment bootstrapped successfully!\")\n",
        "for root, dirs, files in os.walk(\"modules\"):\n",
        "    print(root, files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS5BiuxYASaM",
        "outputId": "223caf42-dc14-47ef-ee9f-b881a6090ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.6.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.48.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.2)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Downloading faker-37.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m130.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker, pydeck, streamlit\n",
            "Successfully installed faker-37.6.0 pydeck-0.9.1 streamlit-1.49.1\n",
            "✅ LoanIQ environment bootstrapped successfully!\n",
            "modules ['__init__.py']\n",
            "modules/features ['__init__.py']\n",
            "modules/api ['__init__.py']\n",
            "modules/ml ['engine.py', '__init__.py']\n",
            "modules/ingestion ['__init__.py']\n",
            "modules/synth ['generators.py', '__init__.py']\n",
            "modules/sandbox ['__init__.py']\n",
            "modules/reports ['__init__.py']\n",
            "modules/bootstrap ['__init__.py']\n",
            "modules/core ['__init__.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Recreate the tab structure (must always exist before referencing tabs[x])\n",
        "tabs = st.tabs([\n",
        "    \"Data Ingestion\",\n",
        "    \"Client Onboarding\",\n",
        "    \"Feature Engineering\",\n",
        "    \"Model Training\",\n",
        "    \"Reports\",\n",
        "    \"Audit Logs\",\n",
        "    \"Sandbox\"\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9lZNHAJDN5u",
        "outputId": "67c9d342-f3a3-4d05-dabe-1811a95c9d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 12:15:38.497 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:15:38.499 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:15:38.505 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:15:38.506 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:15:38.508 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:15:38.509 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:15:38.511 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:15:38.514 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tab: Full Sandbox Control Center ---\n",
        "with tabs[6]:\n",
        "    import importlib, joblib, os\n",
        "    import modules.synth.generators as g\n",
        "    import modules.ml.engine as engine\n",
        "    importlib.reload(engine)\n",
        "\n",
        "    st.subheader(\"🧪 LoanIQ Sandbox Control Center\")\n",
        "\n",
        "    # =====================\n",
        "    # SECTION 1: Data Config\n",
        "    # =====================\n",
        "    st.markdown(\"### 📂 Data Configuration\")\n",
        "    with st.expander(\"Synthetic Data Parameters\", expanded=True):\n",
        "        n_rows = st.slider(\"Dataset Size\", 100, 20000, 2000, 100)\n",
        "        default_rate = st.slider(\"Default Rate (%)\", 0, 50, 15, 1) / 100\n",
        "        gender_ratio = st.slider(\"Female Ratio\", 0.0, 1.0, 0.6, 0.05)\n",
        "        branch_count = st.slider(\"Number of Branches\", 5, 100, 50, 5)\n",
        "        seed = st.number_input(\"Random Seed\", value=42, step=1)\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Or upload real client dataset (CSV)\", type=[\"csv\"])\n",
        "\n",
        "    # =====================\n",
        "    # SECTION 2: Model Config\n",
        "    # =====================\n",
        "    st.markdown(\"### 🤖 Model Configuration\")\n",
        "    with st.expander(\"Select & Configure Models\", expanded=True):\n",
        "        model_choice = st.multiselect(\n",
        "            \"Choose Models to Test\",\n",
        "            [\"LogReg\", \"SGD\", \"XGBoost\", \"HybridBlend\"],\n",
        "            default=[\"LogReg\", \"XGBoost\"]\n",
        "        )\n",
        "\n",
        "        st.write(\"⚙️ Hyperparameters\")\n",
        "        logreg_C = st.slider(\"LogReg Regularization (C)\", 0.01, 10.0, 1.0)\n",
        "        sgd_alpha = st.slider(\"SGD Alpha\", 0.0001, 0.1, 0.001, 0.0001)\n",
        "        xgb_lr = st.slider(\"XGBoost Learning Rate\", 0.01, 0.5, 0.1)\n",
        "        xgb_depth = st.slider(\"XGBoost Max Depth\", 2, 12, 6)\n",
        "\n",
        "        # Pin model\n",
        "        pin_model = st.selectbox(\"📌 Pin Model to Production\", [\"None\"] + model_choice)\n",
        "\n",
        "        # AB Testing toggle\n",
        "        ab_test = st.checkbox(\"Run A/B Test Across Models\", value=True)\n",
        "\n",
        "    # =====================\n",
        "    # SECTION 3: Actions\n",
        "    # =====================\n",
        "    if st.button(\"🚀 Run Sandbox\"):\n",
        "        # --- Data ---\n",
        "        if uploaded_file:\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"✅ Loaded real dataset: {df.shape}\")\n",
        "        else:\n",
        "            df = g.generate_clients_loans(\n",
        "                n_rows=n_rows,\n",
        "                default_rate=default_rate,\n",
        "                gender_ratio=gender_ratio,\n",
        "                branch_count=branch_count,\n",
        "                seed=seed,\n",
        "            )\n",
        "            st.success(f\"✅ Synthetic dataset generated: {df.shape}\")\n",
        "        st.dataframe(df.head())\n",
        "\n",
        "        # --- Models ---\n",
        "        st.subheader(\"📊 Model Results\")\n",
        "        results = {}\n",
        "        for algo in model_choice:\n",
        "            try:\n",
        "                model_path = f\"models/{algo}_model.pkl\"\n",
        "                if not os.path.exists(model_path):\n",
        "                    st.warning(f\"⚠️ {algo} not trained yet.\")\n",
        "                    continue\n",
        "\n",
        "                model = joblib.load(model_path)\n",
        "                X = df.drop(columns=[\"default\"]) if \"default\" in df.columns else df.copy()\n",
        "\n",
        "                preds = model.predict(X)\n",
        "                try:\n",
        "                    scores = model.predict_proba(X)[:, 1]\n",
        "                except:\n",
        "                    scores = preds\n",
        "\n",
        "                df[f\"{algo}_pred\"] = preds\n",
        "                df[f\"{algo}_score\"] = scores\n",
        "                results[algo] = (preds, scores)\n",
        "\n",
        "                st.write(f\"### {algo}\")\n",
        "                st.bar_chart(df[f\"{algo}_pred\"].value_counts())\n",
        "                st.line_chart(df[f\"{algo}_score\"].head(50))\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error running {algo}: {e}\")\n",
        "\n",
        "        # --- Governance ---\n",
        "        if pin_model != \"None\":\n",
        "            with open(\"models/production_model.txt\", \"w\") as f:\n",
        "                f.write(pin_model)\n",
        "            st.success(f\"📌 Pinned {pin_model} as production model.\")\n",
        "\n",
        "    # =====================\n",
        "    # SECTION 4: Drilldown\n",
        "    # =====================\n",
        "    st.markdown(\"### 🔍 Client Drilldown\")\n",
        "    client_id = st.text_input(\"Enter Client ID to Inspect\")\n",
        "    if client_id and \"df\" in locals():\n",
        "        row = df[df[\"customer_id\"].astype(str) == str(client_id)]\n",
        "        if not row.empty:\n",
        "            st.write(\"Client Record:\", row.T)\n",
        "            st.write(\"Predictions:\", {m: row[f\"{m}_pred\"].values[0] for m in model_choice if f\"{m}_pred\" in row})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYcxhK1BC2sP",
        "outputId": "6f4b1ba6-655b-4837-fc90-c9f624c4f557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 12:16:09.924 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.095 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-08-31 12:16:10.096 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.097 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.099 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.101 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.102 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.104 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.105 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.106 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.108 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.122 Session state does not function when running a script without `streamlit run`\n",
            "2025-08-31 12:16:10.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.144 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.145 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.145 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.146 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.146 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.147 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.148 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.148 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.165 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.165 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.167 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.182 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.183 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.184 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.184 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.185 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.187 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.190 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.191 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 12:16:10.194 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ONE-CELL PATCH: Full Admin Sandbox page with training, versioning, A/B, SHAP, audit logs ---\n",
        "\n",
        "import os, json, sqlite3, textwrap, datetime as dt\n",
        "\n",
        "# Ensure folders\n",
        "os.makedirs(\"pages\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Write/overwrite the Admin Sandbox Streamlit page\n",
        "sandbox_code = r'''\n",
        "import os, io, json, glob, sqlite3, importlib, datetime as dt\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "# --- Safe imports of our modules (reload to pick latest code) ---\n",
        "import modules.synth.generators as g\n",
        "import modules.ml.engine as engine\n",
        "import importlib\n",
        "importlib.reload(g)\n",
        "importlib.reload(engine)\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "AUDIT_DB   = \"data/audit.db\"\n",
        "PROD_PIN   = os.path.join(MODELS_DIR, \"PROD_MODEL.txt\")\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "def ensure_audit_db():\n",
        "    os.makedirs(os.path.dirname(AUDIT_DB), exist_ok=True)\n",
        "    with sqlite3.connect(AUDIT_DB) as conn:\n",
        "        conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audit_logs(\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            ts TEXT NOT NULL,\n",
        "            action TEXT NOT NULL,\n",
        "            details TEXT\n",
        "        )\"\"\")\n",
        "        conn.commit()\n",
        "\n",
        "def log_action(action: str, details: dict):\n",
        "    ensure_audit_db()\n",
        "    with sqlite3.connect(AUDIT_DB) as conn:\n",
        "        conn.execute(\n",
        "            \"INSERT INTO audit_logs (ts, action, details) VALUES (?, ?, ?)\",\n",
        "            (dt.datetime.utcnow().isoformat(timespec=\"seconds\")+\"Z\", action, json.dumps(details, default=str))\n",
        "        )\n",
        "        conn.commit()\n",
        "\n",
        "def list_model_files():\n",
        "    files = sorted(glob.glob(os.path.join(MODELS_DIR, \"*.pkl\")))\n",
        "    # attach simple meta if present\n",
        "    out = []\n",
        "    for f in files:\n",
        "        meta_path = f.replace(\".pkl\", \".meta.json\")\n",
        "        meta = {}\n",
        "        if os.path.exists(meta_path):\n",
        "            try:\n",
        "                with open(meta_path, \"r\") as fh:\n",
        "                    meta = json.load(fh)\n",
        "            except Exception:\n",
        "                meta = {}\n",
        "        out.append({\"file\": f, \"meta\": meta})\n",
        "    return out\n",
        "\n",
        "def read_prod_pin():\n",
        "    try:\n",
        "        with open(PROD_PIN, \"r\") as fh:\n",
        "            return fh.read().strip()\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "\n",
        "def write_prod_pin(path: str):\n",
        "    with open(PROD_PIN, \"w\") as fh:\n",
        "        fh.write(path)\n",
        "\n",
        "def load_uploaded_csv(uploaded):\n",
        "    # Accept csv/xlsx\n",
        "    if uploaded is None:\n",
        "        return None\n",
        "    name = uploaded.name.lower()\n",
        "    if name.endswith(\".csv\"):\n",
        "        return pd.read_csv(uploaded)\n",
        "    if name.endswith(\".xlsx\") or name.endswith(\".xls\"):\n",
        "        return pd.read_excel(uploaded)\n",
        "    # Try CSV as fallback\n",
        "    try:\n",
        "        return pd.read_csv(uploaded)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- PAGE LAYOUT ----------\n",
        "st.title(\"🧪 Admin Sandbox — Loan IQ\")\n",
        "\n",
        "tabs = st.tabs([\n",
        "    \"📦 Dataset\",\n",
        "    \"🤖 Train One Model\",\n",
        "    \"📌 Versions\",\n",
        "    \"🆚 A/B Test\",\n",
        "    \"🔍 Explainability\",\n",
        "    \"🧾 Audit Logs\"\n",
        "])\n",
        "\n",
        "# === Tab 0: Dataset ===\n",
        "with tabs[0]:\n",
        "    st.subheader(\"Dataset Source\")\n",
        "\n",
        "    src = st.radio(\"Choose dataset source:\", [\"Generate (synthetic)\", \"Upload file\", \"Pick batch file\"], horizontal=True)\n",
        "\n",
        "    df = None\n",
        "    dataset_info = {}\n",
        "\n",
        "    if src == \"Generate (synthetic)\":\n",
        "        c1, c2, c3 = st.columns(3)\n",
        "        with c1:\n",
        "            n_rows = st.number_input(\"Rows\", 1000, 200000, 5000, step=500)\n",
        "            default_rate = st.slider(\"Default rate\", 0.0, 0.5, 0.12, 0.01)\n",
        "        with c2:\n",
        "            gender_ratio = st.slider(\"Female ratio\", 0.0, 1.0, 0.65, 0.01)\n",
        "            mean_dti     = st.slider(\"Mean DTI\", 0.05, 1.0, 0.35, 0.01)\n",
        "        with c3:\n",
        "            seed = st.number_input(\"Random seed\", 0, 10_000, 42, step=1)\n",
        "            # optional loan sizing knob (multiplier)\n",
        "            loan_scale = st.slider(\"Loan size scale\", 0.5, 3.0, 1.0, 0.1)\n",
        "\n",
        "        if st.button(\"🚀 Generate\"):\n",
        "            df = g.generate_clients_loans(\n",
        "                n_rows=int(n_rows),\n",
        "                default_rate=float(default_rate),\n",
        "                gender_ratio=float(gender_ratio),\n",
        "                mean_dti=float(mean_dti),\n",
        "                seed=int(seed),\n",
        "                loan_scale=float(loan_scale)\n",
        "            )\n",
        "            st.success(f\"Generated dataset: {df.shape}\")\n",
        "            st.dataframe(df.head(50), use_container_width=True)\n",
        "            dataset_info = {\"source\":\"synthetic\",\"rows\":len(df),\"params\":{\"default_rate\":default_rate,\"gender_ratio\":gender_ratio,\"mean_dti\":mean_dti,\"seed\":seed,\"loan_scale\":loan_scale}}\n",
        "\n",
        "    elif src == \"Upload file\":\n",
        "        uploaded = st.file_uploader(\"Upload CSV/XLSX\", type=[\"csv\",\"xlsx\",\"xls\"])\n",
        "        if uploaded:\n",
        "            df = load_uploaded_csv(uploaded)\n",
        "            if df is not None and len(df) > 0:\n",
        "                st.success(f\"Loaded dataset: {df.shape}\")\n",
        "                st.dataframe(df.head(50), use_container_width=True)\n",
        "                dataset_info = {\"source\":\"upload\",\"name\":uploaded.name,\"rows\":len(df)}\n",
        "            else:\n",
        "                st.error(\"Could not read the uploaded file. Please upload CSV/XLSX.\")\n",
        "\n",
        "    else:  # Pick batch file\n",
        "        batch_files = sorted(glob.glob(\"data/sandbox_batches/*.csv\"))\n",
        "        if not batch_files:\n",
        "            st.info(\"No batch files in data/sandbox_batches/. Generate batches from the synth module.\")\n",
        "        else:\n",
        "            pick = st.selectbox(\"Choose batch CSV\", batch_files)\n",
        "            if st.button(\"📥 Load batch\"):\n",
        "                df = pd.read_csv(pick)\n",
        "                st.success(f\"Loaded dataset: {df.shape}\")\n",
        "                st.dataframe(df.head(50), use_container_width=True)\n",
        "                dataset_info = {\"source\":\"batch\",\"file\":pick,\"rows\":len(df)}\n",
        "\n",
        "    # Save dataset to session state for other tabs\n",
        "    if df is not None:\n",
        "        st.session_state[\"sandbox_df\"] = df\n",
        "        st.session_state[\"dataset_info\"] = dataset_info\n",
        "\n",
        "# === Tab 1: Train One Model ===\n",
        "with tabs[1]:\n",
        "    st.subheader(\"Train a Single Model with Controls\")\n",
        "    df = st.session_state.get(\"sandbox_df\", None)\n",
        "    if df is None:\n",
        "        st.warning(\"Load or generate a dataset in the 'Dataset' tab first.\")\n",
        "    else:\n",
        "        algo = st.selectbox(\"Algorithm\", [\"LogReg\", \"SGD\", \"XGBoost\", \"HybridBlend\"])\n",
        "\n",
        "        # Common options\n",
        "        use_smote = st.checkbox(\"Use SMOTE (balance classes)\", value=True)\n",
        "\n",
        "        # Per-model hyperparameters\n",
        "        hp = {}\n",
        "        if algo == \"LogReg\":\n",
        "            c1, c2 = st.columns(2)\n",
        "            with c1:\n",
        "                hp[\"C\"] = st.number_input(\"C (inverse regularization)\", 0.0001, 1000.0, 1.0, step=0.1)\n",
        "                hp[\"max_iter\"] = st.number_input(\"max_iter\", 100, 10000, 200, step=50)\n",
        "            with c2:\n",
        "                hp[\"penalty\"] = st.selectbox(\"penalty\", [\"l2\",\"none\"])\n",
        "                hp[\"fit_intercept\"] = st.checkbox(\"fit_intercept\", True)\n",
        "\n",
        "        elif algo == \"SGD\":\n",
        "            c1, c2 = st.columns(2)\n",
        "            with c1:\n",
        "                hp[\"alpha\"] = st.number_input(\"alpha\", 1e-6, 1.0, 0.0001, format=\"%.6f\")\n",
        "                hp[\"max_iter\"] = st.number_input(\"max_iter\", 100, 5000, 1000, step=100)\n",
        "            with c2:\n",
        "                hp[\"loss\"] = st.selectbox(\"loss\", [\"log_loss\",\"modified_huber\",\"hinge\"])\n",
        "                hp[\"fit_intercept\"] = st.checkbox(\"fit_intercept\", True)\n",
        "\n",
        "        elif algo == \"XGBoost\":\n",
        "            c1, c2, c3 = st.columns(3)\n",
        "            with c1:\n",
        "                hp[\"n_estimators\"] = st.number_input(\"n_estimators\", 50, 2000, 300, step=50)\n",
        "                hp[\"max_depth\"] = st.number_input(\"max_depth\", 1, 12, 4, step=1)\n",
        "            with c2:\n",
        "                hp[\"learning_rate\"] = st.slider(\"learning_rate\", 0.01, 0.5, 0.1, 0.01)\n",
        "                hp[\"subsample\"] = st.slider(\"subsample\", 0.5, 1.0, 0.9, 0.05)\n",
        "            with c3:\n",
        "                hp[\"colsample_bytree\"] = st.slider(\"colsample_bytree\", 0.5, 1.0, 0.9, 0.05)\n",
        "                hp[\"reg_lambda\"] = st.slider(\"reg_lambda\", 0.0, 2.0, 1.0, 0.1)\n",
        "\n",
        "        else:  # HybridBlend\n",
        "            st.info(\"Hybrid uses internal optimized weights. Train its components too for best effect.\")\n",
        "            # optional blend weights\n",
        "            c1, c2, c3 = st.columns(3)\n",
        "            with c1:\n",
        "                hp[\"w_logreg\"] = st.slider(\"w_logreg\", 0.0, 1.0, 0.33, 0.01)\n",
        "            with c2:\n",
        "                hp[\"w_sgd\"] = st.slider(\"w_sgd\", 0.0, 1.0, 0.33, 0.01)\n",
        "            with c3:\n",
        "                hp[\"w_xgb\"] = st.slider(\"w_xgb\", 0.0, 1.0, 0.34, 0.01)\n",
        "\n",
        "        if st.button(\"🏋️ Train Model\"):\n",
        "            try:\n",
        "                metrics, shap_values, model_path = engine.train_model(\n",
        "                    df.copy(), algo,\n",
        "                    use_smote=use_smote,\n",
        "                    **hp\n",
        "                )\n",
        "                st.success(f\"Saved model: {model_path}\")\n",
        "                st.json(metrics)\n",
        "\n",
        "                # Save metadata next to model\n",
        "                meta = {\n",
        "                    \"algo\": algo,\n",
        "                    \"use_smote\": use_smote,\n",
        "                    \"hyperparams\": hp,\n",
        "                    \"metrics\": metrics,\n",
        "                    \"dataset_info\": st.session_state.get(\"dataset_info\", {}),\n",
        "                    \"trained_at_utc\": dt.datetime.utcnow().isoformat(timespec=\"seconds\")+\"Z\"\n",
        "                }\n",
        "                meta_path = model_path.replace(\".pkl\", \".meta.json\")\n",
        "                with open(meta_path, \"w\") as fh:\n",
        "                    json.dump(meta, fh, indent=2)\n",
        "                st.caption(f\"Metadata written → {meta_path}\")\n",
        "\n",
        "                log_action(\"train_model\", {\"model_path\": model_path, **meta})\n",
        "            except Exception as e:\n",
        "                st.error(f\"Training failed: {e}\")\n",
        "\n",
        "# === Tab 2: Versions (Pin / Load) ===\n",
        "with tabs[2]:\n",
        "    st.subheader(\"Model Versions & Pinning\")\n",
        "    models = list_model_files()\n",
        "    if not models:\n",
        "        st.info(\"No saved models yet. Train one in the previous tab.\")\n",
        "    else:\n",
        "        prod = read_prod_pin()\n",
        "        st.caption(f\"Current PROD pin: {prod if prod else 'None'}\")\n",
        "        options = [m[\"file\"] for m in models]\n",
        "        sel = st.selectbox(\"Select a model file to pin\", options)\n",
        "        if st.button(\"📌 Pin as PROD\"):\n",
        "            write_prod_pin(sel)\n",
        "            st.success(f\"Pinned PROD → {sel}\")\n",
        "            log_action(\"pin_model\", {\"pinned\": sel})\n",
        "\n",
        "        st.markdown(\"#### Model Catalog\")\n",
        "        for m in models:\n",
        "            st.write(\"**File:**\", m[\"file\"])\n",
        "            st.json(m[\"meta\"])\n",
        "\n",
        "# === Tab 3: A/B Test ===\n",
        "with tabs[3]:\n",
        "    st.subheader(\"Compare Two Models on a Dataset (A/B)\")\n",
        "    models = list_model_files()\n",
        "    if len(models) < 2:\n",
        "        st.info(\"Need at least two models saved.\")\n",
        "    else:\n",
        "        opt = [m[\"file\"] for m in models]\n",
        "        c1, c2 = st.columns(2)\n",
        "        with c1:\n",
        "            mA = st.selectbox(\"Model A\", opt, key=\"abA\")\n",
        "        with c2:\n",
        "            mB = st.selectbox(\"Model B\", opt, key=\"abB\")\n",
        "\n",
        "        src = st.radio(\"Dataset for test\", [\"Use current dataset\", \"Upload CSV/XLSX\"], horizontal=True)\n",
        "        df_test = None\n",
        "        if src == \"Use current dataset\":\n",
        "            df_test = st.session_state.get(\"sandbox_df\", None)\n",
        "            if df_test is None:\n",
        "                st.warning(\"No dataset in session. Load/generate one in 'Dataset' tab.\")\n",
        "        else:\n",
        "            up = st.file_uploader(\"Upload test CSV/XLSX\", type=[\"csv\",\"xlsx\",\"xls\"], key=\"ab_upload\")\n",
        "            if up:\n",
        "                df_test = load_uploaded_csv(up)\n",
        "\n",
        "        if st.button(\"🔬 Run A/B Test\") and df_test is not None:\n",
        "            try:\n",
        "                res = engine.compare_models(df_test.copy(), mA, mB)\n",
        "                st.success(\"A/B complete.\")\n",
        "                st.json(res)\n",
        "                log_action(\"ab_test\", {\"modelA\": mA, \"modelB\": mB, \"result\": res, \"rows\": len(df_test)})\n",
        "            except Exception as e:\n",
        "                st.error(f\"A/B failed: {e}\")\n",
        "\n",
        "# === Tab 4: Explainability ===\n",
        "with tabs[4]:\n",
        "    st.subheader(\"Explainability (SHAP)\")\n",
        "    models = list_model_files()\n",
        "    if not models:\n",
        "        st.info(\"Train a model first.\")\n",
        "    else:\n",
        "        pick = st.selectbox(\"Pick a model to inspect\", [m[\"file\"] for m in models])\n",
        "        df = st.session_state.get(\"sandbox_df\", None)\n",
        "        if df is None:\n",
        "            st.warning(\"Load or generate a dataset in 'Dataset' first (used for background SHAP).\")\n",
        "        else:\n",
        "            try:\n",
        "                fig = engine.shap_summary_plot(df.copy(), pick)\n",
        "                if fig is None:\n",
        "                    st.info(\"SHAP summary not available for this model, or no numeric features.\")\n",
        "                else:\n",
        "                    st.pyplot(fig)\n",
        "            except Exception as e:\n",
        "                st.error(f\"SHAP summary failed: {e}\")\n",
        "\n",
        "# === Tab 5: Audit Logs ===\n",
        "with tabs[5]:\n",
        "    st.subheader(\"Audit Logs\")\n",
        "    ensure_audit_db()\n",
        "    with sqlite3.connect(AUDIT_DB) as conn:\n",
        "        df_logs = pd.read_sql(\"SELECT * FROM audit_logs ORDER BY id DESC LIMIT 500\", conn)\n",
        "    st.dataframe(df_logs, use_container_width=True)\n",
        "'''\n",
        "\n",
        "page_path = \"pages/04_Admin_Sandbox.py\"\n",
        "with open(page_path, \"w\") as f:\n",
        "    f.write(sandbox_code)\n",
        "\n",
        "# Minimal smoke test: ensure file exists and audit DB can be created\n",
        "def _smoke():\n",
        "    assert os.path.exists(page_path), \"Sandbox page was not written\"\n",
        "    # init audit DB\n",
        "    with sqlite3.connect(\"data/audit.db\") as conn:\n",
        "        conn.execute(\"CREATE TABLE IF NOT EXISTS audit_logs (id INTEGER PRIMARY KEY, ts TEXT, action TEXT, details TEXT)\")\n",
        "        conn.commit()\n",
        "    return True\n",
        "\n",
        "ok = _smoke()\n",
        "print(\"✅ Admin Sandbox page written →\", page_path)\n",
        "print(\"✅ Audit DB initialized at data/audit.db\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  • In Colab you'll see Streamlit warnings (normal).\")\n",
        "print(\"  • To run the UI: `streamlit run pages/04_Admin_Sandbox.py` (locally/Cloud).\")\n",
        "print(\"  • In Colab, you can still import and call engine functions directly for training.\")"
      ],
      "metadata": {
        "id": "qHmY68ZmGieC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ONE-CELL: Loan Company Portal Architecture (modules + pages) + smoke test ===\n",
        "import os, json, sqlite3, textwrap, datetime as dt, re\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Ensure folders ---\n",
        "os.makedirs(\"modules/portal\", exist_ok=True)\n",
        "os.makedirs(\"pages\", exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"tenants\", exist_ok=True)\n",
        "\n",
        "# --- Helper to write files ---\n",
        "def write(path, content):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(textwrap.dedent(content))\n",
        "\n",
        "# ============== modules/portal/__init__.py ==============\n",
        "write(\"modules/portal/__init__.py\", \"\"\"\n",
        "# Loan Company Portal package\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/tenant_store.py ==============\n",
        "write(\"modules/portal/tenant_store.py\", r\"\"\"\n",
        "import os, sqlite3, re\n",
        "from pathlib import Path\n",
        "\n",
        "DB = \"data/tenants.db\"\n",
        "ROOT = Path(\"tenants\")\n",
        "\n",
        "def _slugify(name:str)->str:\n",
        "    s = re.sub(r'[^a-zA-Z0-9]+', '_', name.strip().lower()).strip('_')\n",
        "    return s or \"tenant\"\n",
        "\n",
        "def _ensure_db():\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS tenants(\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            name TEXT UNIQUE NOT NULL,\n",
        "            slug TEXT UNIQUE NOT NULL,\n",
        "            created_at TEXT NOT NULL\n",
        "        )\"\"\")\n",
        "        conn.commit()\n",
        "\n",
        "def ensure_tenant(name:str)->dict:\n",
        "    _ensure_db()\n",
        "    slug = _slugify(name)\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        cur = conn.execute(\"SELECT id, name, slug, created_at FROM tenants WHERE slug=?\", (slug,))\n",
        "        row = cur.fetchone()\n",
        "        if row:\n",
        "            tenant = {\"id\":row[0], \"name\":row[1], \"slug\":row[2], \"created_at\":row[3]}\n",
        "        else:\n",
        "            import datetime as dt\n",
        "            ts = dt.datetime.utcnow().isoformat(timespec=\"seconds\")+\"Z\"\n",
        "            conn.execute(\"INSERT INTO tenants(name, slug, created_at) VALUES (?,?,?)\", (name, slug, ts))\n",
        "            conn.commit()\n",
        "            tenant = {\"id\":conn.execute(\"SELECT last_insert_rowid()\").fetchone()[0], \"name\":name, \"slug\":slug, \"created_at\":ts}\n",
        "    # ensure folders\n",
        "    root = ROOT/slug\n",
        "    for sub in [\"uploads\",\"processed\",\"preds\",\"reports\",\"models\"]:\n",
        "        (root/sub).mkdir(parents=True, exist_ok=True)\n",
        "    return tenant\n",
        "\n",
        "def tenant_root(slug:str)->Path:\n",
        "    return ROOT/slug\n",
        "\n",
        "def list_tenants()->list:\n",
        "    _ensure_db()\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        cur = conn.execute(\"SELECT id,name,slug,created_at FROM tenants ORDER BY id DESC\")\n",
        "        return [{\"id\":r[0],\"name\":r[1],\"slug\":r[2],\"created_at\":r[3]} for r in cur.fetchall()]\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/schemas.py ==============\n",
        "write(\"modules/portal/schemas.py\", r\"\"\"\n",
        "# Minimal column expectations + friendly names / mapping hints\n",
        "REQUIRED = [\n",
        "    \"customer_id\",\"customer_name\",\"gov_id\",\"branch\",\"product\",\"loan_amount\",\n",
        "    \"status\",\"loan_health\",\"created_date\"\n",
        "]\n",
        "OPTIONAL = [\n",
        "    \"age\",\"gender\",\"loan_type\",\"debt_to_income\",\"product_weeks\"\n",
        "]\n",
        "\n",
        "CANONICAL_TYPES = {\n",
        "    \"customer_id\":\"str\",\n",
        "    \"customer_name\":\"str\",\n",
        "    \"gov_id\":\"str\",\n",
        "    \"branch\":\"str\",\n",
        "    \"product\":\"str\",\n",
        "    \"loan_amount\":\"float\",\n",
        "    \"status\":\"str\",\n",
        "    \"loan_health\":\"str\",\n",
        "    \"created_date\":\"date\",\n",
        "    \"age\":\"int\",\n",
        "    \"gender\":\"str\",\n",
        "    \"loan_type\":\"str\",\n",
        "    \"debt_to_income\":\"float\",\n",
        "    \"product_weeks\":\"int\"\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/data_service.py ==============\n",
        "write(\"modules/portal/data_service.py\", r\"\"\"\n",
        "import os, json, re, datetime as dt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from modules.portal import schemas\n",
        "from modules.portal.recommender import risk_tier, recommend_limit\n",
        "from modules.portal.tenant_store import tenant_root\n",
        "from modules.portal.scoring import score_with_model_or_fallback\n",
        "\n",
        "def _parse_date(x):\n",
        "    for fmt in [\"%Y-%m-%d\",\"%d/%m/%Y\",\"%m/%d/%Y\",\"%Y/%m/%d\",\"%d-%m-%Y\"]:\n",
        "        try:\n",
        "            return pd.to_datetime(x, format=fmt, errors=\"raise\")\n",
        "        except Exception:\n",
        "            continue\n",
        "    return pd.to_datetime(x, errors=\"coerce\")\n",
        "\n",
        "def _kenya_age_from_govid(govid:str):\n",
        "    # Heuristic buckets based on Kenyan ID ranges (approx)\n",
        "    # 32xxxxxx-33xxxxx -> 28-30 range comment by user → map buckets\n",
        "    if not isinstance(govid, str):\n",
        "        govid = str(govid)\n",
        "    digits = re.sub(r\"\\D\",\"\", govid)\n",
        "    if len(digits) < 7:  # unknown\n",
        "        return np.nan\n",
        "    # Very rough buckets; refine later if you have a better mapping\n",
        "    prefix = int(digits[:2])  # take first two digits for bucketing\n",
        "    if 28 <= prefix <= 33: return 29\n",
        "    if 34 <= prefix <= 36: return 26\n",
        "    if 20 <= prefix <= 27: return 35\n",
        "    if 10 <= prefix <= 19: return 45\n",
        "    return np.nan\n",
        "\n",
        "def validate_and_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Normalize column names\n",
        "    df = df.copy()\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "    # Ensure required columns exist (lightweight mapping hints can be added later)\n",
        "    missing = [c for c in schemas.REQUIRED if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "    # Types & cleaning\n",
        "    if \"loan_amount\" in df.columns:\n",
        "        df[\"loan_amount\"] = pd.to_numeric(df[\"loan_amount\"], errors=\"coerce\")\n",
        "\n",
        "    if \"debt_to_income\" in df.columns:\n",
        "        df[\"debt_to_income\"] = pd.to_numeric(df[\"debt_to_income\"], errors=\"coerce\")\n",
        "\n",
        "    if \"age\" not in df.columns or df[\"age\"].isna().all():\n",
        "        df[\"age\"] = df.get(\"age\", pd.Series([np.nan]*len(df)))\n",
        "        # fill from gov_id heuristic if missing\n",
        "        df.loc[df[\"age\"].isna(), \"age\"] = df.loc[df[\"age\"].isna(), \"gov_id\"].map(_kenya_age_from_govid)\n",
        "\n",
        "    # Gender guess (very naive) if missing\n",
        "    if \"gender\" not in df.columns:\n",
        "        df[\"gender\"] = \"unknown\"\n",
        "    else:\n",
        "        df[\"gender\"] = df[\"gender\"].fillna(\"unknown\").str.lower()\n",
        "\n",
        "    # Dates\n",
        "    if \"created_date\" in df.columns:\n",
        "        df[\"created_date\"] = df[\"created_date\"].apply(_parse_date)\n",
        "\n",
        "    # Default target if available (loan_health != performing -> default=1)\n",
        "    if \"loan_health\" in df.columns:\n",
        "        df[\"default\"] = (df[\"loan_health\"].str.lower() != \"performing\").astype(int)\n",
        "    else:\n",
        "        df[\"default\"] = 0\n",
        "\n",
        "    # Fill simple gaps\n",
        "    for col in [\"branch\",\"product\",\"status\",\"loan_type\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(\"unknown\").astype(str)\n",
        "\n",
        "    return df\n",
        "\n",
        "def attach_predictions_and_actions(df: pd.DataFrame, prod_model_path:str|None=None) -> pd.DataFrame:\n",
        "    \"\"\"Returns df with default_proba, risk_tier, rec_limit.\"\"\"\n",
        "    df = df.copy()\n",
        "    # Score\n",
        "    scores = score_with_model_or_fallback(df, prod_model_path)\n",
        "    df[\"default_proba\"] = scores\n",
        "    # Risk tier\n",
        "    df[\"risk_tier\"] = df[\"default_proba\"].apply(risk_tier)\n",
        "    # Recommended limit\n",
        "    df[\"rec_limit\"] = df.apply(lambda r: recommend_limit(r), axis=1)\n",
        "    return df\n",
        "\n",
        "def save_tenant_upload(slug:str, df: pd.DataFrame, name:str=\"upload.csv\") -> str:\n",
        "    root = tenant_root(slug)\n",
        "    path = root/\"uploads\"/name\n",
        "    df.to_csv(path, index=False)\n",
        "    return str(path)\n",
        "\n",
        "def save_tenant_preds(slug:str, df: pd.DataFrame, name:str|None=None) -> str:\n",
        "    root = tenant_root(slug)\n",
        "    name = name or f\"preds_{dt.datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    path = root/\"preds\"/name\n",
        "    df.to_csv(path, index=False)\n",
        "    return str(path)\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/scoring.py ==============\n",
        "write(\"modules/portal/scoring.py\", r\"\"\"\n",
        "import os, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "PROD_PIN = \"models/PROD_MODEL.txt\"\n",
        "\n",
        "# Minimal feature selector for inference consistency\n",
        "NUMERIC_FEATURES = [\"loan_amount\",\"debt_to_income\",\"age\",\"product_weeks\"]\n",
        "CAT_FEATURES = [\"branch\",\"product\",\"gender\",\"status\",\"loan_type\",\"loan_health\"]\n",
        "\n",
        "def _safe_numeric(df: pd.DataFrame):\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for c in NUMERIC_FEATURES:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    return out\n",
        "\n",
        "def _prod_model_path(override:str|None)->str|None:\n",
        "    if override and os.path.exists(override): return override\n",
        "    if os.path.exists(PROD_PIN):\n",
        "        try:\n",
        "            with open(PROD_PIN,\"r\") as f:\n",
        "                p = f.read().strip()\n",
        "                return p if os.path.exists(p) else None\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def _heuristic_score(df: pd.DataFrame)->np.ndarray:\n",
        "    # Simple risk from dti + amount scale + young age slightly up-risk\n",
        "    dti = pd.to_numeric(df.get(\"debt_to_income\", 0.3), errors=\"coerce\").fillna(0.3)\n",
        "    amt = pd.to_numeric(df.get(\"loan_amount\", 20000), errors=\"coerce\").fillna(20000)\n",
        "    age = pd.to_numeric(df.get(\"age\", 30), errors=\"coerce\").fillna(30)\n",
        "\n",
        "    # normalize\n",
        "    dti_n = dti.clip(0,1)\n",
        "    amt_n = (amt.clip(lower=0, upper=300000) / 300000.0)\n",
        "    age_pen = np.where(age < 25, 0.05, 0.0)\n",
        "\n",
        "    raw = 0.6*dti_n + 0.35*amt_n + age_pen\n",
        "    return np.clip(raw, 0.01, 0.99).values\n",
        "\n",
        "def score_with_model_or_fallback(df: pd.DataFrame, override_model_path:str|None=None)->np.ndarray:\n",
        "    path = _prod_model_path(override_model_path)\n",
        "    if not path:\n",
        "        return _heuristic_score(df)\n",
        "\n",
        "    try:\n",
        "        model = joblib.load(path)\n",
        "        # Try scikit API\n",
        "        X = _safe_numeric(df)  # keep minimal consistent features\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            proba = model.predict_proba(X)[:,1]\n",
        "        elif hasattr(model, \"decision_function\"):\n",
        "            s = model.decision_function(X)\n",
        "            # map to 0..1\n",
        "            from scipy.special import expit\n",
        "            proba = expit(s)\n",
        "        else:\n",
        "            # last resort: predict labels and soften\n",
        "            pred = model.predict(X)\n",
        "            proba = 0.7*pred + 0.15\n",
        "        return np.clip(proba, 0.01, 0.99)\n",
        "    except Exception:\n",
        "        # Fallback if model can't load\n",
        "        return _heuristic_score(df)\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/recommender.py ==============\n",
        "write(\"modules/portal/recommender.py\", r\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Risk tiers\n",
        "def risk_tier(p: float|int) -> str:\n",
        "    try:\n",
        "        p = float(p)\n",
        "    except Exception:\n",
        "        p = 0.5\n",
        "    if p < 0.10: return \"A (very low)\"\n",
        "    if p < 0.20: return \"B (low)\"\n",
        "    if p < 0.35: return \"C (moderate)\"\n",
        "    if p < 0.50: return \"D (elevated)\"\n",
        "    return \"E (high)\"\n",
        "\n",
        "def _cap_by_tier(base_amt: float, tier: str) -> float:\n",
        "    m = {\n",
        "        \"A (very low)\": 1.25,\n",
        "        \"B (low)\": 1.10,\n",
        "        \"C (moderate)\": 0.90,\n",
        "        \"D (elevated)\": 0.65,\n",
        "        \"E (high)\": 0.40\n",
        "    }\n",
        "    return max(1000.0, base_amt * m.get(tier, 0.8))\n",
        "\n",
        "def recommend_limit(row: pd.Series) -> float:\n",
        "    # baseline = existing requested amount or historical avg; here use loan_amount\n",
        "    base = float(row.get(\"loan_amount\", 20000.0) or 20000.0)\n",
        "    tier = row.get(\"risk_tier\")\n",
        "    return round(_cap_by_tier(base, tier), 2)\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/reporting.py ==============\n",
        "write(\"modules/portal/reporting.py\", r\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "def kpis(df: pd.DataFrame) -> dict:\n",
        "    total_loans = len(df)\n",
        "    total_amount = float(df[\"loan_amount\"].sum()) if \"loan_amount\" in df.columns else 0.0\n",
        "    avg_amount = float(df[\"loan_amount\"].mean()) if \"loan_amount\" in df.columns else 0.0\n",
        "    default_rate = float(df.get(\"default\", pd.Series([0]*len(df))).mean()) if len(df)>0 else 0.0\n",
        "    avg_proba = float(df.get(\"default_proba\", pd.Series([0.0]*len(df))).mean()) if len(df)>0 else 0.0\n",
        "    return dict(\n",
        "        total_loans=total_loans,\n",
        "        total_amount=round(total_amount,2),\n",
        "        avg_amount=round(avg_amount,2),\n",
        "        default_rate=round(default_rate,3),\n",
        "        avg_pred_default=round(avg_proba,3)\n",
        "    )\n",
        "\n",
        "def branch_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    cols = [c for c in [\"branch\",\"loan_amount\",\"default\",\"default_proba\"] if c in df.columns]\n",
        "    if not cols or \"branch\" not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "    agg = {\n",
        "        \"loan_amount\":\"sum\"\n",
        "    }\n",
        "    if \"default\" in df.columns: agg[\"default\"]=\"mean\"\n",
        "    if \"default_proba\" in df.columns: agg[\"default_proba\"]=\"mean\"\n",
        "    out = df.groupby(\"branch\", dropna=False).agg(agg).reset_index()\n",
        "    out = out.rename(columns={\"loan_amount\":\"total_amount\",\"default\":\"default_rate\",\"default_proba\":\"avg_proba\"})\n",
        "    return out.sort_values(\"total_amount\", ascending=False)\n",
        "\n",
        "def product_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"product\" not in df.columns: return pd.DataFrame()\n",
        "    agg = {\"loan_amount\":\"sum\"}\n",
        "    if \"default\" in df.columns: agg[\"default\"]=\"mean\"\n",
        "    if \"default_proba\" in df.columns: agg[\"default_proba\"]=\"mean\"\n",
        "    out = df.groupby(\"product\", dropna=False).agg(agg).reset_index()\n",
        "    out = out.rename(columns={\"loan_amount\":\"total_amount\",\"default\":\"default_rate\",\"default_proba\":\"avg_proba\"})\n",
        "    return out.sort_values(\"total_amount\", ascending=False)\n",
        "\n",
        "def top_risky(df: pd.DataFrame, n:int=50) -> pd.DataFrame:\n",
        "    if \"default_proba\" not in df.columns: return pd.DataFrame()\n",
        "    cols = [c for c in [\"customer_id\",\"customer_name\",\"branch\",\"product\",\"loan_amount\",\"default_proba\",\"risk_tier\",\"rec_limit\"] if c in df.columns]\n",
        "    out = df[cols].copy()\n",
        "    return out.sort_values(\"default_proba\", ascending=False).head(n)\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/viz.py ==============\n",
        "write(\"modules/portal/viz.py\", r\"\"\"\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "def fig_branch_bar(df: pd.DataFrame):\n",
        "    if \"branch\" not in df.columns or \"loan_amount\" not in df.columns:\n",
        "        return None\n",
        "    agg = df.groupby(\"branch\", dropna=False)[\"loan_amount\"].sum().reset_index()\n",
        "    return px.bar(agg, x=\"branch\", y=\"loan_amount\", title=\"Total Amount by Branch\")\n",
        "\n",
        "def fig_product_pie(df: pd.DataFrame):\n",
        "    if \"product\" not in df.columns or \"loan_amount\" not in df.columns:\n",
        "        return None\n",
        "    agg = df.groupby(\"product\", dropna=False)[\"loan_amount\"].sum().reset_index()\n",
        "    return px.pie(agg, names=\"product\", values=\"loan_amount\", title=\"Product Mix\")\n",
        "\n",
        "def fig_risk_hist(df: pd.DataFrame):\n",
        "    if \"default_proba\" not in df.columns:\n",
        "        return None\n",
        "    return px.histogram(df, x=\"default_proba\", nbins=30, title=\"Predicted Default Probability Distribution\")\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/explain.py (stub; portal-side helper) ==============\n",
        "write(\"modules/portal/explain.py\", r\"\"\"\n",
        "# Portal-side lightweight explainability helper; for rich SHAP use Admin Sandbox.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def simple_feature_importance(row: pd.Series) -> list[tuple[str, float]]:\n",
        "    # naive importance proxy using numeric magnitudes\n",
        "    feats = {}\n",
        "    for c in [\"loan_amount\",\"debt_to_income\",\"age\",\"product_weeks\"]:\n",
        "        if c in row:\n",
        "            try:\n",
        "                feats[c] = float(row[c] or 0.0)\n",
        "            except Exception:\n",
        "                feats[c] = 0.0\n",
        "    if not feats:\n",
        "        return []\n",
        "    # normalize\n",
        "    total = sum(abs(v) for v in feats.values()) or 1.0\n",
        "    imp = [(k, abs(v)/total) for k,v in feats.items()]\n",
        "    return sorted(imp, key=lambda t: t[1], reverse=True)\n",
        "\"\"\")\n",
        "\n",
        "# ============== modules/portal/auth.py (very light stub; UI will handle sessions) ==============\n",
        "write(\"modules/portal/auth.py\", r\"\"\"\n",
        "import secrets\n",
        "# Minimal token stub (extend with proper auth later)\n",
        "def issue_token(tenant_slug:str)->str:\n",
        "    return f\"{tenant_slug}.\" + secrets.token_hex(8)\n",
        "\n",
        "def validate_token(tok:str)->str|None:\n",
        "    try:\n",
        "        slug, _ = tok.split(\".\", 1)\n",
        "        return slug\n",
        "    except Exception:\n",
        "        return None\n",
        "\"\"\")\n",
        "\n",
        "# ============== Client-facing pages (Streamlit stubs) ==============\n",
        "write(\"pages/01_Company_Portal_Home.py\", r\"\"\"\n",
        "import streamlit as st\n",
        "st.set_page_config(page_title=\"Loan Company Portal\", layout=\"wide\")\n",
        "def app():\n",
        "    st.title(\"🏦 Loan Company Portal — Home\")\n",
        "    st.info(\"Use the sidebar to navigate: Dashboard, Clients, Reports, Settings.\")\n",
        "if __name__ == \"__main__\":\n",
        "    app()\n",
        "\"\"\")\n",
        "\n",
        "write(\"pages/02_Portfolio_Dashboard.py\", r\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from modules.portal.tenant_store import ensure_tenant, tenant_root\n",
        "from modules.portal.data_service import validate_and_preprocess, attach_predictions_and_actions\n",
        "from modules.portal.reporting import kpis, branch_summary, product_summary, top_risky\n",
        "from modules.portal.scoring import _prod_model_path\n",
        "\n",
        "def app():\n",
        "    st.title(\"📊 Portfolio Dashboard\")\n",
        "    st.caption(\"Upload a file to get KPIs, risk, and summaries. Multi-tenant aware.\")\n",
        "\n",
        "    tenant_name = st.text_input(\"Tenant name\", \"demo_mfi\")\n",
        "    up = st.file_uploader(\"Upload CSV\", type=[\"csv\",\"xlsx\",\"xls\"])\n",
        "    if st.button(\"Process\"):\n",
        "        t = ensure_tenant(tenant_name)\n",
        "        if up:\n",
        "            if up.name.lower().endswith(\".csv\"):\n",
        "                df = pd.read_csv(up)\n",
        "            else:\n",
        "                df = pd.read_excel(up)\n",
        "            df = validate_and_preprocess(df)\n",
        "            df = attach_predictions_and_actions(df, _prod_model_path(None))\n",
        "            st.success(f\"Processed: {df.shape}\")\n",
        "            st.dataframe(df.head(50), use_container_width=True)\n",
        "\n",
        "            m = kpis(df)\n",
        "            st.write(\"**KPIs**\", m)\n",
        "            st.write(\"**By Branch**\", branch_summary(df))\n",
        "            st.write(\"**By Product**\", product_summary(df))\n",
        "            st.write(\"**Top Risky**\", top_risky(df, 20))\n",
        "        else:\n",
        "            st.warning(\"Please upload a file.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app()\n",
        "\"\"\")\n",
        "\n",
        "write(\"pages/03_Client_Explorer.py\", r\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from modules.portal.data_service import validate_and_preprocess, attach_predictions_and_actions\n",
        "from modules.portal.explain import simple_feature_importance\n",
        "from modules.portal.scoring import _prod_model_path\n",
        "\n",
        "def app():\n",
        "    st.title(\"🔎 Client Explorer\")\n",
        "    st.caption(\"Search clients, view full profile with predictions and recommendations.\")\n",
        "\n",
        "    up = st.file_uploader(\"Upload CSV to explore\", type=[\"csv\",\"xlsx\",\"xls\"])\n",
        "    q = st.text_input(\"Search by Customer ID / Name / Gov ID\").strip().lower()\n",
        "\n",
        "    if up and st.button(\"Load & Search\"):\n",
        "        if up.name.lower().endswith(\".csv\"):\n",
        "            df = pd.read_csv(up)\n",
        "        else:\n",
        "            df = pd.read_excel(up)\n",
        "        df = validate_and_preprocess(df)\n",
        "        df = attach_predictions_and_actions(df, _prod_model_path(None))\n",
        "\n",
        "        # search\n",
        "        if q:\n",
        "            mask = (\n",
        "                df.get(\"customer_id\",\"\").astype(str).str.lower().str.contains(q, na=False) |\n",
        "                df.get(\"customer_name\",\"\").astype(str).str.lower().str.contains(q, na=False) |\n",
        "                df.get(\"gov_id\",\"\").astype(str).str.lower().str.contains(q, na=False)\n",
        "            )\n",
        "            hits = df[mask].copy()\n",
        "        else:\n",
        "            hits = df.copy()\n",
        "\n",
        "        st.write(f\"Results: {hits.shape[0]}\")\n",
        "        st.dataframe(hits.head(100), use_container_width=True)\n",
        "\n",
        "        # profile if single selection\n",
        "        if len(hits) == 1:\n",
        "            r = hits.iloc[0]\n",
        "            st.subheader(\"Client Profile\")\n",
        "            st.write(r.to_dict())\n",
        "            st.markdown(f\"**Predicted default probability:** {r['default_proba']:.3f}\")\n",
        "            st.markdown(f\"**Risk tier:** {r['risk_tier']}\")\n",
        "            st.markdown(f\"**Recommended limit:** {r['rec_limit']:,}\")\n",
        "\n",
        "            st.markdown(\"**Why? (simple feature importance proxy)**\")\n",
        "            st.write(simple_feature_importance(r))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app()\n",
        "\"\"\")\n",
        "\n",
        "write(\"pages/06_Reports_Exports.py\", r\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from modules.portal.data_service import validate_and_preprocess, attach_predictions_and_actions\n",
        "from modules.portal.reporting import kpis, branch_summary, product_summary, top_risky\n",
        "from modules.portal.scoring import _prod_model_path\n",
        "\n",
        "def _to_excel(dfs: dict) -> bytes:\n",
        "    bio = BytesIO()\n",
        "    with pd.ExcelWriter(bio, engine=\"xlsxwriter\") as writer:\n",
        "        for name, df in dfs.items():\n",
        "            df.to_excel(writer, sheet_name=name[:31], index=False)\n",
        "    return bio.getvalue()\n",
        "\n",
        "def app():\n",
        "    st.title(\"🧾 Reports & Exports\")\n",
        "    up = st.file_uploader(\"Upload CSV/XLSX\", type=[\"csv\",\"xlsx\",\"xls\"])\n",
        "    if up and st.button(\"Generate Reports\"):\n",
        "        df = pd.read_csv(up) if up.name.lower().endswith(\".csv\") else pd.read_excel(up)"
      ],
      "metadata": {
        "id": "tPJdrC9fJTUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltME-0nhPgS6",
        "outputId": "52652785-cf0d-4396-81bf-05b26c3ae666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === LoanIQ Auth + Login System (all in one cell) ===\n",
        "import sqlite3, hashlib\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "\n",
        "# --- DB Setup ---\n",
        "DB_PATH = Path(\"config/loaniq.db\")\n",
        "DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def init_db():\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS users (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        company TEXT,\n",
        "        email TEXT UNIQUE,\n",
        "        password TEXT,\n",
        "        role TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def hash_pw(password:str) -> str:\n",
        "    return hashlib.sha256(password.encode()).hexdigest()\n",
        "\n",
        "def register_user(company:str, email:str, password:str, role:str=\"client\"):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"INSERT INTO users (company,email,password,role) VALUES (?,?,?,?)\",\n",
        "              (company, email, hash_pw(password), role))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def login_user(email:str, password:str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT id, company, email, role FROM users WHERE email=? AND password=?\",\n",
        "              (email, hash_pw(password)))\n",
        "    row = c.fetchone()\n",
        "    conn.close()\n",
        "    return row\n",
        "\n",
        "# --- Bootstrap: Ensure DB + Admin user ---\n",
        "init_db()\n",
        "try:\n",
        "    register_user(\"SystemAdmin\", \"Admin\", \"Shady868\", role=\"admin\")\n",
        "except Exception:\n",
        "    pass  # already exists\n",
        "\n",
        "# --- Session Setup ---\n",
        "if \"user\" not in st.session_state:\n",
        "    st.session_state[\"user\"] = None\n",
        "\n",
        "# --- UI Logic ---\n",
        "if st.session_state[\"user\"] is None:\n",
        "    st.title(\"🔑 LoanIQ Login\")\n",
        "\n",
        "    mode = st.radio(\"Login or Register?\", [\"Login\", \"Register\"])\n",
        "\n",
        "    if mode == \"Login\":\n",
        "        email = st.text_input(\"Email / Username\")\n",
        "        pw = st.text_input(\"Password\", type=\"password\")\n",
        "        if st.button(\"Login\"):\n",
        "            user = login_user(email, pw)\n",
        "            if user:\n",
        "                st.session_state[\"user\"] = {\n",
        "                    \"id\": user[0],\n",
        "                    \"company\": user[1],\n",
        "                    \"email\": user[2],\n",
        "                    \"role\": user[3]\n",
        "                }\n",
        "                st.experimental_rerun()\n",
        "            else:\n",
        "                st.error(\"❌ Invalid credentials\")\n",
        "\n",
        "    else:  # Register\n",
        "        company = st.text_input(\"Company Name\")\n",
        "        email = st.text_input(\"Email / Username\")\n",
        "        pw = st.text_input(\"Password\", type=\"password\")\n",
        "        if st.button(\"Register & Continue\"):\n",
        "            try:\n",
        "                register_user(company, email, pw)\n",
        "                user = login_user(email, pw)  # auto-login\n",
        "                st.session_state[\"user\"] = {\n",
        "                    \"id\": user[0],\n",
        "                    \"company\": user[1],\n",
        "                    \"email\": user[2],\n",
        "                    \"role\": user[3]\n",
        "                }\n",
        "                st.success(\"✅ Registered and logged in!\")\n",
        "                st.experimental_rerun()\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ Failed: {e}\")\n",
        "\n",
        "else:\n",
        "    user = st.session_state[\"user\"]\n",
        "    st.sidebar.success(f\"Logged in as {user['company']} ({user['role']})\")\n",
        "    if st.sidebar.button(\"Logout\"):\n",
        "        st.session_state[\"user\"] = None\n",
        "        st.experimental_rerun()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67irQvprPOUo",
        "outputId": "da1f90fc-1d74-42b1-c28e-637d17d4359d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 13:08:29.481 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.482 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
            "2025-08-31 13:08:29.483 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.484 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.485 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.486 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.571 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-08-31 13:08:29.572 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.573 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.574 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.575 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.576 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.580 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.582 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.585 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.586 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.588 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.589 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.590 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.591 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.592 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.595 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.597 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.598 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:08:29.598 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Role-Based Login + Dashboards ---\n",
        "import sqlite3, hashlib\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "\n",
        "# --- DB Setup ---\n",
        "DB_FILE = \"config/users.db\"\n",
        "Path(\"config\").mkdir(exist_ok=True)\n",
        "\n",
        "def init_db():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS users (\n",
        "            username TEXT PRIMARY KEY,\n",
        "            password TEXT,\n",
        "            role TEXT DEFAULT 'company'\n",
        "        )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_db()\n",
        "\n",
        "# --- Helpers ---\n",
        "def hash_pw(password: str) -> str:\n",
        "    return hashlib.sha256(password.encode()).hexdigest()\n",
        "\n",
        "def add_user(username, password, role=\"company\"):\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    try:\n",
        "        c.execute(\"INSERT INTO users (username, password, role) VALUES (?, ?, ?)\",\n",
        "                  (username, hash_pw(password), role))\n",
        "        conn.commit()\n",
        "    except sqlite3.IntegrityError:\n",
        "        pass\n",
        "    conn.close()\n",
        "\n",
        "def validate_user(username, password):\n",
        "    # Admin hardcoded\n",
        "    if username == \"Admin\" and password == \"Shady868\":\n",
        "        return \"admin\"\n",
        "\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT role FROM users WHERE username=? AND password=?\",\n",
        "              (username, hash_pw(password)))\n",
        "    row = c.fetchone()\n",
        "    conn.close()\n",
        "    return row[0] if row else None\n",
        "\n",
        "# --- Session state ---\n",
        "if \"user\" not in st.session_state:\n",
        "    st.session_state[\"user\"] = None\n",
        "if \"role\" not in st.session_state:\n",
        "    st.session_state[\"role\"] = None\n",
        "\n",
        "# --- UI ---\n",
        "st.title(\"🔐 LoanIQ Login Portal\")\n",
        "\n",
        "if st.session_state[\"user\"]:\n",
        "    st.success(f\"Welcome, {st.session_state['user']} ({st.session_state['role']})\")\n",
        "\n",
        "    if st.session_state[\"role\"] == \"admin\":\n",
        "        st.subheader(\"🛠️ Admin Dashboard\")\n",
        "        st.write(\"Here you can manage all companies, view models, impersonate users.\")\n",
        "        # TODO: Hook into your Sandbox + Training tabs here\n",
        "\n",
        "    elif st.session_state[\"role\"] == \"company\":\n",
        "        st.subheader(\"📊 Company Dashboard\")\n",
        "        st.write(\"Here you can upload datasets, view reports, predictions, and client insights.\")\n",
        "        # TODO: Hook into Reports + Predictions + Client Profiles\n",
        "\n",
        "    if st.button(\"Logout\"):\n",
        "        st.session_state[\"user\"] = None\n",
        "        st.session_state[\"role\"] = None\n",
        "        st.experimental_rerun()\n",
        "\n",
        "else:\n",
        "    choice = st.radio(\"Choose action\", [\"Login\", \"Register\"])\n",
        "\n",
        "    if choice == \"Login\":\n",
        "        user = st.text_input(\"Username\")\n",
        "        pw = st.text_input(\"Password\", type=\"password\")\n",
        "        if st.button(\"Login\"):\n",
        "            role = validate_user(user, pw)\n",
        "            if role:\n",
        "                st.session_state[\"user\"] = user\n",
        "                st.session_state[\"role\"] = role\n",
        "                st.success(f\"Logged in as {user} ({role})\")\n",
        "                st.experimental_rerun()\n",
        "            else:\n",
        "                st.error(\"❌ Invalid credentials\")\n",
        "\n",
        "    elif choice == \"Register\":\n",
        "        new_user = st.text_input(\"New Username\")\n",
        "        new_pw = st.text_input(\"New Password\", type=\"password\")\n",
        "        if st.button(\"Register\"):\n",
        "            if new_user and new_pw:\n",
        "                add_user(new_user, new_pw, \"company\")\n",
        "                st.success(\"✅ Registration successful. Please login.\")\n",
        "            else:\n",
        "                st.error(\"Fill all fields\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NnQWwDBQLE1",
        "outputId": "496889f2-dbe7-4105-a8da-328a9fd52c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 13:11:24.362 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.362 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.363 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.364 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.365 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.366 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.366 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.368 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.368 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.369 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.370 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.370 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.371 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.372 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.372 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.373 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.375 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.375 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.378 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.378 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.379 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.379 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.379 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.380 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.380 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:11:24.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Role-Based Login + Dashboards ---\n",
        "import sqlite3, hashlib\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "\n",
        "# --- DB Setup ---\n",
        "DB_FILE = \"config/users.db\"\n",
        "Path(\"config\").mkdir(exist_ok=True)\n",
        "\n",
        "def init_db():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS users (\n",
        "            username TEXT PRIMARY KEY,\n",
        "            password TEXT,\n",
        "            role TEXT DEFAULT 'company'\n",
        "        )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_db()\n",
        "\n",
        "# --- Helpers ---\n",
        "def hash_pw(password: str) -> str:\n",
        "    return hashlib.sha256(password.encode()).hexdigest()\n",
        "\n",
        "def add_user(username, password, role=\"company\"):\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    try:\n",
        "        c.execute(\"INSERT INTO users (username, password, role) VALUES (?, ?, ?)\",\n",
        "                  (username, hash_pw(password), role))\n",
        "        conn.commit()\n",
        "    except sqlite3.IntegrityError:\n",
        "        pass\n",
        "    conn.close()\n",
        "\n",
        "def validate_user(username, password):\n",
        "    # Admin hardcoded\n",
        "    if username == \"Admin\" and password == \"Shady868\":\n",
        "        return \"admin\"\n",
        "\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT role FROM users WHERE username=? AND password=?\",\n",
        "              (username, hash_pw(password)))\n",
        "    row = c.fetchone()\n",
        "    conn.close()\n",
        "    return row[0] if row else None\n",
        "\n",
        "# --- Session state ---\n",
        "if \"user\" not in st.session_state:\n",
        "    st.session_state[\"user\"] = None\n",
        "if \"role\" not in st.session_state:\n",
        "    st.session_state[\"role\"] = None\n",
        "\n",
        "# --- UI ---\n",
        "st.title(\"🔐 LoanIQ Login Portal\")\n",
        "\n",
        "if st.session_state[\"user\"]:\n",
        "    st.success(f\"Welcome, {st.session_state['user']} ({st.session_state['role']})\")\n",
        "\n",
        "    # --- Admin Portal ---\n",
        "    if st.session_state[\"role\"] == \"admin\":\n",
        "        st.subheader(\"🛠️ Admin Dashboard\")\n",
        "        st.write(\"Manage companies, run experiments, audit logs, and impersonate users.\")\n",
        "\n",
        "        if st.button(\"🧪 Go to Sandbox / Training\"):\n",
        "            st.switch_page(\"pages/07_Sandbox.py\")\n",
        "\n",
        "        if st.button(\"📜 View Audit Logs\"):\n",
        "            st.switch_page(\"pages/06_Audit.py\")\n",
        "\n",
        "        if st.button(\"👤 Impersonate Company\"):\n",
        "            st.switch_page(\"pages/04_Client_Insights.py\")\n",
        "\n",
        "    # --- Company Portal ---\n",
        "    elif st.session_state[\"role\"] == \"company\":\n",
        "        st.subheader(\"📊 Company Dashboard\")\n",
        "        st.write(\"Upload datasets, view reports, and explore predictions.\")\n",
        "\n",
        "        if st.button(\"📤 Upload & Manage Data\"):\n",
        "            st.switch_page(\"pages/02_DataUpload.py\")\n",
        "\n",
        "        if st.button(\"📑 Reports & Exports\"):\n",
        "            st.switch_page(\"pages/06_Reports_Exports.py\")\n",
        "\n",
        "        if st.button(\"🔮 Client Predictions & Insights\"):\n",
        "            st.switch_page(\"pages/04_Client_Insights.py\")\n",
        "\n",
        "    # --- Logout ---\n",
        "    if st.button(\"Logout\"):\n",
        "        st.session_state[\"user\"] = None\n",
        "        st.session_state[\"role\"] = None\n",
        "        st.experimental_rerun()\n",
        "\n",
        "else:\n",
        "    choice = st.radio(\"Choose action\", [\"Login\", \"Register\"])\n",
        "\n",
        "    if choice == \"Login\":\n",
        "        user = st.text_input(\"Username\")\n",
        "        pw = st.text_input(\"Password\", type=\"password\")\n",
        "        if st.button(\"Login\"):\n",
        "            role = validate_user(user, pw)\n",
        "            if role:\n",
        "                st.session_state[\"user\"] = user\n",
        "                st.session_state[\"role\"] = role\n",
        "                st.success(f\"Logged in as {user} ({role})\")\n",
        "                st.experimental_rerun()\n",
        "            else:\n",
        "                st.error(\"❌ Invalid credentials\")\n",
        "\n",
        "    elif choice == \"Register\":\n",
        "        new_user = st.text_input(\"New Username\")\n",
        "        new_pw = st.text_input(\"New Password\", type=\"password\")\n",
        "        if st.button(\"Register\"):\n",
        "            if new_user and new_pw:\n",
        "                add_user(new_user, new_pw, \"company\")\n",
        "                st.success(\"✅ Registration successful. Please login.\")\n",
        "            else:\n",
        "                st.error(\"Fill all fields\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjUN00jmQg2c",
        "outputId": "d15307f3-b211-430e-c70d-2f50e64b2f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 13:12:53.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:53.996 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:53.997 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:53.998 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:53.998 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:53.999 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.000 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.001 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.002 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.002 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.003 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.004 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.004 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.005 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.008 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.008 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.009 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.009 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.011 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.011 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.012 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.012 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.013 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.013 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.014 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.014 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:12:54.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Role-Based Login + Dashboards with Impersonation + Audit Logs ---\n",
        "import sqlite3, hashlib, datetime\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "\n",
        "# --- DB Setup ---\n",
        "DB_FILE = \"config/users.db\"\n",
        "Path(\"config\").mkdir(exist_ok=True)\n",
        "\n",
        "def init_db():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS users (\n",
        "            username TEXT PRIMARY KEY,\n",
        "            password TEXT,\n",
        "            role TEXT DEFAULT 'company'\n",
        "        )\n",
        "    \"\"\")\n",
        "    c.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audit_logs (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            actor TEXT,\n",
        "            action TEXT,\n",
        "            target TEXT,\n",
        "            ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_db()\n",
        "\n",
        "# --- Helpers ---\n",
        "def hash_pw(password: str) -> str:\n",
        "    return hashlib.sha256(password.encode()).hexdigest()\n",
        "\n",
        "def add_user(username, password, role=\"company\"):\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    try:\n",
        "        c.execute(\"INSERT INTO users (username, password, role) VALUES (?, ?, ?)\",\n",
        "                  (username, hash_pw(password), role))\n",
        "        conn.commit()\n",
        "    except sqlite3.IntegrityError:\n",
        "        pass\n",
        "    conn.close()\n",
        "\n",
        "def validate_user(username, password):\n",
        "    # Admin hardcoded\n",
        "    if username == \"Admin\" and password == \"Shady868\":\n",
        "        return \"admin\"\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT role FROM users WHERE username=? AND password=?\",\n",
        "              (username, hash_pw(password)))\n",
        "    row = c.fetchone()\n",
        "    conn.close()\n",
        "    return row[0] if row else None\n",
        "\n",
        "def list_companies():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT username FROM users WHERE role='company'\")\n",
        "    rows = [r[0] for r in c.fetchall()]\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "def log_action(actor, action, target=\"\"):\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"INSERT INTO audit_logs (actor, action, target, ts) VALUES (?, ?, ?, ?)\",\n",
        "              (actor, action, target, datetime.datetime.now()))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# --- Session state ---\n",
        "if \"user\" not in st.session_state:\n",
        "    st.session_state[\"user\"] = None\n",
        "if \"role\" not in st.session_state:\n",
        "    st.session_state[\"role\"] = None\n",
        "if \"impersonating\" not in st.session_state:\n",
        "    st.session_state[\"impersonating\"] = None\n",
        "\n",
        "# --- UI ---\n",
        "st.title(\"🔐 LoanIQ Login Portal\")\n",
        "\n",
        "if st.session_state[\"user\"]:\n",
        "    display_user = st.session_state[\"impersonating\"] or st.session_state[\"user\"]\n",
        "    st.success(f\"Welcome, {display_user} ({st.session_state['role']})\")\n",
        "\n",
        "    # --- Admin Portal ---\n",
        "    if st.session_state[\"role\"] == \"admin\" and not st.session_state[\"impersonating\"]:\n",
        "        st.subheader(\"🛠️ Admin Dashboard\")\n",
        "\n",
        "        if st.button(\"🧪 Go to Sandbox / Training\"):\n",
        "            st.switch_page(\"pages/07_Sandbox.py\")\n",
        "\n",
        "        if st.button(\"📜 View Audit Logs\"):\n",
        "            conn = sqlite3.connect(DB_FILE)\n",
        "            logs = conn.execute(\"SELECT actor, action, target, ts FROM audit_logs ORDER BY ts DESC LIMIT 50\").fetchall()\n",
        "            conn.close()\n",
        "            for actor, action, target, ts in logs:\n",
        "                st.write(f\"**{ts}** — {actor} → {action} {target}\")\n",
        "\n",
        "        companies = list_companies()\n",
        "        if companies:\n",
        "            choice = st.selectbox(\"👤 Choose a company to impersonate\", companies)\n",
        "            if st.button(\"Impersonate\"):\n",
        "                st.session_state[\"impersonating\"] = choice\n",
        "                st.session_state[\"role\"] = \"company\"\n",
        "                log_action(\"Admin\", \"impersonated\", choice)\n",
        "                st.experimental_rerun()\n",
        "\n",
        "    # --- Company Portal (real or impersonated) ---\n",
        "    if st.session_state[\"role\"] == \"company\":\n",
        "        st.subheader(\"📊 Company Dashboard\")\n",
        "        st.write(\"Upload datasets, view reports, and explore predictions.\")\n",
        "\n",
        "        if st.button(\"📤 Upload & Manage Data\"):\n",
        "            st.switch_page(\"pages/02_DataUpload.py\")\n",
        "\n",
        "        if st.button(\"📑 Reports & Exports\"):\n",
        "            st.switch_page(\"pages/06_Reports_Exports.py\")\n",
        "\n",
        "        if st.button(\"🔮 Client Predictions & Insights\"):\n",
        "            st.switch_page(\"pages/04_Client_Insights.py\")\n",
        "\n",
        "    # --- End Impersonation ---\n",
        "    if st.session_state[\"impersonating\"]:\n",
        "        if st.button(\"❌ Stop Impersonation\"):\n",
        "            log_action(\"Admin\", \"stopped impersonating\", st.session_state[\"impersonating\"])\n",
        "            st.session_state[\"impersonating\"] = None\n",
        "            st.session_state[\"role\"] = \"admin\"\n",
        "            st.experimental_rerun()\n",
        "\n",
        "    # --- Logout ---\n",
        "    if st.button(\"Logout\"):\n",
        "        log_action(st.session_state[\"user\"], \"logout\")\n",
        "        st.session_state[\"user\"] = None\n",
        "        st.session_state[\"role\"] = None\n",
        "        st.session_state[\"impersonating\"] = None\n",
        "        st.experimental_rerun()\n",
        "\n",
        "else:\n",
        "    choice = st.radio(\"Choose action\", [\"Login\", \"Register\"])\n",
        "\n",
        "    if choice == \"Login\":\n",
        "        user = st.text_input(\"Username\")\n",
        "        pw = st.text_input(\"Password\", type=\"password\")\n",
        "        if st.button(\"Login\"):\n",
        "            role = validate_user(user, pw)\n",
        "            if role:\n",
        "                st.session_state[\"user\"] = user\n",
        "                st.session_state[\"role\"] = role\n",
        "                log_action(user, \"login\")\n",
        "                st.experimental_rerun()\n",
        "            else:\n",
        "                st.error(\"❌ Invalid credentials\")\n",
        "\n",
        "    elif choice == \"Register\":\n",
        "        new_user = st.text_input(\"New Username\")\n",
        "        new_pw = st.text_input(\"New Password\", type=\"password\")\n",
        "        if st.button(\"Register\"):\n",
        "            if new_user and new_pw:\n",
        "                add_user(new_user, new_pw, \"company\")\n",
        "                log_action(new_user, \"register\")\n",
        "                st.success(\"✅ Registration successful. Please login.\")\n",
        "            else:\n",
        "                st.error(\"Fill all fields\")"
      ],
      "metadata": {
        "id": "11UrJrucRZ5B",
        "outputId": "8098b4b1-d0bf-4780-d2bf-e3687e1c18ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 13:16:56.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.714 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.716 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.723 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 13:16:56.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# === Step 1: Create folder structure for LoanIQ ===\n",
        "folders = [\n",
        "    \"modules/bootstrap\", \"modules/core\", \"modules/ingestion\",\n",
        "    \"modules/synth\", \"modules/features\", \"modules/ml\",\n",
        "    \"modules/reports\", \"modules/sandbox\", \"modules/api\",\n",
        "    \"pages\", \"tests\", \"data\", \"config\"\n",
        "]\n",
        "for f in folders:\n",
        "    os.makedirs(f, exist_ok=True)\n",
        "\n",
        "# === Step 2: Ensure Python packages ===\n",
        "open(\"modules/__init__.py\", \"a\").close()\n",
        "for d in [\"bootstrap\",\"core\",\"ingestion\",\"synth\",\"features\",\"ml\",\"reports\",\"sandbox\",\"api\"]:\n",
        "    open(f\"modules/{d}/__init__.py\", \"a\").close()\n",
        "\n",
        "# === Step 3: Create placeholder Streamlit pages ===\n",
        "for n in range(1,8):\n",
        "    open(f\"pages/{n:02d}_placeholder.py\", \"a\").close()\n",
        "\n",
        "# === Step 4: Basic tests folder ===\n",
        "open(\"tests/__init__.py\", \"a\").close()\n",
        "\n",
        "# === Step 5: Verify structure ===\n",
        "print(\"✅ Folder scaffolding created.\\n\")\n",
        "for root, dirs, files in os.walk(\".\", topdown=True):\n",
        "    level = root.replace(os.getcwd(), \"\").count(os.sep)\n",
        "    indent = \" \" * (2 * level)\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = \" \" * (2 * (level + 1))\n",
        "    for f in files:\n",
        "        print(f\"{subindent}{f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FxOfLANLJ5R",
        "outputId": "f3bcf2d2-4afb-4241-9464-b60079270a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Folder scaffolding created.\n",
            "\n",
            "./\n",
            "  .config/\n",
            "    .last_survey_prompt.yaml\n",
            "    gce\n",
            "    config_sentinel\n",
            "    default_configs.db\n",
            "    hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            "    .last_update_check.json\n",
            "    active_config\n",
            "    .last_opt_in_prompt.yaml\n",
            "    logs/\n",
            "      2025.08.28/\n",
            "        13.42.14.257094.log\n",
            "        13.42.40.032629.log\n",
            "        13.41.44.528882.log\n",
            "        13.42.40.767285.log\n",
            "        13.42.30.169478.log\n",
            "        13.42.24.254751.log\n",
            "    configurations/\n",
            "      config_default\n",
            "  data/\n",
            "  pages/\n",
            "    02_placeholder.py\n",
            "    06_placeholder.py\n",
            "    04_placeholder.py\n",
            "    07_placeholder.py\n",
            "    01_placeholder.py\n",
            "    05_placeholder.py\n",
            "    03_placeholder.py\n",
            "  modules/\n",
            "    __init__.py\n",
            "    reports/\n",
            "      __init__.py\n",
            "    features/\n",
            "      __init__.py\n",
            "    ml/\n",
            "      __init__.py\n",
            "    bootstrap/\n",
            "      __init__.py\n",
            "    sandbox/\n",
            "      __init__.py\n",
            "    ingestion/\n",
            "      __init__.py\n",
            "    synth/\n",
            "      __init__.py\n",
            "    api/\n",
            "      __init__.py\n",
            "    core/\n",
            "      __init__.py\n",
            "  config/\n",
            "  tests/\n",
            "    __init__.py\n",
            "  sample_data/\n",
            "    README.md\n",
            "    anscombe.json\n",
            "    california_housing_test.csv\n",
            "    mnist_test.csv\n",
            "    california_housing_train.csv\n",
            "    mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RD83Q7vsUzze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "IxIumSHaqtW8",
        "outputId": "d9050efc-540f-4b9a-bddb-6640366aebdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Write the main file\\n!mkdir -p modules/bootstrap\\n!echo -e \"# modules/bootstrap/deps.py\\n$(cat << \\'EOF\\'\\nimport os\\nimport subprocess\\n\\nREQUIRED_LIBS = [\\n    \\'streamlit==1.38.0\\',\\n    \\'pandas==2.2.2\\',\\n    \\'numpy==1.26.4\\',\\n    \\'scikit-learn==1.5.1\\',\\n    \\'xgboost==2.1.1\\',\\n    \\'plotly==5.22.0\\',\\n    \\'faker==28.1.0\\',\\n    \\'openpyxl==3.1.5\\',\\n    \\'reportlab==4.2.2\\',\\n    \\'pytest==8.3.2\\',\\n    \\'shap==0.46.0\\'\\n]\\n\\ndef install_deps():\\n    os.makedirs(\\'data\\', exist_ok=True)\\n    marker_path = os.path.join(\\'data\\', \\'.deps_ok\\')\\n    \\n    if not os.path.exists(marker_path):\\n        for lib in REQUIRED_LIBS:\\n            try:\\n                __import__(lib.split(\\'==\\')[0])\\n            except ImportError:\\n                subprocess.check_call([\\'pip\\', \\'install\\', lib])\\n        with open(marker_path, \\'w\\') as f:\\n            f.write(\\'OK\\')\\n        print(\"Dependencies installed successfully.\")\\n    else:\\n        print(\"Dependencies already installed.\")\\n\\nif __name__ == \\'__main__\\':\\n    install_deps()\\nEOF\\n)\" > modules/bootstrap/deps.py\\n\\n# Write the test file\\n!mkdir -p tests\\n!echo -e \"# tests/test_bootstrap.py\\nimport os\\nimport pytest\\n\\ndef test_deps_install():\\n    from modules.bootstrap import deps\\n    deps.install_deps()\\n    marker_path = os.path.join(\\'data\\', \\'.deps_ok\\')\\n    assert os.path.exists(marker_path), \\'Marker file not created\\'\\n    with open(marker_path, \\'r\\') as f:\\n        assert f.read() == \\'OK\\', \\'Marker file content incorrect\\'\" > tests/test_bootstrap.py\\n\\n# Run the script\\n!python modules/bootstrap/deps.py\\n\\n# Run the test\\n!pytest tests/test_bootstrap.py -v\\n\\n# Verify marker file\\n!ls data\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "# Colab cell to create, run, and test modules/bootstrap/deps.py\n",
        "# Run this entire block in Colab to execute all steps\n",
        "\n",
        "# %%writefile modules/bootstrap/deps.py\n",
        "# Estimated line count: 50\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# List of required free libraries\n",
        "REQUIRED_LIBS = [\n",
        "    'streamlit==1.38.0',\n",
        "    'pandas==2.2.2',\n",
        "    'numpy==1.26.4',\n",
        "    'scikit-learn==1.5.1',\n",
        "    'xgboost==2.1.1',\n",
        "    'plotly==5.22.0',\n",
        "    'faker==28.1.0',\n",
        "    'openpyxl==3.1.5',\n",
        "    'reportlab==4.2.2',\n",
        "    'pytest==8.3.2',\n",
        "    'shap==0.46.0'  # For explainability\n",
        "]\n",
        "\n",
        "def install_deps():\n",
        "    \"\"\"Install required libraries and create marker file.\"\"\"\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    marker_path = os.path.join('data', '.deps_ok')\n",
        "\n",
        "    if not os.path.exists(marker_path):\n",
        "        for lib in REQUIRED_LIBS:\n",
        "            try:\n",
        "                __import__(lib.split('==')[0])\n",
        "            except ImportError:\n",
        "                subprocess.check_call(['pip', 'install', lib])\n",
        "        with open(marker_path, 'w') as f:\n",
        "            f.write('OK')\n",
        "        print(\"Dependencies installed successfully.\")\n",
        "    else:\n",
        "        print(\"Dependencies already installed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    install_deps()\n",
        "\n",
        "# Test code (will be written to tests/test_bootstrap.py)\n",
        "\"\"\"\n",
        "# tests/test_bootstrap.py\n",
        "import os\n",
        "import pytest\n",
        "\n",
        "def test_deps_install():\n",
        "    from modules.bootstrap import deps\n",
        "    deps.install_deps()\n",
        "    marker_path = os.path.join('data', '.deps_ok')\n",
        "    assert os.path.exists(marker_path), \"Marker file not created\"\n",
        "    with open(marker_path, 'r') as f:\n",
        "        assert f.read() == 'OK', \"Marker file content incorrect\"\n",
        "\"\"\"\n",
        "\n",
        "# Colab commands to execute (included in this cell)\n",
        "\"\"\"\n",
        "# Write the main file\n",
        "!mkdir -p modules/bootstrap\n",
        "!echo -e \"# modules/bootstrap/deps.py\\n$(cat << 'EOF'\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "REQUIRED_LIBS = [\n",
        "    'streamlit==1.38.0',\n",
        "    'pandas==2.2.2',\n",
        "    'numpy==1.26.4',\n",
        "    'scikit-learn==1.5.1',\n",
        "    'xgboost==2.1.1',\n",
        "    'plotly==5.22.0',\n",
        "    'faker==28.1.0',\n",
        "    'openpyxl==3.1.5',\n",
        "    'reportlab==4.2.2',\n",
        "    'pytest==8.3.2',\n",
        "    'shap==0.46.0'\n",
        "]\n",
        "\n",
        "def install_deps():\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    marker_path = os.path.join('data', '.deps_ok')\n",
        "\n",
        "    if not os.path.exists(marker_path):\n",
        "        for lib in REQUIRED_LIBS:\n",
        "            try:\n",
        "                __import__(lib.split('==')[0])\n",
        "            except ImportError:\n",
        "                subprocess.check_call(['pip', 'install', lib])\n",
        "        with open(marker_path, 'w') as f:\n",
        "            f.write('OK')\n",
        "        print(\"Dependencies installed successfully.\")\n",
        "    else:\n",
        "        print(\"Dependencies already installed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    install_deps()\n",
        "EOF\n",
        ")\" > modules/bootstrap/deps.py\n",
        "\n",
        "# Write the test file\n",
        "!mkdir -p tests\n",
        "!echo -e \"# tests/test_bootstrap.py\\nimport os\\nimport pytest\\n\\ndef test_deps_install():\\n    from modules.bootstrap import deps\\n    deps.install_deps()\\n    marker_path = os.path.join('data', '.deps_ok')\\n    assert os.path.exists(marker_path), 'Marker file not created'\\n    with open(marker_path, 'r') as f:\\n        assert f.read() == 'OK', 'Marker file content incorrect'\" > tests/test_bootstrap.py\n",
        "\n",
        "# Run the script\n",
        "!python modules/bootstrap/deps.py\n",
        "\n",
        "# Run the test\n",
        "!pytest tests/test_bootstrap.py -v\n",
        "\n",
        "# Verify marker file\n",
        "!ls data\n",
        "\"\"\"\n",
        "\n",
        "# Expected output:\n",
        "# Dependencies installed successfully.\n",
        "# ============================= test session starts =============================\n",
        "# tests/test_bootstrap.py::test_deps_install PASSED\n",
        "# =========================== 1 passed in 0.XXs ===========================\n",
        "# .deps_ok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Up# Colab cell to create, run, and test modules/bootstrap/drive_persist.py and modules/bootstrap/tunnel.py\n",
        "# Run this entire block in Colab to execute all steps\n",
        "\n",
        "# %%writefile modules/bootstrap/drive_persist.py\n",
        "# Estimated line count: 80\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import hashlib\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "class DrivePersist:\n",
        "    \"\"\"Manage Google Drive persistence for Colab with atomic writes and retries.\"\"\"\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive/loan_iq\"\n",
        "    MOUNT_PATH = \"/content/drive\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Mount Drive and ensure root directory.\"\"\"\n",
        "        if not os.path.exists(self.MOUNT_PATH):\n",
        "            drive.mount(self.MOUNT_PATH)\n",
        "        os.makedirs(self.DRIVE_ROOT, exist_ok=True)\n",
        "\n",
        "    def persist_path(self, local_path):\n",
        "        \"\"\"Get Drive path for a local file.\"\"\"\n",
        "        relative_path = os.path.relpath(local_path, start=os.getcwd())\n",
        "        return os.path.join(self.DRIVE_ROOT, relative_path)\n",
        "\n",
        "    def save_file(self, local_path, data, max_retries=3):\n",
        "        \"\"\"Save data to Drive with atomic writes and retries.\"\"\"\n",
        "        drive_path = self.persist_path(local_path)\n",
        "        os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                temp_path = drive_path + '.tmp'\n",
        "                with open(temp_path, 'wb') as f:\n",
        "                    pickle.dump(data, f)\n",
        "                os.rename(temp_path, drive_path)\n",
        "\n",
        "                # Compute and save hash\n",
        "                file_hash = hashlib.md5(str(data).encode()).hexdigest()\n",
        "                with open(drive_path + '.hash', 'w') as f:\n",
        "                    f.write(file_hash)\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Retry {attempt + 1}/{max_retries} for {drive_path}: {e}\")\n",
        "                time.sleep(1)\n",
        "        return False\n",
        "\n",
        "    def load_file(self, local_path):\n",
        "        \"\"\"Load data from Drive, verify hash.\"\"\"\n",
        "        drive_path = self.persist_path(local_path)\n",
        "        if not os.path.exists(drive_path):\n",
        "            return None\n",
        "        try:\n",
        "            with open(drive_path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            hash_path = drive_path + '.hash'\n",
        "            if os.path.exists(hash_path):\n",
        "                with open(hash_path, 'r') as f:\n",
        "                    stored_hash = f.read()\n",
        "                current_hash = hashlib.md5(str(data).encode()).hexdigest()\n",
        "                if stored_hash != current_hash:\n",
        "                    print(f\"Hash mismatch for {drive_path}\")\n",
        "                    return None\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {drive_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    persist = DrivePersist()\n",
        "    test_data = {'test': 'data'}\n",
        "    test_path = os.path.join('data', 'test.pkl')\n",
        "    persist.save_file(test_path, test_data)\n",
        "    print(f\"Saved to {persist.persist_path(test_path)}\")\n",
        "    loaded = persist.load_file(test_path)\n",
        "    print(f\"Loaded: {loaded}\")\n",
        "\n",
        "# %%writefile modules/bootstrap/tunnel.py\n",
        "# Estimated line count: 60\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\"\n",
        "\n",
        "def setup_tunnel(port=8501):\n",
        "    \"\"\"Set up Ngrok tunnel for Streamlit with hardcoded authtoken.\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call(['pip', 'install', 'pyngrok==7.2.0'])\n",
        "        from pyngrok import ngrok\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "        # Terminate existing tunnels\n",
        "        ngrok.kill()\n",
        "\n",
        "        # Start new tunnel\n",
        "        tunnel = ngrok.connect(port, bind_tls=True)\n",
        "        public_url = tunnel.public_url\n",
        "        print(f\"Streamlit accessible at: {public_url}\")\n",
        "        return public_url\n",
        "    except Exception as e:\n",
        "        print(f\"Tunnel setup failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_new_tunnel_url():\n",
        "    \"\"\"Command to get new Ngrok URL (for README/runbook).\"\"\"\n",
        "    cmd = f\"!ngrok http 8501 --authtoken {NGROK_AUTH_TOKEN}\"\n",
        "    print(f\"Run this in Colab to get new URL:\\n{cmd}\")\n",
        "    return cmd\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    setup_tunnel()\n",
        "    get_new_tunnel_url()\n",
        "\n",
        "# Test code (will be written to tests/test_bootstrap.py)\n",
        "\"\"\"\n",
        "# tests/test_bootstrap.py\n",
        "import os\n",
        "import pytest\n",
        "from modules.bootstrap import drive_persist, tunnel\n",
        "\n",
        "def test_drive_persist():\n",
        "    persist = drive_persist.DrivePersist()\n",
        "    test_data = {'test': 'data'}\n",
        "    test_path = os.path.join('data', 'test.pkl')\n",
        "    assert persist.save_file(test_path, test_data), \"Failed to save to Drive\"\n",
        "    loaded = persist.load_file(test_path)\n",
        "    assert loaded == test_data, \"Loaded data mismatch\"\n",
        "    assert os.path.exists(persist.persist_path(test_path) + '.hash'), \"Hash file missing\"\n",
        "\n",
        "def test_tunnel_setup():\n",
        "    public_url = tunnel.setup_tunnel()\n",
        "    assert public_url is None or isinstance(public_url, str), \"Invalid tunnel URL\"\n",
        "    cmd = tunnel.get_new_tunnel_url()\n",
        "    assert NGROK_AUTH_TOKEN in cmd, \"Ngrok authtoken not in command\"\n",
        "\"\"\"\n",
        "\n",
        "# Colab commands to execute (run this entire cell)\n",
        "\"\"\"\n",
        "# Create directories\n",
        "!mkdir -p modules/bootstrap tests data\n",
        "\n",
        "# Write drive_persist.py\n",
        "!echo -e \"# modules/bootstrap/drive_persist.py\\n$(cat << 'EOF'\n",
        "import os\n",
        "from google.colab import drive\n",
        "import hashlib\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "class DrivePersist:\n",
        "    DRIVE_ROOT = \\\"/content/drive/MyDrive/loan_iq\\\"\n",
        "    MOUNT_PATH = \\\"/content/drive\\\"\n",
        "\n",
        "    def __init__(self):\n",
        "        if not os.path.exists(self.MOUNT_PATH):\n",
        "            drive.mount(self.MOUNT_PATH)\n",
        "        os.makedirs(self.DRIVE_ROOT, exist_ok=True)\n",
        "\n",
        "    def persist_path(self, local_path):\n",
        "        relative_path = os.path.relpath(local_path, start=os.getcwd())\n",
        "        return os.path.join(self.DRIVE_ROOT, relative_path)\n",
        "\n",
        "    def save_file(self, local_path, data, max_retries=3):\n",
        "        os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                temp_path = drive_path + '.tmp'\n",
        "                with open(temp_path, 'wb') as f:\n",
        "                    pickle.dump(data, f)\n",
        "                os.rename(temp_path, drive_path)\n",
        "                file_hash = hashlib.md5(str(data).encode()).hexdigest()\n",
        "                with open(drive_path + '.hash', 'w') as f:\n",
        "                    f.write(file_hash)\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\\\"Retry {attempt + 1}/{max_retries} for {drive_path}: {e}\\\")\n",
        "                time.sleep(1)\n",
        "        return False\n",
        "\n",
        "    def load_file(self, local_path):\n",
        "        drive_path = self.persist_path(local_path)\n",
        "        if not os.path.exists(drive_path):\n",
        "            return None\n",
        "        try:\n",
        "            with open(drive_path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            hash_path = drive_path + '.hash'\n",
        "            if os.path.exists(hash_path):\n",
        "                with open(hash_path, 'r') as f:\n",
        "                    stored_hash = f.read()\n",
        "                current_hash = hashlib.md5(str(data).encode()).hexdigest()\n",
        "                if stored_hash != current_hash:\n",
        "                    print(f\\\"Hash mismatch for {drive_path}\\\")\n",
        "                    return None\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\\\"Error loading {drive_path}: {e}\\\")\n",
        "            return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    persist = DrivePersist()\n",
        "    test_data = {'test': 'data'}\n",
        "    test_path = os.path.join('data', 'test.pkl')\n",
        "    persist.save_file(test_path, test_data)\n",
        "    print(f\\\"Saved to {persist.persist_path(test_path)}\\\")\n",
        "    loaded = persist.load_file(test_path)\n",
        "    print(f\\\"Loaded: {loaded}\\\")\n",
        "EOF\n",
        ")\" > modules/bootstrap/drive_persist.py\n",
        "\n",
        "# Write tunnel.py\n",
        "!echo -e \"# modules/bootstrap/tunnel.py\\n$(cat << 'EOF'\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "NGROK_AUTH_TOKEN = \\\"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\\\"\n",
        "\n",
        "def setup_tunnel(port=8501):\n",
        "    try:\n",
        "        subprocess.check_call(['pip', 'install', 'pyngrok==7.2.0'])\n",
        "        from pyngrok import ngrok\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        ngrok.kill()\n",
        "        tunnel = ngrok.connect(port, bind_tls=True)\n",
        "        public_url = tunnel.public_url\n",
        "        print(f\\\"Streamlit accessible at: {public_url}\\\")\n",
        "        return public_url\n",
        "    except Exception as e:\n",
        "        print(f\\\"Tunnel setup failed: {e}\\\")\n",
        "        return None\n",
        "\n",
        "def get_new_tunnel_url():\n",
        "    cmd = f\\\"!ngrok http 8501 --authtoken {NGROK_AUTH_TOKEN}\\\"\n",
        "    print(f\\\"Run this in Colab to get new URL:\\n{cmd}\\\")\n",
        "    return cmd\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    setup_tunnel()\n",
        "    get_new_tunnel_url()\n",
        "EOF\n",
        ")\" > modules/bootstrap/tunnel.py\n",
        "\n",
        "# Write test file (appending to existing test_bootstrap.py)\n",
        "!echo -e \"# tests/test_bootstrap.py\\n$(cat << 'EOF'\n",
        "import os\n",
        "import pytest\n",
        "from modules.bootstrap import drive_persist, tunnel\n",
        "\n",
        "def test_deps_install():\n",
        "    from modules.bootstrap import deps\n",
        "    deps.install_deps()\n",
        "    marker_path = os.path.join('data', '.deps_ok')\n",
        "    assert os.path.exists(marker_path), 'Marker file not created'\n",
        "    with open(marker_path, 'r') as f:\n",
        "        assert f.read() == 'OK', 'Marker file content incorrect'\n",
        "\n",
        "def test_drive_persist():\n",
        "    persist = drive_persist.DrivePersist()\n",
        "    test_data = {'test': 'data'}\n",
        "    test_path = os.path.join('data', 'test.pkl')\n",
        "    assert persist.save_file(test_path, test_data), 'Failed to save to Drive'\n",
        "    loaded = persist.load_file(test_path)\n",
        "    assert loaded == test_data, 'Loaded data mismatch'\n",
        "    assert os.path.exists(persist.persist_path(test_path) + '.hash'), 'Hash file missing'\n",
        "\n",
        "def test_tunnel_setup():\n",
        "    public_url = tunnel.setup_tunnel()\n",
        "    assert public_url is None or isinstance(public_url, str), 'Invalid tunnel URL'\n",
        "    cmd = tunnel.get_new_tunnel_url()\n",
        "    assert NGROK_AUTH_TOKEN in cmd, 'Ngrok authtoken not in command'\n",
        "EOF\n",
        ")\" > tests/test_bootstrap.py\n",
        "\n",
        "# Run dependencies (ensure environment)\n",
        "!python modules/bootstrap/deps.py\n",
        "\n",
        "# Run drive_persist.py (will prompt for Google Drive auth code)\n",
        "!python modules/bootstrap/drive_persist.py\n",
        "\n",
        "# Run tunnel.py (may take time to set up Ngrok)\n",
        "!python modules/bootstrap/tunnel.py\n",
        "\n",
        "# Run tests\n",
        "!pytest tests/test_bootstrap.py -v\n",
        "\n",
        "# Verify files\n",
        "!ls data\n",
        "!ls modules/bootstrap\n",
        "\"\"\"\n",
        "\n",
        "# Expected output:\n",
        "# Dependencies installed successfully.\n",
        "# Mounted at /content/drive\n",
        "# Saved to /content/drive/MyDrive/loan_iq/data/test.pkl\n",
        "# Loaded: {'test': 'data'}\n",
        "# Streamlit accessible at: https://<ngrok-url>.ngrok.io\n",
        "# Run this in Colab to get new URL:\n",
        "# !ngrok http 8501 --authtoken 31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\n",
        "# ============================= test session starts =============================\n",
        "# tests/test_bootstrap.py::test_deps_install PASSED\n",
        "# tests/test_bootstrap.py::test_drive_persist PASSED\n",
        "# tests/test_bootstrap.py::test_tunnel_setup PASSED\n",
        "# =========================== 3 passed in 0.XXs ===========================\n",
        "# test.pkl  test.pkl.hash  .deps_ok\n",
        "# deps.py  drive_persist.py  tunnel.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "b2epouRCrnHJ",
        "outputId": "ea1d5b6d-56e5-427b-b5f6-784e0715699a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Saved to /content/drive/MyDrive/loan_iq/data/test.pkl\n",
            "Loaded: {'test': 'data'}\n",
            "Streamlit accessible at: https://f7e668fceeb4.ngrok-free.app\n",
            "Run this in Colab to get new URL:\n",
            "!ngrok http 8501 --authtoken 31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Create directories\\n!mkdir -p modules/bootstrap tests data\\n\\n# Write drive_persist.py\\n!echo -e \"# modules/bootstrap/drive_persist.py\\n$(cat << \\'EOF\\'\\nimport os\\nfrom google.colab import drive\\nimport hashlib\\nimport pickle\\nimport time\\n\\nclass DrivePersist:\\n    DRIVE_ROOT = \"/content/drive/MyDrive/loan_iq\"\\n    MOUNT_PATH = \"/content/drive\"\\n    \\n    def __init__(self):\\n        if not os.path.exists(self.MOUNT_PATH):\\n            drive.mount(self.MOUNT_PATH)\\n        os.makedirs(self.DRIVE_ROOT, exist_ok=True)\\n    \\n    def persist_path(self, local_path):\\n        relative_path = os.path.relpath(local_path, start=os.getcwd())\\n        return os.path.join(self.DRIVE_ROOT, relative_path)\\n    \\n    def save_file(self, local_path, data, max_retries=3):\\n        os.makedirs(os.path.dirname(drive_path), exist_ok=True)\\n        for attempt in range(max_retries):\\n            try:\\n                temp_path = drive_path + \\'.tmp\\'\\n                with open(temp_path, \\'wb\\') as f:\\n                    pickle.dump(data, f)\\n                os.rename(temp_path, drive_path)\\n                file_hash = hashlib.md5(str(data).encode()).hexdigest()\\n                with open(drive_path + \\'.hash\\', \\'w\\') as f:\\n                    f.write(file_hash)\\n                return True\\n            except Exception as e:\\n                print(f\"Retry {attempt + 1}/{max_retries} for {drive_path}: {e}\")\\n                time.sleep(1)\\n        return False\\n    \\n    def load_file(self, local_path):\\n        drive_path = self.persist_path(local_path)\\n        if not os.path.exists(drive_path):\\n            return None\\n        try:\\n            with open(drive_path, \\'rb\\') as f:\\n                data = pickle.load(f)\\n            hash_path = drive_path + \\'.hash\\'\\n            if os.path.exists(hash_path):\\n                with open(hash_path, \\'r\\') as f:\\n                    stored_hash = f.read()\\n                current_hash = hashlib.md5(str(data).encode()).hexdigest()\\n                if stored_hash != current_hash:\\n                    print(f\"Hash mismatch for {drive_path}\")\\n                    return None\\n            return data\\n        except Exception as e:\\n            print(f\"Error loading {drive_path}: {e}\")\\n            return None\\n\\nif __name__ == \\'__main__\\':\\n    persist = DrivePersist()\\n    test_data = {\\'test\\': \\'data\\'}\\n    test_path = os.path.join(\\'data\\', \\'test.pkl\\')\\n    persist.save_file(test_path, test_data)\\n    print(f\"Saved to {persist.persist_path(test_path)}\")\\n    loaded = persist.load_file(test_path)\\n    print(f\"Loaded: {loaded}\")\\nEOF\\n)\" > modules/bootstrap/drive_persist.py\\n\\n# Write tunnel.py\\n!echo -e \"# modules/bootstrap/tunnel.py\\n$(cat << \\'EOF\\'\\nimport os\\nimport subprocess\\nimport time\\n\\nNGROK_AUTH_TOKEN = \"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\"\\n\\ndef setup_tunnel(port=8501):\\n    try:\\n        subprocess.check_call([\\'pip\\', \\'install\\', \\'pyngrok==7.2.0\\'])\\n        from pyngrok import ngrok\\n        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\\n        ngrok.kill()\\n        tunnel = ngrok.connect(port, bind_tls=True)\\n        public_url = tunnel.public_url\\n        print(f\"Streamlit accessible at: {public_url}\")\\n        return public_url\\n    except Exception as e:\\n        print(f\"Tunnel setup failed: {e}\")\\n        return None\\n\\ndef get_new_tunnel_url():\\n    cmd = f\"!ngrok http 8501 --authtoken {NGROK_AUTH_TOKEN}\"\\n    print(f\"Run this in Colab to get new URL:\\n{cmd}\")\\n    return cmd\\n\\nif __name__ == \\'__main__\\':\\n    setup_tunnel()\\n    get_new_tunnel_url()\\nEOF\\n)\" > modules/bootstrap/tunnel.py\\n\\n# Write test file (appending to existing test_bootstrap.py)\\n!echo -e \"# tests/test_bootstrap.py\\n$(cat << \\'EOF\\'\\nimport os\\nimport pytest\\nfrom modules.bootstrap import drive_persist, tunnel\\n\\ndef test_deps_install():\\n    from modules.bootstrap import deps\\n    deps.install_deps()\\n    marker_path = os.path.join(\\'data\\', \\'.deps_ok\\')\\n    assert os.path.exists(marker_path), \\'Marker file not created\\'\\n    with open(marker_path, \\'r\\') as f:\\n        assert f.read() == \\'OK\\', \\'Marker file content incorrect\\'\\n\\ndef test_drive_persist():\\n    persist = drive_persist.DrivePersist()\\n    test_data = {\\'test\\': \\'data\\'}\\n    test_path = os.path.join(\\'data\\', \\'test.pkl\\')\\n    assert persist.save_file(test_path, test_data), \\'Failed to save to Drive\\'\\n    loaded = persist.load_file(test_path)\\n    assert loaded == test_data, \\'Loaded data mismatch\\'\\n    assert os.path.exists(persist.persist_path(test_path) + \\'.hash\\'), \\'Hash file missing\\'\\n\\ndef test_tunnel_setup():\\n    public_url = tunnel.setup_tunnel()\\n    assert public_url is None or isinstance(public_url, str), \\'Invalid tunnel URL\\'\\n    cmd = tunnel.get_new_tunnel_url()\\n    assert NGROK_AUTH_TOKEN in cmd, \\'Ngrok authtoken not in command\\'\\nEOF\\n)\" > tests/test_bootstrap.py\\n\\n# Run dependencies (ensure environment)\\n!python modules/bootstrap/deps.py\\n\\n# Run drive_persist.py (will prompt for Google Drive auth code)\\n!python modules/bootstrap/drive_persist.py\\n\\n# Run tunnel.py (may take time to set up Ngrok)\\n!python modules/bootstrap/tunnel.py\\n\\n# Run tests\\n!pytest tests/test_bootstrap.py -v\\n\\n# Verify files\\n!ls data\\n!ls modules/bootstrap\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell to create, run, and test modules/core/config.py and modules/core/db.py\n",
        "# Run this entire block in Colab to execute all steps\n",
        "\n",
        "# Ensure Python path includes current directory for module imports\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# Create directories to prevent path errors\n",
        "!mkdir -p modules/core tests data models data/reports\n",
        "!ls modules/core || echo \"Directory modules/core created\"\n",
        "!ls data || echo \"Directory data created\"\n",
        "\n",
        "# Write config.py\n",
        "!echo -e \"# modules/core/config.py\\n# Estimated line count: 60\\n\\nimport sys\\nimport os\\nsys.path.append(os.getcwd())\\nimport random\\nimport numpy as np\\n\\n# Hardcoded admin credentials\\nADMIN_CREDENTIALS = {\\n    \\\"username\\\": \\\"admin\\\",\\n    \\\"password\\\": \\\"Shady868\\\"\\n}\\n\\n# Random seeds for reproducibility\\nSEEDS = {\\n    \\\"faker\\\": 42,\\n    \\\"numpy\\\": 42,\\n    \\\"random\\\": 42\\n}\\n\\n# App configuration\\nCONFIG = {\\n    \\\"data_dir\\\": os.path.join(\\\"data\\\"),\\n    \\\"model_dir\\\": os.path.join(\\\"models\\\"),\\n    \\\"report_dir\\\": os.path.join(\\\"data\\\", \\\"reports\\\"),\\n    \\\"db_path\\\": os.path.join(\\\"data\\\", \\\"loan_iq.db\\\"),\\n    \\\"drive_root\\\": \\\"/content/drive/MyDrive/loan_iq\\\",\\n    \\\"streamlit_port\\\": 8501,\\n    \\\"fraud_types\\\": [\\\"ghost_client\\\", \\\"duplicate_id\\\", \\\"missed_payment\\\", \\\"identity_theft\\\"],\\n    \\\"regions\\\": [\\\"urban\\\", \\\"rural\\\", \\\"semi_urban\\\"],\\n    \\\"max_clients_batch\\\": 70000,\\n    \\\"default_batch_size\\\": 1000\\n}\\n\\ndef init_seeds():\\n    \\\"\\\"\\\"Initialize random seeds for reproducibility.\\\"\\\"\\\"\\n    random.seed(SEEDS[\\\"random\\\"])\\n    np.random.seed(SEEDS[\\\"numpy\\\"])\\n\\ndef get_config():\\n    \\\"\\\"\\\"Return config dictionary, ensure directories exist.\\\"\\\"\\\"\\n    os.makedirs(CONFIG[\\\"data_dir\\\"], exist_ok=True)\\n    os.makedirs(CONFIG[\\\"model_dir\\\"], exist_ok=True)\\n    os.makedirs(CONFIG[\\\"report_dir\\\"], exist_ok=True)\\n    return CONFIG\\n\\nif __name__ == \\\"__main__\\\":\\n    init_seeds()\\n    config = get_config()\\n    print(f\\\"Config loaded: {config}\\\")\" > modules/core/config.py\n",
        "\n",
        "# Write db.py\n",
        "!echo -e \"# modules/core/db.py\\n# Estimated line count: 120\\n\\nimport sys\\nimport os\\nsys.path.append(os.getcwd())\\nimport sqlite3\\nimport json\\nfrom datetime import datetime\\ntry:\\n    from modules.core import config\\nexcept ImportError as e:\\n    print(f\\\"Import error: {e}\\\")\\n    raise\\n\\nclass DB:\\n    \\\"\\\"\\\"SQLite database wrapper for Loan IQ.\\\"\\\"\\\"\\n    def __init__(self):\\n        print(f\\\"sys.path: {sys.path}\\\")  # Debug path\\n        self.db_path = config.get_config()[\\\"db_path\\\"]\\n        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\\n        self.conn = sqlite3.connect(self.db_path)\\n        self.cursor = self.conn.cursor()\\n        self.create_tables()\\n\\n    def create_tables(self):\\n        \\\"\\\"\\\"Create database tables.\\\"\\\"\\\"\\n        tables = [\\n            \\\"CREATE TABLE IF NOT EXISTS users (user_id INTEGER PRIMARY KEY, username TEXT UNIQUE, password TEXT, role TEXT)\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS clients (client_id TEXT PRIMARY KEY, name TEXT, branch TEXT, region TEXT, income REAL, created_at TIMESTAMP)\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS loans (loan_id TEXT PRIMARY KEY, client_id TEXT, amount REAL, status TEXT, start_date TIMESTAMP, FOREIGN KEY (client_id) REFERENCES clients(client_id))\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS transactions (transaction_id TEXT PRIMARY KEY, loan_id TEXT, amount REAL, date TIMESTAMP, type TEXT, FOREIGN KEY (loan_id) REFERENCES loans(loan_id))\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS models (model_id TEXT PRIMARY KEY, type TEXT, version TEXT, created_at TIMESTAMP)\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS model_versions (version_id TEXT PRIMARY KEY, model_id TEXT, config_json TEXT, data_hash TEXT, metrics_json TEXT, commit_ref TEXT, comments TEXT, created_at TIMESTAMP, FOREIGN KEY (model_id) REFERENCES models(model_id))\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS audit_logs (log_id INTEGER PRIMARY KEY AUTOINCREMENT, actor_id TEXT, actor_role TEXT, action TEXT, target_id TEXT, target_type TEXT, reason TEXT, timestamp TIMESTAMP, before_snapshot TEXT, after_snapshot TEXT, reversible BOOLEAN, reversal_id INTEGER)\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS simulations (sim_id TEXT PRIMARY KEY, user_id TEXT, params_json TEXT, created_at TIMESTAMP)\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS reports (report_id TEXT PRIMARY KEY, type TEXT, path TEXT, created_at TIMESTAMP)\\\",\\n            \\\"CREATE TABLE IF NOT EXISTS assets (asset_id TEXT PRIMARY KEY, path TEXT, type TEXT, created_at TIMESTAMP)\\\"\\n        ]\\n        for table_sql in tables:\\n            self.cursor.execute(table_sql)\\n        self.conn.commit()\\n\\n    def log_action(self, actor_id, actor_role, action, target_id, target_type, reason, before_snapshot, after_snapshot, reversible=False):\\n        \\\"\\\"\\\"Log an admin action to audit_logs.\\\"\\\"\\\"\\n        timestamp = datetime.utcnow().isoformat()\\n        snapshot_before = json.dumps(before_snapshot) if before_snapshot else \\\"\\\"\\n        snapshot_after = json.dumps(after_snapshot) if after_snapshot else \\\"\\\"\\n        self.cursor.execute(\\n            \\\"INSERT INTO audit_logs (actor_id, actor_role, action, target_id, target_type, reason, timestamp, before_snapshot, after_snapshot, reversible) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\\\",\\n            (actor_id, actor_role, action, target_id, target_type, reason, timestamp, snapshot_before, snapshot_after, reversible)\\n        )\\n        self.conn.commit()\\n        return self.cursor.lastrowid\\n\\n    def get_audit_trail(self, target_id=None, target_type=None):\\n        \\\"\\\"\\\"Retrieve audit logs, optionally filtered.\\\"\\\"\\\"\\n        query = \\\"SELECT * FROM audit_logs\\\"\\n        params = []\\n        if target_id and target_type:\\n            query += \\\" WHERE target_id = ? AND target_type = ?\\\"\\n            params = [target_id, target_type]\\n        self.cursor.execute(query, params)\\n        return self.cursor.fetchall()\\n\\n    def rollback_action(self, action_id):\\n        \\\"\\\"\\\"Attempt to rollback an action if reversible.\\\"\\\"\\\"\\n        self.cursor.execute(\\\"SELECT reversible, before_snapshot, target_id, target_type, action FROM audit_logs WHERE log_id = ?\\\", (action_id,))\\n        result = self.cursor.fetchone()\\n        if not result or not result[0]:\\n            return False\\n        before_snapshot = json.loads(result[1]) if result[1] else {}\\n        target_id, target_type, action = result[2], result[3], result[4]\\n        if target_type == \\\"user\\\" and action == \\\"edit\\\":\\n            self.cursor.execute(\\\"UPDATE users SET username = ?, password = ?, role = ? WHERE user_id = ?\\\",\\n                              (before_snapshot.get(\\\"username\\\"), before_snapshot.get(\\\"password\\\"), before_snapshot.get(\\\"role\\\"), target_id))\\n            self.conn.commit()\\n            return True\\n        return False\\n\\n    def close(self):\\n        \\\"\\\"\\\"Close database connection.\\\"\\\"\\\"\\n        self.conn.close()\\n\\nif __name__ == \\\"__main__\\\":\\n    db = DB()\\n    db.cursor.execute(\\\"INSERT OR IGNORE INTO users (user_id, username, password, role) VALUES (?, ?, ?, ?)\\\",\\n                     (1, \\\"admin\\\", \\\"Shady868\\\", \\\"admin\\\"))\\n    db.conn.commit()\\n    db.log_action(\\\"1\\\", \\\"admin\\\", \\\"init\\\", \\\"1\\\", \\\"user\\\", \\\"Initialize admin user\\\", {}, {\\\"username\\\": \\\"admin\\\"})\\n    print(\\\"Database initialized.\\\")\\n    db.close()\" > modules/core/db.py\n",
        "\n",
        "# Write test file\n",
        "!echo -e \"# tests/test_core.py\\n# Estimated line count: 20\\n\\nimport sys\\nimport os\\nsys.path.append(os.getcwd())\\nfrom modules.core import config, db\\n\\ndef test_config_init():\\n    cfg = config.get_config()\\n    assert os.path.exists(cfg[\\\"data_dir\\\"]), \\\"Data directory not created\\\"\\n    assert cfg[\\\"streamlit_port\\\"] == 8501, \\\"Incorrect port\\\"\\n    assert config.ADMIN_CREDENTIALS[\\\"username\\\"] == \\\"admin\\\", \\\"Admin username incorrect\\\"\\n\\ndef test_db_create_and_log():\\n    database = db.DB()\\n    database.cursor.execute(\\\"SELECT name FROM sqlite_master WHERE type='table' AND name='audit_logs'\\\")\\n    assert database.cursor.fetchone(), \\\"Audit logs table not created\\\"\\n    log_id = database.log_action(\\\"1\\\", \\\"admin\\\", \\\"test_action\\\", \\\"test_id\\\", \\\"test_type\\\", \\\"Test reason\\\", {\\\"key\\\": \\\"before\\\"}, {\\\"key\\\": \\\"after\\\"}, True)\\n    assert log_id, \\\"Failed to log action\\\"\\n    audit_logs = database.get_audit_trail(\\\"test_id\\\", \\\"test_type\\\")\\n    assert len(audit_logs) > 0, \\\"Audit log not recorded\\\"\\n    database.close()\" > tests/test_core.py\n",
        "\n",
        "# Ensure dependencies are installed\n",
        "!python modules/bootstrap/deps.py\n",
        "\n",
        "# Verify directories\n",
        "!ls modules/core || echo \"modules/core not found\"\n",
        "!ls data || echo \"data not found\"\n",
        "\n",
        "# Run config.py\n",
        "!python modules/core/config.py\n",
        "\n",
        "# Run db.py\n",
        "!python modules/core/db.py\n",
        "\n",
        "# Run tests\n",
        "!pytest tests/test_core.py -v\n",
        "\n",
        "# Verify files\n",
        "!ls modules/core\n",
        "!ls data\n",
        "\n",
        "# Expected output:\n",
        "# Dependencies installed successfully.\n",
        "# modules/core created\n",
        "# data created\n",
        "# Config loaded: {'data_dir': 'data', 'model_dir': 'models', 'report_dir': 'data/reports', 'db_path': 'data/loan_iq.db', 'drive_root': '/content/drive/MyDrive/loan_iq', 'streamlit_port': 8501, 'fraud_types': ['ghost_client', 'duplicate_id', 'missed_payment', 'identity_theft'], 'regions': ['urban', 'rural', 'semi_urban'], 'max_clients_batch': 70000, 'default_batch_size': 1000}\n",
        "# sys.path: [...'/content'...]\n",
        "# Database initialized.\n",
        "# ============================= test session starts =============================\n",
        "# tests/test_core.py::test_config_init PASSED\n",
        "# tests/test_core.py::test_db_create_and_log PASSED\n",
        "# =========================== 2 passed in 0.XXs ===========================\n",
        "# config.py  db.py\n",
        "# .deps_ok  loan_iq.db  reports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyW3tlN8uLtz",
        "outputId": "76c2eb26-287d-4539-b47a-4003f49a03ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reports\n",
            "python3: can't open file '/content/modules/bootstrap/deps.py': [Errno 2] No such file or directory\n",
            "config.py  db.py\n",
            "reports\n",
            "Config loaded: {'data_dir': 'data', 'model_dir': 'models', 'report_dir': 'data/reports', 'db_path': 'data/loan_iq.db', 'drive_root': '/content/drive/MyDrive/loan_iq', 'streamlit_port': 8501, 'fraud_types': ['ghost_client', 'duplicate_id', 'missed_payment', 'identity_theft'], 'regions': ['urban', 'rural', 'semi_urban'], 'max_clients_batch': 70000, 'default_batch_size': 1000}\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content']\n",
            "/content/modules/core/db.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  timestamp = datetime.utcnow().isoformat()\n",
            "Database initialized.\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: Faker-28.1.0, anyio-4.10.0, typeguard-4.4.4, langsmith-0.4.16\n",
            "collected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_core.py::test_config_init \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 50%]\u001b[0m\n",
            "tests/test_core.py::test_db_create_and_log \u001b[32mPASSED\u001b[0m\u001b[32m                        [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
            "config.py  db.py  __pycache__\n",
            "loan_iq.db  reports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell to create, run, and test modules/bootstrap/deps.py, modules/core/config.py, modules/core/db.py, modules/core/utils.py, and modules/core/auth.py\n",
        "# Run this entire block in Colab to execute all steps\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "print(f\"Current working directory: {os.getcwd()}\")  # Debug\n",
        "\n",
        "# Create directories and reset database/dependencies\n",
        "!mkdir -p modules/bootstrap modules/core tests data models data/reports\n",
        "!rm -f data/loan_iq.db data/.deps_ok\n",
        "!ls modules/bootstrap || echo \"Directory modules/bootstrap created\"\n",
        "!ls modules/core || echo \"Directory modules/core created\"\n",
        "!ls data || echo \"Directory data created\"\n",
        "\n",
        "# Write deps.py using Python\n",
        "os.makedirs('modules/bootstrap', exist_ok=True)\n",
        "with open('modules/bootstrap/deps.py', 'w') as f:\n",
        "    f.write('''# modules/bootstrap/deps.py\n",
        "# Estimated line count: 50\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "REQUIRED_LIBS = [\n",
        "    'streamlit==1.38.0',\n",
        "    'pandas==2.2.2',\n",
        "    'numpy==1.26.4',\n",
        "    'scikit-learn==1.5.1',\n",
        "    'xgboost==2.1.1',\n",
        "    'plotly==5.22.0',\n",
        "    'faker==28.1.0',\n",
        "    'openpyxl==3.1.5',\n",
        "    'reportlab==4.2.2',\n",
        "    'pytest==8.3.2',\n",
        "    'shap==0.46.0'\n",
        "]\n",
        "\n",
        "def install_deps():\n",
        "    \"\"\"Install required libraries and create marker file.\"\"\"\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    marker_path = os.path.join('data', '.deps_ok')\n",
        "    if not os.path.exists(marker_path):\n",
        "        for lib in REQUIRED_LIBS:\n",
        "            try:\n",
        "                __import__(lib.split('==')[0])\n",
        "            except ImportError:\n",
        "                subprocess.check_call(['pip', 'install', lib])\n",
        "        with open(marker_path, 'w') as f:\n",
        "            f.write('OK')\n",
        "        print(\"Dependencies installed successfully.\")\n",
        "    else:\n",
        "        print(\"Dependencies already installed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    install_deps()\n",
        "''')\n",
        "!test -f modules/bootstrap/deps.py && echo \"deps.py created\" || echo \"Failed to create deps.py\"\n",
        "\n",
        "# Write config.py using Python\n",
        "os.makedirs('modules/core', exist_ok=True)\n",
        "with open('modules/core/config.py', 'w') as f:\n",
        "    f.write('''# modules/core/config.py\n",
        "# Estimated line count: 60\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "ADMIN_CREDENTIALS = {\n",
        "    \"username\": \"admin\",\n",
        "    \"password\": \"Shady868\"\n",
        "}\n",
        "\n",
        "SEEDS = {\n",
        "    \"faker\": 42,\n",
        "    \"numpy\": 42,\n",
        "    \"random\": 42\n",
        "}\n",
        "\n",
        "CONFIG = {\n",
        "    \"data_dir\": os.path.join(\"data\"),\n",
        "    \"model_dir\": os.path.join(\"models\"),\n",
        "    \"report_dir\": os.path.join(\"data\", \"reports\"),\n",
        "    \"db_path\": os.path.join(\"data\", \"loan_iq.db\"),\n",
        "    \"drive_root\": \"/content/drive/MyDrive/loan_iq\",\n",
        "    \"streamlit_port\": 8501,\n",
        "    \"fraud_types\": [\"ghost_client\", \"duplicate_id\", \"missed_payment\", \"identity_theft\"],\n",
        "    \"regions\": [\"urban\", \"rural\", \"semi_urban\"],\n",
        "    \"max_clients_batch\": 70000,\n",
        "    \"default_batch_size\": 1000\n",
        "}\n",
        "\n",
        "def init_seeds():\n",
        "    \"\"\"Initialize random seeds for reproducibility.\"\"\"\n",
        "    random.seed(SEEDS[\"random\"])\n",
        "    np.random.seed(SEEDS[\"numpy\"])\n",
        "\n",
        "def get_config():\n",
        "    \"\"\"Return config dictionary, ensure directories exist.\"\"\"\n",
        "    os.makedirs(CONFIG[\"data_dir\"], exist_ok=True)\n",
        "    os.makedirs(CONFIG[\"model_dir\"], exist_ok=True)\n",
        "    os.makedirs(CONFIG[\"report_dir\"], exist_ok=True)\n",
        "    return CONFIG\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    init_seeds()\n",
        "    config = get_config()\n",
        "    print(f\"Config loaded: {config}\")\n",
        "''')\n",
        "!test -f modules/core/config.py && echo \"config.py created\" || echo \"Failed to create config.py\"\n",
        "\n",
        "# Write db.py using Python\n",
        "with open('modules/core/db.py', 'w') as f:\n",
        "    f.write('''# modules/core/db.py\n",
        "# Estimated line count: 120\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "import sqlite3\n",
        "import json\n",
        "from datetime import datetime, UTC\n",
        "try:\n",
        "    from modules.core import config\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class DB:\n",
        "    \"\"\"SQLite database wrapper for Loan IQ.\"\"\"\n",
        "    def __init__(self):\n",
        "        print(f\"sys.path: {sys.path}\")  # Debug path\n",
        "        self.db_path = config.get_config()[\"db_path\"]\n",
        "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
        "        print(f\"Creating database at: {self.db_path}\")  # Debug\n",
        "        self.conn = sqlite3.connect(self.db_path)\n",
        "        self.conn.row_factory = sqlite3.Row  # Enable dict-like row access\n",
        "        self.cursor = self.conn.cursor()\n",
        "        self.create_tables()\n",
        "        print(f\"Database created: {os.path.exists(self.db_path)}\")  # Debug\n",
        "\n",
        "    def create_tables(self):\n",
        "        \"\"\"Create database tables.\"\"\"\n",
        "        tables = [\n",
        "            \"CREATE TABLE IF NOT EXISTS users (user_id TEXT PRIMARY KEY, username TEXT UNIQUE, password TEXT, role TEXT)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS clients (client_id TEXT PRIMARY KEY, name TEXT, branch TEXT, region TEXT, income REAL, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS loans (loan_id TEXT PRIMARY KEY, client_id TEXT, amount REAL, status TEXT, start_date TIMESTAMP, FOREIGN KEY (client_id) REFERENCES clients(client_id))\",\n",
        "            \"CREATE TABLE IF NOT EXISTS transactions (transaction_id TEXT PRIMARY KEY, loan_id TEXT, amount REAL, date TIMESTAMP, type TEXT, FOREIGN KEY (loan_id) REFERENCES loans(loan_id))\",\n",
        "            \"CREATE TABLE IF NOT EXISTS models (model_id TEXT PRIMARY KEY, type TEXT, version TEXT, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS model_versions (version_id TEXT PRIMARY KEY, model_id TEXT, config_json TEXT, data_hash TEXT, metrics_json TEXT, commit_ref TEXT, comments TEXT, created_at TIMESTAMP, FOREIGN KEY (model_id) REFERENCES models(model_id))\",\n",
        "            \"CREATE TABLE IF NOT EXISTS audit_logs (log_id INTEGER PRIMARY KEY AUTOINCREMENT, actor_id TEXT, actor_role TEXT, action TEXT, target_id TEXT, target_type TEXT, reason TEXT, timestamp TIMESTAMP, before_snapshot TEXT, after_snapshot TEXT, reversible BOOLEAN, reversal_id INTEGER)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS simulations (sim_id TEXT PRIMARY KEY, user_id TEXT, params_json TEXT, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS reports (report_id TEXT PRIMARY KEY, type TEXT, path TEXT, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS assets (asset_id TEXT PRIMARY KEY, path TEXT, type TEXT, created_at TIMESTAMP)\"\n",
        "        ]\n",
        "        for table_sql in tables:\n",
        "            self.cursor.execute(table_sql)\n",
        "        self.conn.commit()\n",
        "\n",
        "    def log_action(self, actor_id, actor_role, action, target_id, target_type, reason, before_snapshot, after_snapshot, reversible=False):\n",
        "        \"\"\"Log an admin action to audit_logs.\"\"\"\n",
        "        timestamp = datetime.now(UTC).isoformat()\n",
        "        snapshot_before = json.dumps(before_snapshot) if before_snapshot else \"\"\n",
        "        snapshot_after = json.dumps(after_snapshot) if after_snapshot else \"\"\n",
        "        self.cursor.execute(\n",
        "            \"INSERT INTO audit_logs (actor_id, actor_role, action, target_id, target_type, reason, timestamp, before_snapshot, after_snapshot, reversible) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "            (actor_id, actor_role, action, target_id, target_type, reason, timestamp, snapshot_before, snapshot_after, reversible)\n",
        "        )\n",
        "        self.conn.commit()\n",
        "        return self.cursor.lastrowid\n",
        "\n",
        "    def get_audit_trail(self, target_id=None, target_type=None):\n",
        "        \"\"\"Retrieve audit logs, optionally filtered.\"\"\"\n",
        "        query = \"SELECT * FROM audit_logs\"\n",
        "        params = []\n",
        "        if target_id and target_type:\n",
        "            query += \" WHERE target_id = ? AND target_type = ?\"\n",
        "            params = [target_id, target_type]\n",
        "        self.cursor.execute(query, params)\n",
        "        return self.cursor.fetchall()\n",
        "\n",
        "    def rollback_action(self, action_id):\n",
        "        \"\"\"Attempt to rollback an action if reversible.\"\"\"\n",
        "        self.cursor.execute(\"SELECT reversible, before_snapshot, target_id, target_type, action FROM audit_logs WHERE log_id = ?\", (action_id,))\n",
        "        result = self.cursor.fetchone()\n",
        "        if not result or not result[0]:\n",
        "            return False\n",
        "        before_snapshot = json.loads(result[1]) if result[1] else {}\n",
        "        target_id, target_type, action = result[2], result[3], result[4]\n",
        "        if target_type == \"user\" and action == \"edit\":\n",
        "            self.cursor.execute(\"UPDATE users SET username = ?, password = ?, role = ? WHERE user_id = ?\",\n",
        "                              (before_snapshot.get(\"username\"), before_snapshot.get(\"password\"), before_snapshot.get(\"role\"), target_id))\n",
        "            self.conn.commit()\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection.\"\"\"\n",
        "        self.conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    db = DB()\n",
        "    db.cursor.execute(\"INSERT OR IGNORE INTO users (user_id, username, password, role) VALUES (?, ?, ?, ?)\",\n",
        "                     (\"1\", \"admin\", \"Shady868\", \"admin\"))\n",
        "    db.conn.commit()\n",
        "    db.log_action(\"1\", \"admin\", \"init\", \"1\", \"user\", \"Initialize admin user\", {}, {\"username\": \"admin\"})\n",
        "    print(\"Database initialized.\")\n",
        "    db.close()\n",
        "''')\n",
        "!test -f modules/core/db.py && echo \"db.py created\" || echo \"Failed to create db.py\"\n",
        "\n",
        "# Write utils.py using Python\n",
        "with open('modules/core/utils.py', 'w') as f:\n",
        "    f.write('''# modules/core/utils.py\n",
        "# Estimated line count: 80\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "import json\n",
        "from functools import wraps\n",
        "try:\n",
        "    from modules.core import db, config\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "def audit_wrapper(func):\n",
        "    \"\"\"Decorator to log admin actions with snapshots and reason.\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, actor_id, actor_role, reason, **kwargs):\n",
        "        if not reason:\n",
        "            raise ValueError(\"Reason is required for audited actions\")\n",
        "        database = db.DB()\n",
        "        target_id = kwargs.get('target_id', args[0] if args else 'unknown')\n",
        "        target_type = kwargs.get('target_type', func.__name__)\n",
        "        before_snapshot = {}\n",
        "        try:\n",
        "            if target_type in ['user', 'edit_user', 'delete_user', 'add_user']:\n",
        "                database.cursor.execute(\"SELECT * FROM users WHERE user_id = ?\", (target_id,))\n",
        "                row = database.cursor.fetchone()\n",
        "                before_snapshot = dict(row) if row else {}\n",
        "                print(f\"Before snapshot: {before_snapshot}\")  # Debug\n",
        "            filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'target_type'}\n",
        "            result = func(*args, actor_id=actor_id, actor_role=actor_role, reason=reason, **filtered_kwargs)\n",
        "            after_snapshot = {}\n",
        "            if target_type in ['user', 'edit_user', 'delete_user', 'add_user']:\n",
        "                database.cursor.execute(\"SELECT * FROM users WHERE user_id = ?\", (target_id,))\n",
        "                row = database.cursor.fetchone()\n",
        "                after_snapshot = dict(row) if row else {}\n",
        "                print(f\"After snapshot: {after_snapshot}\")  # Debug\n",
        "            reversible = target_type in ['user', 'edit_user', 'add_user']\n",
        "            log_id = database.log_action(\n",
        "                actor_id, actor_role, func.__name__, target_id, target_type, reason,\n",
        "                before_snapshot, after_snapshot, reversible\n",
        "            )\n",
        "            database.close()\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            database.close()\n",
        "            raise Exception(f\"Action failed: {e}\")\n",
        "    return wrapper\n",
        "\n",
        "def dict_diff(before, after):\n",
        "    \"\"\"Compute difference between two dictionaries for audit logging.\"\"\"\n",
        "    diff = {}\n",
        "    for key in set(before.keys()) | set(after.keys()):\n",
        "        if before.get(key) != after.get(key):\n",
        "            diff[key] = {'before': before.get(key), 'after': after.get(key)}\n",
        "    return diff\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    @audit_wrapper\n",
        "    def test_action(target_id, actor_id, actor_role, reason):\n",
        "        return {\"result\": \"test\"}\n",
        "    result = test_action(\"test_id\", actor_id=\"1\", actor_role=\"admin\", reason=\"Test audit\")\n",
        "    print(f\"Test action result: {result}\")\n",
        "''')\n",
        "!test -f modules/core/utils.py && echo \"utils.py created\" || echo \"Failed to create utils.py\"\n",
        "\n",
        "# Write auth.py using Python\n",
        "with open('modules/core/auth.py', 'w') as f:\n",
        "    f.write('''# modules/core/auth.py\n",
        "# Estimated line count: 80\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "import sqlite3\n",
        "try:\n",
        "    from modules.core import config, db, utils\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "def authenticate(username, password):\n",
        "    \"\"\"Authenticate user against stored credentials.\"\"\"\n",
        "    cfg = config.get_config()\n",
        "    if username == config.ADMIN_CREDENTIALS[\"username\"] and password == config.ADMIN_CREDENTIALS[\"password\"]:\n",
        "        return {\"user_id\": \"1\", \"role\": \"admin\"}\n",
        "    database = db.DB()\n",
        "    database.cursor.execute(\"SELECT user_id, role FROM users WHERE username = ? AND password = ?\", (username, password))\n",
        "    user = database.cursor.fetchone()\n",
        "    database.close()\n",
        "    if user:\n",
        "        return {\"user_id\": user[0], \"role\": user[1]}\n",
        "    return None\n",
        "\n",
        "@utils.audit_wrapper\n",
        "def add_user(username, password, role, actor_id, actor_role, reason, target_id=None):\n",
        "    \"\"\"Add a new user with audit logging.\"\"\"\n",
        "    target_id = target_id or f\"u_{str(hash(username))[:8]}\"\n",
        "    print(f\"Adding user with target_id: {target_id}\")  # Debug\n",
        "    database = db.DB()\n",
        "    try:\n",
        "        database.cursor.execute(\"INSERT INTO users (user_id, username, password, role) VALUES (?, ?, ?, ?)\",\n",
        "                              (target_id, username, password, role))\n",
        "        database.conn.commit()\n",
        "        database.close()\n",
        "        return target_id\n",
        "    except sqlite3.IntegrityError as e:\n",
        "        database.close()\n",
        "        raise ValueError(f\"Failed to add user {username}: {e}\")\n",
        "\n",
        "@utils.audit_wrapper\n",
        "def edit_user(user_id, updates, actor_id, actor_role, reason, target_id=None):\n",
        "    \"\"\"Edit user details with audit logging.\"\"\"\n",
        "    target_id = target_id or user_id\n",
        "    database = db.DB()\n",
        "    allowed_fields = ['username', 'password', 'role']\n",
        "    updates = {k: v for k, v in updates.items() if k in allowed_fields}\n",
        "    if not updates:\n",
        "        database.close()\n",
        "        raise ValueError(\"No valid fields to update\")\n",
        "    set_clause = \", \".join(f\"{k} = ?\" for k in updates.keys())\n",
        "    values = list(updates.values()) + [user_id]\n",
        "    try:\n",
        "        database.cursor.execute(f\"UPDATE users SET {set_clause} WHERE user_id = ?\", values)\n",
        "        database.conn.commit()\n",
        "        database.close()\n",
        "        return True\n",
        "    except sqlite3.IntegrityError as e:\n",
        "        database.close()\n",
        "        raise ValueError(f\"Failed to edit user {user_id}: {e}\")\n",
        "\n",
        "@utils.audit_wrapper\n",
        "def delete_user(user_id, actor_id, actor_role, reason, target_id=None, confirmation=None):\n",
        "    \"\"\"Delete user with audit logging and confirmation.\"\"\"\n",
        "    target_id = target_id or user_id\n",
        "    if confirmation != f\"CONFIRM DELETE {user_id}\":\n",
        "        raise ValueError(\"Invalid confirmation for deletion\")\n",
        "    database = db.DB()\n",
        "    try:\n",
        "        database.cursor.execute(\"DELETE FROM users WHERE user_id = ?\", (user_id,))\n",
        "        database.conn.commit()\n",
        "        database.close()\n",
        "        return True\n",
        "    except sqlite3.Error as e:\n",
        "        database.close()\n",
        "        raise ValueError(f\"Failed to delete user {user_id}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user = authenticate(\"admin\", \"Shady868\")\n",
        "    print(f\"Auth result: {user}\")\n",
        "    try:\n",
        "        new_user_id = add_user(\"test_user\", \"test_pass\", \"user\", actor_id=\"1\", actor_role=\"admin\", reason=\"Test add user\", target_id=\"test_1\")\n",
        "        print(f\"Added user: {new_user_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding user: {e}\")\n",
        "''')\n",
        "!test -f modules/core/auth.py && echo \"auth.py created\" || echo \"Failed to create auth.py\"\n",
        "\n",
        "# Write test_core.py using Python\n",
        "os.makedirs('tests', exist_ok=True)\n",
        "with open('tests/test_core.py', 'w') as f:\n",
        "    f.write('''# tests/test_core.py\n",
        "# Estimated line count: 60\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import sqlite3\n",
        "sys.path.append(os.getcwd())\n",
        "try:\n",
        "    from modules.core import config, db, utils, auth\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "def test_config_init():\n",
        "    cfg = config.get_config()\n",
        "    assert os.path.exists(cfg[\"data_dir\"]), \"Data directory not created\"\n",
        "    assert cfg[\"streamlit_port\"] == 8501, \"Incorrect port\"\n",
        "    assert config.ADMIN_CREDENTIALS[\"username\"] == \"admin\", \"Admin username incorrect\"\n",
        "\n",
        "def test_db_create_and_log():\n",
        "    database = db.DB()\n",
        "    database.cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='audit_logs'\")\n",
        "    assert database.cursor.fetchone(), \"Audit logs table not created\"\n",
        "    log_id = database.log_action(\"1\", \"admin\", \"test_action\", \"test_id\", \"test_type\", \"Test reason\", {\"key\": \"before\"}, {\"key\": \"after\"}, True)\n",
        "    assert log_id, \"Failed to log action\"\n",
        "    audit_logs = database.get_audit_trail(\"test_id\", \"test_type\")\n",
        "    assert len(audit_logs) > 0, \"Audit log not recorded\"\n",
        "    database.close()\n",
        "\n",
        "def test_authenticate():\n",
        "    user = auth.authenticate(\"admin\", \"Shady868\")\n",
        "    assert user == {\"user_id\": \"1\", \"role\": \"admin\"}, \"Admin authentication failed\"\n",
        "    user = auth.authenticate(\"wrong\", \"wrong\")\n",
        "    assert user is None, \"Invalid credentials should fail\"\n",
        "\n",
        "def test_audit_wrapper():\n",
        "    @utils.audit_wrapper\n",
        "    def test_action(target_id, actor_id, actor_role, reason):\n",
        "        return True\n",
        "    result = test_action(\"test_id\", actor_id=\"1\", actor_role=\"admin\", reason=\"Test audit\")\n",
        "    assert result, \"Audit wrapper failed\"\n",
        "    database = db.DB()\n",
        "    logs = database.get_audit_trail(\"test_id\", \"test_action\")\n",
        "    assert len(logs) > 0, \"Audit log not recorded\"\n",
        "    database.close()\n",
        "\n",
        "def test_add_user():\n",
        "    database = db.DB()\n",
        "    database.cursor.execute(\"DELETE FROM users WHERE user_id = ?\", (\"test_2\",))\n",
        "    database.conn.commit()\n",
        "    database.close()\n",
        "    user_id = auth.add_user(\"test_user2\", \"test_pass2\", \"user\", actor_id=\"1\", actor_role=\"admin\", reason=\"Test add user\", target_id=\"test_2\")\n",
        "    assert user_id == \"test_2\", \"Failed to add user\"\n",
        "    database = db.DB()\n",
        "    database.cursor.execute(\"SELECT username FROM users WHERE user_id = ?\", (\"test_2\",))\n",
        "    result = database.cursor.fetchone()\n",
        "    assert result and result[0] == \"test_user2\", \"User not added correctly\"\n",
        "    database.close()\n",
        "\n",
        "def test_edit_user():\n",
        "    database = db.DB()\n",
        "    database.cursor.execute(\"DELETE FROM users WHERE user_id = ?\", (\"test_2\",))\n",
        "    database.cursor.execute(\"INSERT INTO users (user_id, username, password, role) VALUES (?, ?, ?, ?)\",\n",
        "                          (\"test_2\", \"test_user2\", \"test_pass2\", \"user\"))\n",
        "    database.conn.commit()\n",
        "    database.close()\n",
        "    result = auth.edit_user(\"test_2\", {\"password\": \"new_pass\"}, actor_id=\"1\", actor_role=\"admin\", reason=\"Test edit user\", target_id=\"test_2\")\n",
        "    assert result, \"Failed to edit user\"\n",
        "    database = db.DB()\n",
        "    database.cursor.execute(\"SELECT password FROM users WHERE user_id = ?\", (\"test_2\",))\n",
        "    result = database.cursor.fetchone()\n",
        "    assert result and result[0] == \"new_pass\", \"User not edited correctly\"\n",
        "    database.close()\n",
        "\n",
        "def test_db_existence():\n",
        "    assert os.path.exists(\"data/loan_iq.db\"), \"Database file not created\"\n",
        "''')\n",
        "!test -f tests/test_core.py && echo \"test_core.py created\" || echo \"Failed to create test_core.py\"\n",
        "\n",
        "# Ensure dependencies are installed\n",
        "!python modules/bootstrap/deps.py\n",
        "\n",
        "# Verify directories\n",
        "!ls modules/bootstrap || echo \"modules/bootstrap not found\"\n",
        "!ls modules/core || echo \"modules/core not found\"\n",
        "!ls data || echo \"data not found\"\n",
        "\n",
        "# Run config.py\n",
        "!python modules/core/config.py\n",
        "\n",
        "# Run db.py\n",
        "!python modules/core/db.py\n",
        "\n",
        "# Run utils.py\n",
        "!python modules/core/utils.py\n",
        "\n",
        "# Run auth.py\n",
        "!python modules/core/auth.py\n",
        "\n",
        "# Run tests\n",
        "!pytest tests/test_core.py -v\n",
        "\n",
        "# Verify files\n",
        "!ls modules/bootstrap\n",
        "!ls modules/core\n",
        "!ls data\n",
        "\n",
        "# Expected output:\n",
        "# Current working directory: /content\n",
        "# Directory modules/bootstrap created\n",
        "# Directory modules/core created\n",
        "# Directory data created\n",
        "# deps.py created\n",
        "# config.py created\n",
        "# db.py created\n",
        "# utils.py created\n",
        "# auth.py created\n",
        "# test_core.py created\n",
        "# Dependencies installed successfully.\n",
        "# deps.py\n",
        "# auth.py  config.py  db.py  utils.py\n",
        "# .deps_ok  loan_iq.db  reports\n",
        "# Config loaded: {'data_dir': 'data', 'model_dir': 'models', 'report_dir': 'data/reports', 'db_path': 'data/loan_iq.db', 'drive_root': '/content/drive/MyDrive/loan_iq', 'streamlit_port': 8501, 'fraud_types': ['ghost_client', 'duplicate_"
      ],
      "metadata": {
        "id": "jQ-DFEWxznU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d655bcf5-8de2-4ede-ff92-be41c01dd15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "deps.py\n",
            "auth.py  config.py  db.py  __pycache__\tutils.py\n",
            "reports\n",
            "deps.py created\n",
            "config.py created\n",
            "db.py created\n",
            "utils.py created\n",
            "auth.py created\n",
            "test_core.py created\n",
            "Requirement already satisfied: scikit-learn==1.5.1 in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (3.6.0)\n",
            "Dependencies installed successfully.\n",
            "deps.py\n",
            "auth.py  config.py  db.py  __pycache__\tutils.py\n",
            "reports\n",
            "Config loaded: {'data_dir': 'data', 'model_dir': 'models', 'report_dir': 'data/reports', 'db_path': 'data/loan_iq.db', 'drive_root': '/content/drive/MyDrive/loan_iq', 'streamlit_port': 8501, 'fraud_types': ['ghost_client', 'duplicate_id', 'missed_payment', 'identity_theft'], 'regions': ['urban', 'rural', 'semi_urban'], 'max_clients_batch': 70000, 'default_batch_size': 1000}\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Database initialized.\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Test action result: {'result': 'test'}\n",
            "Auth result: {'user_id': '1', 'role': 'admin'}\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Before snapshot: {}\n",
            "Adding user with target_id: test_1\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "After snapshot: {'user_id': 'test_1', 'username': 'test_user', 'password': 'test_pass', 'role': 'user'}\n",
            "Added user: test_1\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: Faker-28.1.0, anyio-4.10.0, typeguard-4.4.4, langsmith-0.4.16\n",
            "collected 7 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_core.py::test_config_init \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 14%]\u001b[0m\n",
            "tests/test_core.py::test_db_create_and_log \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 28%]\u001b[0m\n",
            "tests/test_core.py::test_authenticate \u001b[32mPASSED\u001b[0m\u001b[32m                             [ 42%]\u001b[0m\n",
            "tests/test_core.py::test_audit_wrapper \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 57%]\u001b[0m\n",
            "tests/test_core.py::test_add_user \u001b[32mPASSED\u001b[0m\u001b[32m                                 [ 71%]\u001b[0m\n",
            "tests/test_core.py::test_edit_user \u001b[32mPASSED\u001b[0m\u001b[32m                                [ 85%]\u001b[0m\n",
            "tests/test_core.py::test_db_existence \u001b[32mPASSED\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.26s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
            "deps.py\n",
            "auth.py  config.py  db.py  __pycache__\tutils.py\n",
            "loan_iq.db  reports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell to create, run, and test modules/synth/faker_engine.py and modules/synth/generators.py\n",
        "# Run this entire block in Colab to execute all steps\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "print(f\"Current working directory: {os.getcwd()}\")  # Debug\n",
        "\n",
        "# Create directories and reset database to prevent schema conflicts\n",
        "!mkdir -p modules/synth tests data models data/reports\n",
        "!rm -f data/loan_iq.db\n",
        "!ls modules/synth || echo \"Directory modules/synth created\"\n",
        "!ls tests || echo \"Directory tests created\"\n",
        "!ls data || echo \"Directory data created\"\n",
        "\n",
        "# Write faker_engine.py using Python\n",
        "os.makedirs('modules/synth', exist_ok=True)\n",
        "with open('modules/synth/faker_engine.py', 'w') as f:\n",
        "    f.write('''# modules/synth/faker_engine.py\n",
        "# Estimated line count: 300\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "try:\n",
        "    from modules.core import config\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class LoanIQFaker:\n",
        "    \"\"\"Custom Faker for generating Loan IQ synthetic data with fraud patterns.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.faker = Faker()\n",
        "        Faker.seed(config.SEEDS[\"faker\"])\n",
        "        random.seed(config.SEEDS[\"random\"])\n",
        "        self.config = config.get_config()\n",
        "        self.fraud_types = self.config[\"fraud_types\"]\n",
        "        self.regions = self.config[\"regions\"]\n",
        "\n",
        "    def client_id(self):\n",
        "        \"\"\"Generate unique client ID.\"\"\"\n",
        "        return f\"C_{self.faker.uuid4().split('-')[0]}\"\n",
        "\n",
        "    def loan_id(self):\n",
        "        \"\"\"Generate unique loan ID.\"\"\"\n",
        "        return f\"L_{self.faker.uuid4().split('-')[0]}\"\n",
        "\n",
        "    def transaction_id(self):\n",
        "        \"\"\"Generate unique transaction ID.\"\"\"\n",
        "        return f\"T_{self.faker.uuid4().split('-')[0]}\"\n",
        "\n",
        "    def client_name(self, fraud_type=None):\n",
        "        \"\"\"Generate client name, with ghost client fraud option.\"\"\"\n",
        "        if fraud_type == \"ghost_client\" and random.random() < 0.1:\n",
        "            return None  # Ghost client has no name\n",
        "        return self.faker.name()\n",
        "\n",
        "    def duplicate_id(self, existing_ids):\n",
        "        \"\"\"Generate client ID with chance of duplication for fraud.\"\"\"\n",
        "        if random.random() < 0.05:  # 5% chance of duplicate ID\n",
        "            return random.choice(existing_ids) if existing_ids else self.client_id()\n",
        "        return self.client_id()\n",
        "\n",
        "    def income(self, fraud_type=None):\n",
        "        \"\"\"Generate income, with variance for fraud.\"\"\"\n",
        "        if fraud_type == \"identity_theft\" and random.random() < 0.1:\n",
        "            return random.uniform(100000, 1000000)  # Suspiciously high income\n",
        "        return random.uniform(20000, 100000)\n",
        "\n",
        "    def branch(self):\n",
        "        \"\"\"Generate branch name.\"\"\"\n",
        "        return self.faker.city()\n",
        "\n",
        "    def region(self):\n",
        "        \"\"\"Generate region from config.\"\"\"\n",
        "        return random.choice(self.regions)\n",
        "\n",
        "    def loan_amount(self, fraud_type=None):\n",
        "        \"\"\"Generate loan amount, with variance for fraud.\"\"\"\n",
        "        if fraud_type == \"missed_payment\" and random.random() < 0.2:\n",
        "            return random.uniform(50000, 200000)  # Higher loan for missed payments\n",
        "        return random.uniform(1000, 50000)\n",
        "\n",
        "    def loan_status(self, fraud_type=None):\n",
        "        \"\"\"Generate loan status, with fraud influence.\"\"\"\n",
        "        statuses = [\"active\", \"paid\", \"default\"]\n",
        "        if fraud_type == \"missed_payment\" and random.random() < 0.3:\n",
        "            return \"default\"\n",
        "        return random.choice(statuses)\n",
        "\n",
        "    def transaction_amount(self, loan_amount):\n",
        "        \"\"\"Generate transaction amount based on loan.\"\"\"\n",
        "        return random.uniform(100, min(loan_amount * 0.1, 5000))\n",
        "\n",
        "    def transaction_type(self, fraud_type=None):\n",
        "        \"\"\"Generate transaction type, with fraud influence.\"\"\"\n",
        "        types = [\"payment\", \"fee\", \"interest\"]\n",
        "        if fraud_type == \"identity_theft\" and random.random() < 0.1:\n",
        "            return \"suspicious_transfer\"\n",
        "        return random.choice(types)\n",
        "\n",
        "    def random_date(self, start_days=-365, end_days=0):\n",
        "        \"\"\"Generate random date within range.\"\"\"\n",
        "        start = datetime.now() + timedelta(days=start_days)\n",
        "        end = datetime.now() + timedelta(days=end_days)\n",
        "        return self.faker.date_time_between(start, end).isoformat()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    faker = LoanIQFaker()\n",
        "    print(f\"Client ID: {faker.client_id()}\")\n",
        "    print(f\"Client Name: {faker.client_name()}\")\n",
        "    print(f\"Loan ID: {faker.loan_id()}\")\n",
        "    print(f\"Transaction ID: {faker.transaction_id()}\")\n",
        "    print(f\"Income: {faker.income()}\")\n",
        "    print(f\"Branch: {faker.branch()}\")\n",
        "    print(f\"Region: {faker.region()}\")\n",
        "    print(f\"Loan Amount: {faker.loan_amount()}\")\n",
        "    print(f\"Loan Status: {faker.loan_status()}\")\n",
        "    print(f\"Transaction Amount: {faker.transaction_amount(10000)}\")\n",
        "    print(f\"Transaction Type: {faker.transaction_type()}\")\n",
        "    print(f\"Random Date: {faker.random_date()}\")\n",
        "''')\n",
        "!test -f modules/synth/faker_engine.py && echo \"faker_engine.py created\" || echo \"Failed to create faker_engine.py\"\n",
        "\n",
        "# Write generators.py using Python\n",
        "with open('modules/synth/generators.py', 'w') as f:\n",
        "    f.write('''# modules/synth/generators.py\n",
        "# Estimated line count: 250\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "sys.path.append(os.getcwd())\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "try:\n",
        "    from modules.core import db, config\n",
        "    from modules.synth import faker_engine\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class DataGenerator:\n",
        "    \"\"\"Generate synthetic data for Loan IQ and store in database.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.faker = faker_engine.LoanIQFaker()\n",
        "        self.config = config.get_config()\n",
        "        self.db_path = self.config[\"db_path\"]\n",
        "\n",
        "    def generate_clients(self, n, fraud_ratio=0.1):\n",
        "        \"\"\"Generate n clients with optional fraud patterns.\"\"\"\n",
        "        clients = []\n",
        "        existing_ids = []\n",
        "        for _ in range(n):\n",
        "            fraud_type = random.choices(\n",
        "                self.faker.fraud_types + [None],\n",
        "                weights=[fraud_ratio / len(self.faker.fraud_types)] * len(self.faker.fraud_types) + [1 - fraud_ratio],\n",
        "                k=1\n",
        "            )[0]\n",
        "            client_id = self.faker.duplicate_id(existing_ids) if fraud_type == \"duplicate_id\" else self.faker.client_id()\n",
        "            existing_ids.append(client_id)\n",
        "            clients.append({\n",
        "                \"client_id\": client_id,\n",
        "                \"name\": self.faker.client_name(fraud_type),\n",
        "                \"branch\": self.faker.branch(),\n",
        "                \"region\": self.faker.region(),\n",
        "                \"income\": self.faker.income(fraud_type),\n",
        "                \"created_at\": self.faker.random_date()\n",
        "            })\n",
        "        return pd.DataFrame(clients)\n",
        "\n",
        "    def generate_loans(self, clients, n_per_client=2, fraud_ratio=0.1):\n",
        "        \"\"\"Generate loans for given clients.\"\"\"\n",
        "        loans = []\n",
        "        for client_id in clients[\"client_id\"]:\n",
        "            fraud_type = random.choices(\n",
        "                self.faker.fraud_types + [None],\n",
        "                weights=[fraud_ratio / len(self.faker.fraud_types)] * len(self.faker.fraud_types) + [1 - fraud_ratio],\n",
        "                k=1\n",
        "            )[0]\n",
        "            for _ in range(random.randint(1, n_per_client)):\n",
        "                loans.append({\n",
        "                    \"loan_id\": self.faker.loan_id(),\n",
        "                    \"client_id\": client_id,\n",
        "                    \"amount\": self.faker.loan_amount(fraud_type),\n",
        "                    \"status\": self.faker.loan_status(fraud_type),\n",
        "                    \"start_date\": self.faker.random_date()\n",
        "                })\n",
        "        return pd.DataFrame(loans)\n",
        "\n",
        "    def generate_transactions(self, loans, n_per_loan=3, fraud_ratio=0.1):\n",
        "        \"\"\"Generate transactions for given loans.\"\"\"\n",
        "        transactions = []\n",
        "        for loan_id, loan_amount in zip(loans[\"loan_id\"], loans[\"amount\"]):\n",
        "            fraud_type = random.choices(\n",
        "                self.faker.fraud_types + [None],\n",
        "                weights=[fraud_ratio / len(self.faker.fraud_types)] * len(self.faker.fraud_types) + [1 - fraud_ratio],\n",
        "                k=1\n",
        "            )[0]\n",
        "            for _ in range(random.randint(1, n_per_loan)):\n",
        "                transactions.append({\n",
        "                    \"transaction_id\": self.faker.transaction_id(),\n",
        "                    \"loan_id\": loan_id,\n",
        "                    \"amount\": self.faker.transaction_amount(loan_amount),\n",
        "                    \"date\": self.faker.random_date(),\n",
        "                    \"type\": self.faker.transaction_type(fraud_type)\n",
        "                })\n",
        "        return pd.DataFrame(transactions)\n",
        "\n",
        "    def save_to_db(self, clients, loans, transactions, actor_id=\"1\", actor_role=\"admin\", reason=\"Synthetic data generation\"):\n",
        "        \"\"\"Save generated data to loan_iq.db with audit logging.\"\"\"\n",
        "        database = db.DB()\n",
        "        print(f\"Saving to database: {self.db_path}\")  # Debug\n",
        "        try:\n",
        "            # Save clients\n",
        "            for _, row in clients.iterrows():\n",
        "                database.cursor.execute(\n",
        "                    \"INSERT OR IGNORE INTO clients (client_id, name, branch, region, income, created_at) VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "                    (row[\"client_id\"], row[\"name\"], row[\"branch\"], row[\"region\"], row[\"income\"], row[\"created_at\"])\n",
        "                )\n",
        "            # Save loans\n",
        "            for _, row in loans.iterrows():\n",
        "                database.cursor.execute(\n",
        "                    \"INSERT OR IGNORE INTO loans (loan_id, client_id, amount, status, start_date) VALUES (?, ?, ?, ?, ?)\",\n",
        "                    (row[\"loan_id\"], row[\"client_id\"], row[\"amount\"], row[\"status\"], row[\"start_date\"])\n",
        "                )\n",
        "            # Save transactions\n",
        "            for _, row in transactions.iterrows():\n",
        "                database.cursor.execute(\n",
        "                    \"INSERT OR IGNORE INTO transactions (transaction_id, loan_id, amount, date, type) VALUES (?, ?, ?, ?, ?)\",\n",
        "                    (row[\"transaction_id\"], row[\"loan_id\"], row[\"amount\"], row[\"date\"], row[\"type\"])\n",
        "                )\n",
        "            database.conn.commit()\n",
        "            database.log_action(\n",
        "                actor_id, actor_role, \"generate_data\", \"multiple\", \"synthetic_data\", reason,\n",
        "                {}, {\"clients\": len(clients), \"loans\": len(loans), \"transactions\": len(transactions)}\n",
        "            )\n",
        "            print(f\"Saved {len(clients)} clients, {len(loans)} loans, {len(transactions)} transactions to DB\")\n",
        "        finally:\n",
        "            database.close()\n",
        "\n",
        "    def export_to_csv(self, clients, loans, transactions, output_dir=None):\n",
        "        \"\"\"Export data to CSV files.\"\"\"\n",
        "        output_dir = output_dir or self.config[\"data_dir\"]\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        clients.to_csv(os.path.join(output_dir, \"clients.csv\"), index=False)\n",
        "        loans.to_csv(os.path.join(output_dir, \"loans.csv\"), index=False)\n",
        "        transactions.to_csv(os.path.join(output_dir, \"transactions.csv\"), index=False)\n",
        "        print(f\"Exported data to {output_dir}/[clients,loans,transactions].csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generator = DataGenerator()\n",
        "    clients = generator.generate_clients(10, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.save_to_db(clients, loans, transactions)\n",
        "    generator.export_to_csv(clients, loans, transactions)\n",
        "    print(\"Generated and saved synthetic data.\")\n",
        "''')\n",
        "!test -f modules/synth/generators.py && echo \"generators.py created\" || echo \"Failed to create generators.py\"\n",
        "\n",
        "# Write test_synth.py using Python\n",
        "with open('tests/test_synth.py', 'w') as f:\n",
        "    f.write('''# tests/test_synth.py\n",
        "# Estimated line count: 80\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "sys.path.append(os.getcwd())\n",
        "try:\n",
        "    from modules.core import config, db\n",
        "    from modules.synth import faker_engine, generators\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "def test_faker_engine():\n",
        "    faker = faker_engine.LoanIQFaker()\n",
        "    assert len(faker.client_id()) > 0, \"Client ID not generated\"\n",
        "    assert faker.region() in config.get_config()[\"regions\"], \"Invalid region\"\n",
        "    assert isinstance(faker.income(), float), \"Income not float\"\n",
        "    assert isinstance(faker.loan_amount(), float), \"Loan amount not float\"\n",
        "    assert faker.loan_status() in [\"active\", \"paid\", \"default\"], \"Invalid loan status\"\n",
        "\n",
        "def test_generate_clients():\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(5, fraud_ratio=0.2)\n",
        "    assert len(clients) == 5, \"Incorrect number of clients\"\n",
        "    assert set(clients.columns) == {\"client_id\", \"name\", \"branch\", \"region\", \"income\", \"created_at\"}, \"Incorrect client columns\"\n",
        "    assert clients[\"region\"].isin(config.get_config()[\"regions\"]).all(), \"Invalid regions\"\n",
        "\n",
        "def test_generate_loans():\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(3, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    assert len(loans) >= 3, \"Incorrect number of loans\"\n",
        "    assert set(loans.columns) == {\"loan_id\", \"client_id\", \"amount\", \"status\", \"start_date\"}, \"Incorrect loan columns\"\n",
        "    assert loans[\"client_id\"].isin(clients[\"client_id\"]).all(), \"Invalid client IDs in loans\"\n",
        "\n",
        "def test_generate_transactions():\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(2, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    assert len(transactions) >= 2, \"Incorrect number of transactions\"\n",
        "    assert set(transactions.columns) == {\"transaction_id\", \"loan_id\", \"amount\", \"date\", \"type\"}, \"Incorrect transaction columns\"\n",
        "    assert transactions[\"loan_id\"].isin(loans[\"loan_id\"]).all(), \"Invalid loan IDs in transactions\"\n",
        "\n",
        "def test_save_to_db():\n",
        "    generator = generators.DataGenerator()\n",
        "    database = db.DB()\n",
        "    print(f\"Clearing tables for test\")  # Debug\n",
        "    database.cursor.execute(\"DELETE FROM clients\")\n",
        "    database.cursor.execute(\"DELETE FROM loans\")\n",
        "    database.cursor.execute(\"DELETE FROM transactions\")\n",
        "    database.conn.commit()\n",
        "    database.close()\n",
        "    clients = generator.generate_clients(5, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.save_to_db(clients, loans, transactions)\n",
        "    database = db.DB()\n",
        "    database.cursor.execute(\"SELECT COUNT(*) FROM clients\")\n",
        "    assert database.cursor.fetchone()[0] == 5, \"Clients not saved to DB\"\n",
        "    database.cursor.execute(\"SELECT COUNT(*) FROM loans\")\n",
        "    assert database.cursor.fetchone()[0] >= 5, \"Loans not saved to DB\"\n",
        "    database.cursor.execute(\"SELECT COUNT(*) FROM transactions\")\n",
        "    assert database.cursor.fetchone()[0] >= 5, \"Transactions not saved to DB\"\n",
        "    database.cursor.execute(\"SELECT * FROM audit_logs WHERE target_type = 'synthetic_data'\")\n",
        "    assert len(database.cursor.fetchall()) > 0, \"Audit log not recorded\"\n",
        "    database.close()\n",
        "\n",
        "def test_export_to_csv():\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(5, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.export_to_csv(clients, loans, transactions)\n",
        "    assert os.path.exists(os.path.join(config.get_config()[\"data_dir\"], \"clients.csv\")), \"Clients CSV not exported\"\n",
        "    assert os.path.exists(os.path.join(config.get_config()[\"data_dir\"], \"loans.csv\")), \"Loans CSV not exported\"\n",
        "    assert os.path.exists(os.path.join(config.get_config()[\"data_dir\"], \"transactions.csv\")), \"Transactions CSV not exported\"\n",
        "''')\n",
        "!test -f tests/test_synth.py && echo \"test_synth.py created\" || echo \"Failed to create test_synth.py\"\n",
        "\n",
        "# Ensure dependencies are installed (assuming deps.py exists from previous cell)\n",
        "!python modules/bootstrap/deps.py\n",
        "\n",
        "# Verify directories\n",
        "!ls modules/synth || echo \"modules/synth not found\"\n",
        "!ls tests || echo \"tests not found\"\n",
        "!ls data || echo \"data not found\"\n",
        "\n",
        "# Run faker_engine.py\n",
        "!python modules/synth/faker_engine.py\n",
        "\n",
        "# Run generators.py\n",
        "!python modules/synth/generators.py\n",
        "\n",
        "# Run tests\n",
        "!pytest tests/test_synth.py -v\n",
        "\n",
        "# Verify files\n",
        "!ls modules/synth\n",
        "!ls tests\n",
        "!ls data\n",
        "\n",
        "# Expected output:\n",
        "# Current working directory: /content\n",
        "# Directory modules/synth created\n",
        "# Directory tests created\n",
        "# Directory data created\n",
        "# faker_engine.py created\n",
        "# generators.py created\n",
        "# test_synth.py created\n",
        "# Dependencies installed successfully.\n",
        "# faker_engine.py  generators.py\n",
        "# test_core.py  test_synth.py\n",
        "# .deps_ok  clients.csv  loans.csv  loan_iq.db  reports  transactions.csv\n",
        "# Client ID: C_...\n",
        "# Client Name: ...\n",
        "# Loan ID: L_...\n",
        "# Transaction ID: T_...\n",
        "# Income: ...\n",
        "# Branch: ...\n",
        "# Region: ...\n",
        "# Loan Amount: ...\n",
        "# Loan Status: ...\n",
        "# Transaction Amount: ...\n",
        "# Transaction Type: ...\n",
        "# Random Date: ...\n",
        "# Saving to database: data/loan_iq.db\n",
        "# Saved 10 clients, ... loans, ... transactions to DB\n",
        "# Exported data to data/[clients,loans,transactions].csv\n",
        "# Generated and saved synthetic data.\n",
        "# ============================= test session starts =============================\n",
        "# tests/test_synth.py::test_faker_engine PASSED\n",
        "# tests/test_synth.py::test_generate_clients PASSED\n",
        "# tests/test_synth.py::test_generate_loans PASSED\n",
        "# tests/test_synth.py::test_generate_transactions PASSED\n",
        "# tests/test_synth.py::test_save_to_db PASSED\n",
        "# tests/test_synth.py::test_export_to_csv PASSED\n",
        "# =========================== 6 passed in 0.XXs ==========================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8AdXBGc1g8U",
        "outputId": "dcadeb69-66bb-48d5-b5b8-bb105b8f3089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "faker_engine.py  generators.py\t__pycache__\n",
            "__pycache__  test_core.py  test_synth.py\n",
            "reports\n",
            "faker_engine.py created\n",
            "generators.py created\n",
            "test_synth.py created\n",
            "Dependencies already installed.\n",
            "faker_engine.py  generators.py\t__pycache__\n",
            "__pycache__  test_core.py  test_synth.py\n",
            "reports\n",
            "Client ID: C_bdd640fb\n",
            "Client Name: Daniel Doyle\n",
            "Loan ID: L_8b9d2434\n",
            "Transaction ID: T_0822e8f3\n",
            "Income: 71154.1438766307\n",
            "Branch: North Jefferyhaven\n",
            "Region: urban\n",
            "Loan Amount: 37335.97448823181\n",
            "Loan Status: active\n",
            "Transaction Amount: 300.88966433394046\n",
            "Transaction Type: interest\n",
            "Random Date: 2025-04-07T05:38:09.639320\n",
            "sys.path: ['/content/modules/synth', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Saving to database: data/loan_iq.db\n",
            "Saved 10 clients, 15 loans, 30 transactions to DB\n",
            "Exported data to data/[clients,loans,transactions].csv\n",
            "Generated and saved synthetic data.\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: Faker-28.1.0, anyio-4.10.0, typeguard-4.4.4, langsmith-0.4.16\n",
            "collected 6 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_synth.py::test_faker_engine \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 16%]\u001b[0m\n",
            "tests/test_synth.py::test_generate_clients \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 33%]\u001b[0m\n",
            "tests/test_synth.py::test_generate_loans \u001b[32mPASSED\u001b[0m\u001b[32m                          [ 50%]\u001b[0m\n",
            "tests/test_synth.py::test_generate_transactions \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 66%]\u001b[0m\n",
            "tests/test_synth.py::test_save_to_db \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 83%]\u001b[0m\n",
            "tests/test_synth.py::test_export_to_csv \u001b[32mPASSED\u001b[0m\u001b[32m                           [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 1.08s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
            "faker_engine.py  generators.py\t__pycache__\n",
            "__pycache__  test_core.py  test_synth.py\n",
            "clients.csv  loan_iq.db  loans.csv  reports  transactions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.makedirs('modules/bootstrap', exist_ok=True)\n",
        "with open('modules/bootstrap/deps.py', 'w') as f:\n",
        "    f.write('''# modules/bootstrap/deps.py\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "REQUIRED_LIBS = [\n",
        "    'streamlit==1.38.0',\n",
        "    'pandas==2.2.2',\n",
        "    'numpy==1.26.4',\n",
        "    'scikit-learn==1.5.1',\n",
        "    'xgboost==2.1.1',\n",
        "    'plotly==5.22.0',\n",
        "    'faker==28.1.0',\n",
        "    'openpyxl==3.1.5',\n",
        "    'reportlab==4.2.2',\n",
        "    'pytest==8.3.2',\n",
        "    'shap==0.46.0'\n",
        "]\n",
        "\n",
        "def install_deps():\n",
        "    \"\"\"Install required libraries and create marker file.\"\"\"\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    marker_path = os.path.join('data', '.deps_ok')\n",
        "    if not os.path.exists(marker_path):\n",
        "        for lib in REQUIRED_LIBS:\n",
        "            try:\n",
        "                __import__(lib.split('==')[0])\n",
        "            except ImportError:\n",
        "                subprocess.check_call(['pip', 'install', lib])\n",
        "        with open(marker_path, 'w') as f:\n",
        "            f.write('OK')\n",
        "        print(\"Dependencies installed successfully.\")\n",
        "    else:\n",
        "        print(\"Dependencies already installed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    install_deps()\n",
        "''')\n",
        "!test -f modules/bootstrap/deps.py && echo \"deps.py created\" || echo \"Failed to create deps.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2V4dfVbVx4a",
        "outputId": "96381521-4fee-40e7-8204-71446de4e647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deps.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('modules/core', exist_ok=True)\n",
        "with open('modules/core/config.py', 'w') as f:\n",
        "    f.write('''# modules/core/config.py\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "ADMIN_CREDENTIALS = {\n",
        "    \"username\": \"admin\",\n",
        "    \"password\": \"Shady868\"\n",
        "}\n",
        "\n",
        "SEEDS = {\n",
        "    \"faker\": 42,\n",
        "    \"numpy\": 42,\n",
        "    \"random\": 42\n",
        "}\n",
        "\n",
        "CONFIG = {\n",
        "    \"data_dir\": os.path.join(\"data\"),\n",
        "    \"model_dir\": os.path.join(\"models\"),\n",
        "    \"report_dir\": os.path.join(\"data\", \"reports\"),\n",
        "    \"db_path\": os.path.join(\"data\", \"loan_iq.db\"),\n",
        "    \"drive_root\": \"/content/drive/MyDrive/loan_iq\",\n",
        "    \"streamlit_port\": 8501,\n",
        "    \"fraud_types\": [\"ghost_client\", \"duplicate_id\", \"missed_payment\", \"identity_theft\"],\n",
        "    \"regions\": [\"urban\", \"rural\", \"semi_urban\"],\n",
        "    \"max_clients_batch\": 70000,\n",
        "    \"default_batch_size\": 1000\n",
        "}\n",
        "\n",
        "def init_seeds():\n",
        "    \"\"\"Initialize random seeds for reproducibility.\"\"\"\n",
        "    random.seed(SEEDS[\"random\"])\n",
        "    np.random.seed(SEEDS[\"numpy\"])\n",
        "\n",
        "def get_config():\n",
        "    \"\"\"Return config dictionary, ensure directories exist.\"\"\"\n",
        "    os.makedirs(CONFIG[\"data_dir\"], exist_ok=True)\n",
        "    os.makedirs(CONFIG[\"model_dir\"], exist_ok=True)\n",
        "    os.makedirs(CONFIG[\"report_dir\"], exist_ok=True)\n",
        "    return CONFIG\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    init_seeds()\n",
        "    config = get_config()\n",
        "    print(f\"Config loaded: {config}\")\n",
        "''')\n",
        "!test -f modules/core/config.py && echo \"config.py created\" || echo \"Failed to create config.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHryHIa6V4BL",
        "outputId": "0b7e6917-d05b-4126-9fd4-b8739c32d448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('modules/core', exist_ok=True)\n",
        "with open('modules/core/db.py', 'w') as f:\n",
        "    f.write('''# modules/core/db.py\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "import sqlite3\n",
        "import json\n",
        "from datetime import datetime, UTC\n",
        "try:\n",
        "    from modules.core import config\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class DB:\n",
        "    \"\"\"SQLite database wrapper for Loan IQ.\"\"\"\n",
        "    def __init__(self):\n",
        "        print(f\"sys.path: {sys.path}\")  # Debug\n",
        "        self.db_path = config.get_config()[\"db_path\"]\n",
        "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
        "        print(f\"Creating database at: {self.db_path}\")  # Debug\n",
        "        self.conn = sqlite3.connect(self.db_path)\n",
        "        self.conn.row_factory = sqlite3.Row\n",
        "        self.cursor = self.conn.cursor()\n",
        "        self.create_tables()\n",
        "        print(f\"Database created: {os.path.exists(self.db_path)}\")  # Debug\n",
        "\n",
        "    def create_tables(self):\n",
        "        \"\"\"Create database tables.\"\"\"\n",
        "        tables = [\n",
        "            \"CREATE TABLE IF NOT EXISTS users (user_id TEXT PRIMARY KEY, username TEXT UNIQUE, password TEXT, role TEXT)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS clients (client_id TEXT PRIMARY KEY, name TEXT, branch TEXT, region TEXT, income REAL, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS loans (loan_id TEXT PRIMARY KEY, client_id TEXT, amount REAL, status TEXT, start_date TIMESTAMP, FOREIGN KEY (client_id) REFERENCES clients(client_id))\",\n",
        "            \"CREATE TABLE IF NOT EXISTS transactions (transaction_id TEXT PRIMARY KEY, loan_id TEXT, amount REAL, date TIMESTAMP, type TEXT, FOREIGN KEY (loan_id) REFERENCES loans(loan_id))\",\n",
        "            \"CREATE TABLE IF NOT EXISTS models (model_id TEXT PRIMARY KEY, type TEXT, version TEXT, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS model_versions (version_id TEXT PRIMARY KEY, model_id TEXT, config_json TEXT, data_hash TEXT, metrics_json TEXT, commit_ref TEXT, comments TEXT, created_at TIMESTAMP, FOREIGN KEY (model_id) REFERENCES models(model_id))\",\n",
        "            \"CREATE TABLE IF NOT EXISTS audit_logs (log_id INTEGER PRIMARY KEY AUTOINCREMENT, actor_id TEXT, actor_role TEXT, action TEXT, target_id TEXT, target_type TEXT, reason TEXT, timestamp TIMESTAMP, before_snapshot TEXT, after_snapshot TEXT, reversible BOOLEAN, reversal_id INTEGER)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS simulations (sim_id TEXT PRIMARY KEY, user_id TEXT, params_json TEXT, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS reports (report_id TEXT PRIMARY KEY, type TEXT, path TEXT, created_at TIMESTAMP)\",\n",
        "            \"CREATE TABLE IF NOT EXISTS assets (asset_id TEXT PRIMARY KEY, path TEXT, type TEXT, created_at TIMESTAMP)\"\n",
        "        ]\n",
        "        for table_sql in tables:\n",
        "            self.cursor.execute(table_sql)\n",
        "        self.conn.commit()\n",
        "\n",
        "    def log_action(self, actor_id, actor_role, action, target_id, target_type, reason, before_snapshot, after_snapshot, reversible=False):\n",
        "        \"\"\"Log an admin action to audit_logs.\"\"\"\n",
        "        timestamp = datetime.now(UTC).isoformat()\n",
        "        snapshot_before = json.dumps(before_snapshot) if before_snapshot else \"\"\n",
        "        snapshot_after = json.dumps(after_snapshot) if after_snapshot else \"\"\n",
        "        self.cursor.execute(\n",
        "            \"INSERT INTO audit_logs (actor_id, actor_role, action, target_id, target_type, reason, timestamp, before_snapshot, after_snapshot, reversible) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "            (actor_id, actor_role, action, target_id, target_type, reason, timestamp, snapshot_before, snapshot_after, reversible)\n",
        "        )\n",
        "        self.conn.commit()\n",
        "        return self.cursor.lastrowid\n",
        "\n",
        "    def get_audit_trail(self, target_id=None, target_type=None):\n",
        "        \"\"\"Retrieve audit logs, optionally filtered.\"\"\"\n",
        "        query = \"SELECT * FROM audit_logs\"\n",
        "        params = []\n",
        "        if target_id and target_type:\n",
        "            query += \" WHERE target_id = ? AND target_type = ?\"\n",
        "            params = [target_id, target_type]\n",
        "        self.cursor.execute(query, params)\n",
        "        return self.cursor.fetchall()\n",
        "\n",
        "    def rollback_action(self, action_id):\n",
        "        \"\"\"Attempt to rollback an action if reversible.\"\"\"\n",
        "        self.cursor.execute(\"SELECT reversible, before_snapshot, target_id, target_type, action FROM audit_logs WHERE log_id = ?\", (action_id,))\n",
        "        result = self.cursor.fetchone()\n",
        "        if not result or not result[0]:\n",
        "            return False\n",
        "        before_snapshot = json.loads(result[1]) if result[1] else {}\n",
        "        target_id, target_type, action = result[2], result[3], result[4]\n",
        "        if target_type == \"user\" and action == \"edit\":\n",
        "            self.cursor.execute(\"UPDATE users SET username = ?, password = ?, role = ? WHERE user_id = ?\",\n",
        "                              (before_snapshot.get(\"username\"), before_snapshot.get(\"password\"), before_snapshot.get(\"role\"), target_id))\n",
        "            self.conn.commit()\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection.\"\"\"\n",
        "        self.conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    db = DB()\n",
        "    db.cursor.execute(\"INSERT OR IGNORE INTO users (user_id, username, password, role) VALUES (?, ?, ?, ?)\",\n",
        "                     (\"1\", \"admin\", \"Shady868\", \"admin\"))\n",
        "    db.conn.commit()\n",
        "    db.log_action(\"1\", \"admin\", \"init\", \"1\", \"user\", \"Initialize admin user\", {}, {\"username\": \"admin\"})\n",
        "    print(\"Database initialized.\")\n",
        "    db.close()\n",
        "''')\n",
        "!test -f modules/core/db.py && echo \"db.py created\" || echo \"Failed to create db.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBz7okUVV9Ay",
        "outputId": "7b501209-c56e-4116-de90-9f75e6bfc4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "db.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('modules/core', exist_ok=True)\n",
        "with open('modules/core/utils.py', 'w') as f:\n",
        "    f.write('''# modules/core/utils.py\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "import json\n",
        "from functools import wraps\n",
        "try:\n",
        "    from modules.core import db, config\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "def audit_wrapper(func):\n",
        "    \"\"\"Decorator to log admin actions with snapshots and reason.\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, actor_id, actor_role, reason, **kwargs):\n",
        "        if not reason:\n",
        "            raise ValueError(\"Reason is required for audited actions\")\n",
        "        database = db.DB()\n",
        "        target_id = kwargs.get('target_id', args[0] if args else 'unknown')\n",
        "        target_type = kwargs.get('target_type', func.__name__)\n",
        "        before_snapshot = {}\n",
        "        try:\n",
        "            if target_type in ['user', 'edit_user', 'delete_user', 'add_user']:\n",
        "                database.cursor.execute(\"SELECT * FROM users WHERE user_id = ?\", (target_id,))\n",
        "                row = database.cursor.fetchone()\n",
        "                before_snapshot = dict(row) if row else {}\n",
        "                print(f\"Before snapshot: {before_snapshot}\")  # Debug\n",
        "            filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'target_type'}\n",
        "            result = func(*args, actor_id=actor_id, actor_role=actor_role, reason=reason, **filtered_kwargs)\n",
        "            after_snapshot = {}\n",
        "            if target_type in ['user', 'edit_user', 'delete_user', 'add_user']:\n",
        "                database.cursor.execute(\"SELECT * FROM users WHERE user_id = ?\", (target_id,))\n",
        "                row = database.cursor.fetchone()\n",
        "                after_snapshot = dict(row) if row else {}\n",
        "                print(f\"After snapshot: {after_snapshot}\")  # Debug\n",
        "            reversible = target_type in ['user', 'edit_user', 'add_user']\n",
        "            log_id = database.log_action(\n",
        "                actor_id, actor_role, func.__name__, target_id, target_type, reason,\n",
        "                before_snapshot, after_snapshot, reversible\n",
        "            )\n",
        "            database.close()\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            database.close()\n",
        "            raise Exception(f\"Action failed: {e}\")\n",
        "    return wrapper\n",
        "\n",
        "def dict_diff(before, after):\n",
        "    \"\"\"Compute difference between two dictionaries for audit logging.\"\"\"\n",
        "    diff = {}\n",
        "    for key in set(before.keys()) | set(after.keys()):\n",
        "        if before.get(key) != after.get(key):\n",
        "            diff[key] = {'before': before.get(key), 'after': after.get(key)}\n",
        "    return diff\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    @audit_wrapper\n",
        "    def test_action(target_id, actor_id, actor_role, reason):\n",
        "        return {\"result\": \"test\"}\n",
        "    result = test_action(\"test_id\", actor_id=\"1\", actor_role=\"admin\", reason=\"Test audit\")\n",
        "    print(f\"Test action result: {result}\")\n",
        "''')\n",
        "!test -f modules/core/utils.py && echo \"utils.py created\" || echo \"Failed to create utils.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmjziNv9WCSb",
        "outputId": "bba17447-1f30-477d-a122-ec8b09c9d7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utils.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('modules/core', exist_ok=True)\n",
        "with open('modules/core/auth.py', 'w') as f:\n",
        "    f.write('''# modules/core/auth.py\n",
        "import sys\n",
        "import os\n",
        "import sqlite3\n",
        "import uuid\n",
        "from datetime import datetime, UTC\n",
        "sys.path.append(os.getcwd())\n",
        "try:\n",
        "    from modules.core import db, config\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class Auth:\n",
        "    def __init__(self):\n",
        "        self.config = config.get_config()\n",
        "        self.db_path = self.config[\"db_path\"]\n",
        "\n",
        "    def authenticate(self, username, password):\n",
        "        database = db.DB()\n",
        "        try:\n",
        "            database.cursor.execute(\n",
        "                \"SELECT user_id, role FROM users WHERE username = ? AND password = ?\",\n",
        "                (username, password)\n",
        "            )\n",
        "            user = database.cursor.fetchone()\n",
        "            if user:\n",
        "                return {\"user_id\": user[0], \"role\": user[1]}\n",
        "            return None\n",
        "        finally:\n",
        "            database.close()\n",
        "\n",
        "    def register(self, username, password):\n",
        "        \"\"\"Register a new user with default user role.\"\"\"\n",
        "        database = db.DB()\n",
        "        try:\n",
        "            user_id = f\"U_{uuid.uuid4().hex[:8]}\"\n",
        "            database.cursor.execute(\n",
        "                \"INSERT OR IGNORE INTO users (user_id, username, password, role, created_at) VALUES (?, ?, ?, ?, ?)\",\n",
        "                (user_id, username, password, \"user\", datetime.now(UTC).isoformat())\n",
        "            )\n",
        "            database.conn.commit()\n",
        "            database.log_action(\n",
        "                \"1\", \"admin\", \"register_user\", user_id, \"user\",\n",
        "                f\"Registered new user {username}\", {}, {}\n",
        "            )\n",
        "            print(f\"Registered user: {username} with role: user\")\n",
        "            return {\"user_id\": user_id, \"role\": \"user\"}\n",
        "        except sqlite3.IntegrityError:\n",
        "            print(f\"Registration failed: Username {username} already exists\")\n",
        "            return None\n",
        "        finally:\n",
        "            database.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    database = db.DB()\n",
        "    try:\n",
        "        # Hardcode admin user\n",
        "        database.cursor.execute(\n",
        "            \"INSERT OR REPLACE INTO users (user_id, username, password, role, created_at) VALUES (?, ?, ?, ?, ?)\",\n",
        "            (\"1\", \"admin\", \"Shady868\", \"admin\", datetime.now(UTC).isoformat())\n",
        "        )\n",
        "        # Add test user\n",
        "        database.cursor.execute(\n",
        "            \"INSERT OR REPLACE INTO users (user_id, username, password, role, created_at) VALUES (?, ?, ?, ?, ?)\",\n",
        "            (\"test_1\", \"test_user\", \"test_pass\", \"user\", datetime.now(UTC).isoformat())\n",
        "        )\n",
        "        database.conn.commit()\n",
        "        database.log_action(\n",
        "            \"1\", \"admin\", \"add_user\", \"test_1\", \"user\", \"Added test user\", {}, {}\n",
        "        )\n",
        "        print(\"Added user: test_1\")\n",
        "    finally:\n",
        "        database.close()\n",
        "''')\n",
        "!test -f modules/core/auth.py && echo \"auth.py created\" || echo \"Failed to create auth.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG07al2dWGlC",
        "outputId": "6b1b76c6-9c61-48e7-c60a-babb4b5cd00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auth.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('modules/synth', exist_ok=True)\n",
        "with open('modules/synth/faker_engine.py', 'w') as f:\n",
        "    f.write('''# modules/synth/faker_engine.py\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "try:\n",
        "    from modules.core import config\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class LoanIQFaker:\n",
        "    \"\"\"Custom Faker for generating Loan IQ synthetic data with patterns.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.faker = Faker()\n",
        "        Faker.seed(config.SEEDS[\"faker\"])\n",
        "        random.seed(config.SEEDS[\"random\"])\n",
        "        self.config = config.get_config()\n",
        "        self.fraud_types = self.config[\"fraud_types\"]\n",
        "        self.regions = self.config[\"regions\"]\n",
        "\n",
        "    def client_id(self):\n",
        "        \"\"\"Generate unique client ID.\"\"\"\n",
        "        return f\"C_{self.faker.uuid4().split('-')[0]}\"\n",
        "\n",
        "    def loan_id(self):\n",
        "        \"\"\"Generate unique loan ID.\"\"\"\n",
        "        return f\"L_{self.faker.uuid4().split('-')[0]}\"\n",
        "\n",
        "    def transaction_id(self):\n",
        "        \"\"\"Generate unique transaction ID.\"\"\"\n",
        "        return f\"T_{self.faker.uuid4().split('-')[0]}\"\n",
        "\n",
        "    def client_name(self, fraud_type=None):\n",
        "        \"\"\"Generate client name, with ghost client pattern.\"\"\"\n",
        "        if fraud_type == \"ghost_client\" and random.random() < 0.1:\n",
        "            return None\n",
        "        return self.faker.name()\n",
        "\n",
        "    def duplicate_id(self, existing_ids):\n",
        "        \"\"\"Generate client ID with chance of duplication.\"\"\"\n",
        "        if random.random() < 0.05:\n",
        "            return random.choice(existing_ids) if existing_ids else self.client_id()\n",
        "        return self.client_id()\n",
        "\n",
        "    def income(self, fraud_type=None):\n",
        "        \"\"\"Generate income, with variance for patterns.\"\"\"\n",
        "        if fraud_type == \"identity_theft\" and random.random() < 0.1:\n",
        "            return random.uniform(100000, 1000000)\n",
        "        return random.uniform(20000, 100000)\n",
        "\n",
        "    def branch(self):\n",
        "        \"\"\"Generate branch name.\"\"\"\n",
        "        return self.faker.city()\n",
        "\n",
        "    def region(self):\n",
        "        \"\"\"Generate region from config.\"\"\"\n",
        "        return random.choice(self.regions)\n",
        "\n",
        "    def loan_amount(self, fraud_type=None):\n",
        "        \"\"\"Generate loan amount, with variance for patterns.\"\"\"\n",
        "        if fraud_type == \"missed_payment\" and random.random() < 0.2:\n",
        "            return random.uniform(50000, 200000)\n",
        "        return random.uniform(1000, 50000)\n",
        "\n",
        "    def loan_status(self, fraud_type=None):\n",
        "        \"\"\"Generate loan status, with pattern influence.\"\"\"\n",
        "        statuses = [\"active\", \"paid\", \"default\"]\n",
        "        if fraud_type == \"missed_payment\" and random.random() < 0.3:\n",
        "            return \"default\"\n",
        "        return random.choice(statuses)\n",
        "\n",
        "    def transaction_amount(self, loan_amount):\n",
        "        \"\"\"Generate transaction amount based on loan.\"\"\"\n",
        "        return random.uniform(100, min(loan_amount * 0.1, 5000))\n",
        "\n",
        "    def transaction_type(self, fraud_type=None):\n",
        "        \"\"\"Generate transaction type, with pattern influence.\"\"\"\n",
        "        types = [\"payment\", \"fee\", \"interest\"]\n",
        "        if fraud_type == \"identity_theft\" and random.random() < 0.1:\n",
        "            return \"suspicious_transfer\"\n",
        "        return random.choice(types)\n",
        "\n",
        "    def random_date(self, start_days=-365, end_days=0):\n",
        "        \"\"\"Generate random date within range.\"\"\"\n",
        "        start = datetime.now() + timedelta(days=start_days)\n",
        "        end = datetime.now() + timedelta(days=end_days)\n",
        "        return self.faker.date_time_between(start, end).isoformat()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    faker = LoanIQFaker()\n",
        "    print(f\"Client ID: {faker.client_id()}\")\n",
        "    print(f\"Client Name: {faker.client_name()}\")\n",
        "    print(f\"Loan ID: {faker.loan_id()}\")\n",
        "    print(f\"Transaction ID: {faker.transaction_id()}\")\n",
        "    print(f\"Income: {faker.income()}\")\n",
        "    print(f\"Branch: {faker.branch()}\")\n",
        "    print(f\"Region: {faker.region()}\")\n",
        "    print(f\"Loan Amount: {faker.loan_amount()}\")\n",
        "    print(f\"Loan Status: {faker.loan_status()}\")\n",
        "    print(f\"Transaction Amount: {faker.transaction_amount(10000)}\")\n",
        "    print(f\"Transaction Type: {faker.transaction_type()}\")\n",
        "    print(f\"Random Date: {faker.random_date()}\")\n",
        "''')\n",
        "!test -f modules/synth/faker_engine.py && echo \"faker_engine.py created\" || echo \"Failed to create faker_engine.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYQVd11FWLyK",
        "outputId": "78b09e09-4c7a-4278-8ba5-cdeb3e5ab368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "faker_engine.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('modules/synth', exist_ok=True)\n",
        "with open('modules/synth/generators.py', 'w') as f:\n",
        "    f.write('''# modules/synth/generators.py\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "sys.path.append(os.getcwd())\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "try:\n",
        "    from modules.core import db, config\n",
        "    from modules.synth import faker_engine\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class DataGenerator:\n",
        "    \"\"\"Generate synthetic data for Loan IQ and store in database.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.faker = faker_engine.LoanIQFaker()\n",
        "        self.config = config.get_config()\n",
        "        self.db_path = self.config[\"db_path\"]\n",
        "\n",
        "    def generate_clients(self, n, fraud_ratio=0.1):\n",
        "        \"\"\"Generate n clients with optional patterns.\"\"\"\n",
        "        clients = []\n",
        "        existing_ids = []\n",
        "        for _ in range(n):\n",
        "            fraud_type = random.choices(\n",
        "                self.faker.fraud_types + [None],\n",
        "                weights=[fraud_ratio / len(self.faker.fraud_types)] * len(self.faker.fraud_types) + [1 - fraud_ratio],\n",
        "                k=1\n",
        "            )[0]\n",
        "            client_id = self.faker.duplicate_id(existing_ids) if fraud_type == \"duplicate_id\" else self.faker.client_id()\n",
        "            existing_ids.append(client_id)\n",
        "            clients.append({\n",
        "                \"client_id\": client_id,\n",
        "                \"name\": self.faker.client_name(fraud_type),\n",
        "                \"branch\": self.faker.branch(),\n",
        "                \"region\": self.faker.region(),\n",
        "                \"income\": self.faker.income(fraud_type),\n",
        "                \"created_at\": self.faker.random_date()\n",
        "            })\n",
        "        return pd.DataFrame(clients)\n",
        "\n",
        "    def generate_loans(self, clients, n_per_client=2, fraud_ratio=0.1):\n",
        "        \"\"\"Generate loans for given clients.\"\"\"\n",
        "        loans = []\n",
        "        for client_id in clients[\"client_id\"]:\n",
        "            fraud_type = random.choices(\n",
        "                self.faker.fraud_types + [None],\n",
        "                weights=[fraud_ratio / len(self.faker.fraud_types)] * len(self.faker.fraud_types) + [1 - fraud_ratio],\n",
        "                k=1\n",
        "            )[0]\n",
        "            for _ in range(random.randint(1, n_per_client)):\n",
        "                loans.append({\n",
        "                    \"loan_id\": self.faker.loan_id(),\n",
        "                    \"client_id\": client_id,\n",
        "                    \"amount\": self.faker.loan_amount(fraud_type),\n",
        "                    \"status\": self.faker.loan_status(fraud_type),\n",
        "                    \"start_date\": self.faker.random_date()\n",
        "                })\n",
        "        return pd.DataFrame(loans)\n",
        "\n",
        "    def generate_transactions(self, loans, n_per_loan=3, fraud_ratio=0.1):\n",
        "        \"\"\"Generate transactions for given loans.\"\"\"\n",
        "        transactions = []\n",
        "        for loan_id, loan_amount in zip(loans[\"loan_id\"], loans[\"amount\"]):\n",
        "            fraud_type = random.choices(\n",
        "                self.faker.fraud_types + [None],\n",
        "                weights=[fraud_ratio / len(self.faker.fraud_types)] * len(self.faker.fraud_types) + [1 - fraud_ratio],\n",
        "                k=1\n",
        "            )[0]\n",
        "            for _ in range(random.randint(1, n_per_loan)):\n",
        "                transactions.append({\n",
        "                    \"transaction_id\": self.faker.transaction_id(),\n",
        "                    \"loan_id\": loan_id,\n",
        "                    \"amount\": self.faker.transaction_amount(loan_amount),\n",
        "                    \"date\": self.faker.random_date(),\n",
        "                    \"type\": self.faker.transaction_type(fraud_type)\n",
        "                })\n",
        "        return pd.DataFrame(transactions)\n",
        "\n",
        "    def save_to_db(self, clients, loans, transactions, actor_id=\"1\", actor_role=\"admin\", reason=\"Synthetic data generation\"):\n",
        "        \"\"\"Save generated data to loan_iq.db with audit logging.\"\"\"\n",
        "        database = db.DB()\n",
        "        print(f\"Saving to database: {self.db_path}\")  # Debug\n",
        "        try:\n",
        "            for _, row in clients.iterrows():\n",
        "                database.cursor.execute(\n",
        "                    \"INSERT OR IGNORE INTO clients (client_id, name, branch, region, income, created_at) VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "                    (row[\"client_id\"], row[\"name\"], row[\"branch\"], row[\"region\"], row[\"income\"], row[\"created_at\"])\n",
        "                )\n",
        "            for _, row in loans.iterrows():\n",
        "                database.cursor.execute(\n",
        "                    \"INSERT OR IGNORE INTO loans (loan_id, client_id, amount, status, start_date) VALUES (?, ?, ?, ?, ?)\",\n",
        "                    (row[\"loan_id\"], row[\"client_id\"], row[\"amount\"], row[\"status\"], row[\"start_date\"])\n",
        "                )\n",
        "            for _, row in transactions.iterrows():\n",
        "                database.cursor.execute(\n",
        "                    \"INSERT OR IGNORE INTO transactions (transaction_id, loan_id, amount, date, type) VALUES (?, ?, ?, ?, ?)\",\n",
        "                    (row[\"transaction_id\"], row[\"loan_id\"], row[\"amount\"], row[\"date\"], row[\"type\"])\n",
        "                )\n",
        "            database.conn.commit()\n",
        "            database.log_action(\n",
        "                actor_id, actor_role, \"generate_data\", \"multiple\", \"synthetic_data\", reason,\n",
        "                {}, {\"clients\": len(clients), \"loans\": len(loans), \"transactions\": len(transactions)}\n",
        "            )\n",
        "            print(f\"Saved {len(clients)} clients, {len(loans)} loans, {len(transactions)} transactions to DB\")\n",
        "        finally:\n",
        "            database.close()\n",
        "\n",
        "    def export_to_csv(self, clients, loans, transactions, output_dir=None):\n",
        "        \"\"\"Export data to CSV files.\"\"\"\n",
        "        output_dir = output_dir or self.config[\"data_dir\"]\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        clients.to_csv(os.path.join(output_dir, \"clients.csv\"), index=False)\n",
        "        loans.to_csv(os.path.join(output_dir, \"loans.csv\"), index=False)\n",
        "        transactions.to_csv(os.path.join(output_dir, \"transactions.csv\"), index=False)\n",
        "        print(f\"Exported data to {output_dir}/[clients,loans,transactions].csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generator = DataGenerator()\n",
        "    clients = generator.generate_clients(10, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.save_to_db(clients, loans, transactions)\n",
        "    generator.export_to_csv(clients, loans, transactions)\n",
        "    print(\"Generated and saved synthetic data.\")\n",
        "''')\n",
        "!test -f modules/synth/generators.py && echo \"generators.py created\" || echo \"Failed to create generators.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBaEUs7SWTwU",
        "outputId": "4470f549-2ffc-4d37-a70d-d93cd6e15260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generators.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('modules/models', exist_ok=True)\n",
        "with open('modules/models/train.py', 'w') as f:\n",
        "    f.write('''# modules/models/train.py\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import random\n",
        "import uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, UTC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "sys.path.append(os.getcwd())\n",
        "try:\n",
        "    from modules.core import db, config\n",
        "    from modules.synth import generators\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Train XGBoost model for default probability and loan limits.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.config = config.get_config()\n",
        "        self.db_path = self.config[\"db_path\"]\n",
        "        self.model_dir = self.config[\"model_dir\"]\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "        random.seed(config.SEEDS[\"random\"])\n",
        "        np.random.seed(config.SEEDS[\"numpy\"])\n",
        "\n",
        "    def prepare_data(self, clients, loans, transactions):\n",
        "        \"\"\"Prepare features and labels for training.\"\"\"\n",
        "        print(\"Preparing data for training\")  # Debug\n",
        "        data = loans.merge(clients, on=\"client_id\", how=\"left\")\n",
        "        # Aggregate transactions and flatten column names\n",
        "        agg_data = transactions.groupby(\"loan_id\").agg({\n",
        "            \"amount\": [\"sum\", \"count\"],\n",
        "            \"type\": lambda x: x.value_counts().index[0] if not x.empty else \"none\"\n",
        "        }).reset_index()\n",
        "        # Flatten MultiIndex by renaming columns\n",
        "        agg_data.columns = ['loan_id', 'transaction_amount_sum', 'transaction_count', 'transaction_type']\n",
        "        data = data.merge(agg_data, on=\"loan_id\", how=\"left\")\n",
        "        data = data[[\n",
        "            \"loan_id\", \"client_id\", \"amount\", \"status\", \"start_date\",\n",
        "            \"name\", \"branch\", \"region\", \"income\", \"created_at\",\n",
        "            \"transaction_amount_sum\", \"transaction_count\", \"transaction_type\"\n",
        "        ]]\n",
        "        features = [\"loan_amount\", \"income\", \"transaction_amount_sum\", \"transaction_count\"]\n",
        "        data = data.rename(columns={\"amount\": \"loan_amount\"})  # Rename for consistency\n",
        "        X = data[features].fillna(0)\n",
        "        y = data[\"status\"].apply(lambda x: 1 if x == \"default\" else 0)\n",
        "        print(f\"Prepared {X.shape[0]} samples with features: {features}\")  # Debug\n",
        "        return X, y\n",
        "\n",
        "    def train_model(self, X, y, model_id=None):\n",
        "        \"\"\"Train XGBoost model and save to file and database.\"\"\"\n",
        "        model_id = model_id or f\"M_{random.getrandbits(32):08x}\"\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100, max_depth=3, learning_rate=0.1,\n",
        "            random_state=config.SEEDS[\"random\"], eval_metric=\"auc\"\n",
        "        )\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict_proba(X)[:, 1]\n",
        "        auc = roc_auc_score(y, y_pred)\n",
        "        accuracy = accuracy_score(y, model.predict(X))\n",
        "        model_path = os.path.join(self.model_dir, f\"{model_id}.pkl\")\n",
        "        with open(model_path, \"wb\") as f:\n",
        "            pickle.dump(model, f)\n",
        "        print(f\"Model saved to {model_path}\")  # Debug\n",
        "        database = db.DB()\n",
        "        try:\n",
        "            database.cursor.execute(\n",
        "                \"INSERT OR IGNORE INTO models (model_id, type, version, created_at) VALUES (?, ?, ?, ?)\",\n",
        "                (model_id, \"xgboost\", \"1.0\", datetime.now(UTC).isoformat())\n",
        "            )\n",
        "            database.cursor.execute(\n",
        "                \"INSERT INTO model_versions (version_id, model_id, config_json, data_hash, metrics_json, commit_ref, comments, created_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "                (\n",
        "                    f\"V_{uuid.uuid4().hex[:8]}\", model_id,  # Unique version_id\n",
        "                    json.dumps({\"n_estimators\": 100, \"max_depth\": 3, \"learning_rate\": 0.1}),\n",
        "                    str(X.values.tobytes()),  # Use bytes of numeric data for hash\n",
        "                    json.dumps({\"auc\": float(auc), \"accuracy\": float(accuracy)}),\n",
        "                    \"initial\", \"Trained for default probability\", datetime.now(UTC).isoformat()\n",
        "                )\n",
        "            )\n",
        "            database.conn.commit()\n",
        "            database.log_action(\n",
        "                \"1\", \"admin\", \"train_model\", model_id, \"model\",\n",
        "                \"Trained model for default probability\", {}, {\"auc\": float(auc), \"accuracy\": float(accuracy)}\n",
        "            )\n",
        "            print(f\"Model {model_id} trained. AUC: {auc:.3f}, Accuracy: {accuracy:.3f}\")\n",
        "            return model_id\n",
        "        finally:\n",
        "            database.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trainer = ModelTrainer()\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(100, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.save_to_db(clients, loans, transactions)\n",
        "    X, y = trainer.prepare_data(clients, loans, transactions)\n",
        "    model_id = trainer.train_model(X, y)\n",
        "    print(f\"Trained model: {model_id}\")\n",
        "''')\n",
        "!test -f modules/models/train.py && echo \"train.py created\" || echo \"Failed to create train.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaegXJepWZ0s",
        "outputId": "72b336fb-08b3-49f2-b7c4-663690cadc48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.makedirs('modules/models', exist_ok=True)\n",
        "with open('modules/models/predict.py', 'w') as f:\n",
        "    f.write('''# modules/models/predict.py\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from datetime import datetime, UTC\n",
        "sys.path.append(os.getcwd())\n",
        "try:\n",
        "    from modules.core import db, config\n",
        "    from modules.models import train\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class ModelPredictor:\n",
        "    \"\"\"Predict default probability and loan limits using trained model.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.config = config.get_config()\n",
        "        self.db_path = self.config[\"db_path\"]\n",
        "        self.model_dir = self.config[\"model_dir\"]\n",
        "        self.explainer = None\n",
        "\n",
        "    def predict(self, model_id, data):\n",
        "        \"\"\"Make predictions for given data using specified model.\"\"\"\n",
        "        model_path = os.path.join(self.model_dir, f\"{model_id}.pkl\")\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            model = pickle.load(f)\n",
        "        print(f\"Loaded model: {model_id}\")  # Debug\n",
        "        trainer = train.ModelTrainer()\n",
        "        X, _ = trainer.prepare_data(data[\"clients\"], data[\"loans\"], data[\"transactions\"])\n",
        "        probs = model.predict_proba(X)[:, 1]\n",
        "        # Merge loans with clients to align incomes with loans\n",
        "        merged_data = data[\"loans\"].merge(data[\"clients\"][[\"client_id\", \"income\"]],\n",
        "                                        on=\"client_id\", how=\"left\")\n",
        "        loan_limits = merged_data[\"income\"] * 2.0 * (1 - probs)\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X)\n",
        "        result = pd.DataFrame({\n",
        "            \"loan_id\": data[\"loans\"][\"loan_id\"],\n",
        "            \"default_probability\": probs,\n",
        "            \"recommended_loan_limit\": loan_limits,\n",
        "            \"shap_values\": [json.dumps(s.tolist()) for s in shap_values]\n",
        "        })\n",
        "        database = db.DB()\n",
        "        try:\n",
        "            database.log_action(\n",
        "                \"1\", \"admin\", \"predict\", model_id, \"model\",\n",
        "                \"Made predictions for loans\", {}, {\"num_predictions\": len(probs)}\n",
        "            )\n",
        "            database.conn.commit()\n",
        "            print(f\"Predictions made for {len(probs)} loans\")  # Debug\n",
        "            return result\n",
        "        finally:\n",
        "            database.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predictor = ModelPredictor()\n",
        "    trainer = train.ModelTrainer()\n",
        "    generator = train.generators.DataGenerator()\n",
        "    clients = generator.generate_clients(10, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.save_to_db(clients, loans, transactions)\n",
        "    X, y = trainer.prepare_data(clients, loans, transactions)\n",
        "    model_id = trainer.train_model(X, y)\n",
        "    predictions = predictor.predict(model_id, {\"clients\": clients, \"loans\": loans, \"transactions\": transactions})\n",
        "    print(predictions)\n",
        "''')\n",
        "!test -f modules/models/predict.py && echo \"predict.py created\" || echo \"Failed to create predict.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBzBxtaKWfK6",
        "outputId": "ca2f1415-e0df-45df-fa0c-aeb30ad71fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('tests', exist_ok=True)\n",
        "with open('tests/test_models.py', 'w') as f:\n",
        "    f.write('''# tests/test_models.py\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sys.path.append(os.getcwd())\n",
        "try:\n",
        "    from modules.core import config, db\n",
        "    from modules.synth import generators\n",
        "    from modules.models import train, predict\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "def test_prepare_data():\n",
        "    trainer = train.ModelTrainer()\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(5, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    X, y = trainer.prepare_data(clients, loans, transactions)\n",
        "    assert X.shape[0] == len(loans), \"Incorrect number of samples\"\n",
        "    assert set(X.columns) == {\"loan_amount\", \"income\", \"transaction_amount_sum\", \"transaction_count\"}, \"Incorrect features\"\n",
        "    assert y.isin([0, 1]).all(), \"Invalid labels\"\n",
        "\n",
        "def test_train_model():\n",
        "    trainer = train.ModelTrainer()\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(10, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.save_to_db(clients, loans, transactions)\n",
        "    X, y = trainer.prepare_data(clients, loans, transactions)\n",
        "    model_id = trainer.train_model(X, y)\n",
        "    assert os.path.exists(os.path.join(config.get_config()[\"model_dir\"], f\"{model_id}.pkl\")), \"Model file not saved\"\n",
        "    database = db.DB()\n",
        "    try:\n",
        "        database.cursor.execute(\"SELECT * FROM models WHERE model_id = ?\", (model_id,))\n",
        "        assert database.cursor.fetchone(), \"Model not saved to DB\"\n",
        "        database.cursor.execute(\"SELECT * FROM model_versions WHERE model_id = ?\", (model_id,))\n",
        "        assert database.cursor.fetchone(), \"Model version not saved to DB\"\n",
        "        database.cursor.execute(\"SELECT * FROM audit_logs WHERE target_type = 'model' AND action = 'train_model'\")\n",
        "        assert len(database.cursor.fetchall()) > 0, \"Audit log not recorded\"\n",
        "    finally:\n",
        "        database.close()\n",
        "\n",
        "def test_predict():\n",
        "    trainer = train.ModelTrainer()\n",
        "    predictor = predict.ModelPredictor()\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(5, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    generator.save_to_db(clients, loans, transactions)\n",
        "    X, y = trainer.prepare_data(clients, loans, transactions)\n",
        "    model_id = trainer.train_model(X, y)\n",
        "    predictions = predictor.predict(model_id, {\"clients\": clients, \"loans\": loans, \"transactions\": transactions})\n",
        "    assert len(predictions) == len(loans), \"Incorrect number of predictions\"\n",
        "    assert set(predictions.columns) == {\"loan_id\", \"default_probability\", \"recommended_loan_limit\", \"shap_values\"}, \"Incorrect prediction columns\"\n",
        "    assert (predictions[\"default_probability\"] >= 0).all() and (predictions[\"default_probability\"] <= 1).all(), \"Invalid probabilities\"\n",
        "    assert (predictions[\"recommended_loan_limit\"] >= 0).all(), \"Invalid loan limits\"\n",
        "    database = db.DB()\n",
        "    try:\n",
        "        database.cursor.execute(\"SELECT * FROM audit_logs WHERE target_type = 'model' AND action = 'predict'\")\n",
        "        assert len(database.cursor.fetchall()) > 0, \"Audit log not recorded\"\n",
        "    finally:\n",
        "        database.close()\n",
        "\n",
        "def test_model_persistence():\n",
        "    trainer = train.ModelTrainer()\n",
        "    generator = generators.DataGenerator()\n",
        "    clients = generator.generate_clients(5, fraud_ratio=0.2)\n",
        "    loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "    transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "    X, y = trainer.prepare_data(clients, loans, transactions)\n",
        "    model_id = trainer.train_model(X, y)\n",
        "    model_path = os.path.join(config.get_config()[\"model_dir\"], f\"{model_id}.pkl\")\n",
        "    with open(model_path, \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    assert model is not None, \"Model not loaded correctly\"\n",
        "''')\n",
        "!test -f tests/test_models.py && echo \"test_models.py created\" || echo \"Failed to create test_models.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj5rOQxbWurA",
        "outputId": "a3c6b9c5-8904-4097-8930-902f299c08ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_models.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "print(f\"Current working directory: {os.getcwd()}\")  # Debug\n",
        "\n",
        "# Reset database and marker file\n",
        "!rm -f data/loan_iq.db data/.deps_ok\n",
        "\n",
        "# Ensure dependencies are installed\n",
        "!python modules/bootstrap/deps.py\n",
        "\n",
        "# Verify directories\n",
        "!ls modules/bootstrap || echo \"modules/bootstrap not found\"\n",
        "!ls modules/core || echo \"modules/core not found\"\n",
        "!ls modules/synth || echo \"modules/synth not found\"\n",
        "!ls modules/models || echo \"modules/models not found\"\n",
        "!ls tests || echo \"tests not found\"\n",
        "!ls data || echo \"data not found\"\n",
        "\n",
        "# Run all scripts\n",
        "!python modules/core/config.py\n",
        "!python modules/core/db.py\n",
        "!python modules/core/utils.py\n",
        "!python modules/core/auth.py\n",
        "!python modules/synth/faker_engine.py\n",
        "!python modules/synth/generators.py\n",
        "!python modules/models/train.py\n",
        "!python modules/models/predict.py\n",
        "\n",
        "# Run tests\n",
        "!pytest tests/test_models.py -v\n",
        "\n",
        "# Verify files\n",
        "!ls modules/bootstrap\n",
        "!ls modules/core\n",
        "!ls modules/synth\n",
        "!ls modules/models\n",
        "!ls tests\n",
        "!ls data\n",
        "\n",
        "# Expected output:\n",
        "# Current working directory: /content\n",
        "# Dependencies installed successfully.\n",
        "# deps.py\n",
        "# auth.py  config.py  db.py  utils.py\n",
        "# faker_engine.py  generators.py\n",
        "# predict.py  train.py\n",
        "# test_models.py\n",
        "# .deps_ok  clients.csv  loans.csv  loan_iq.db  models  reports  transactions.csv\n",
        "# Config loaded: {...}\n",
        "# sys.path: [...]\n",
        "# Creating database at: data/loan_iq.db\n",
        "# Database created: True\n",
        "# Database initialized.\n",
        "# Before snapshot: {}\n",
        "# After snapshot: {}\n",
        "# Test action result: {'result': 'test'}\n",
        "# Auth result: {'user_id': '1', 'role': 'admin'}\n",
        "# Adding user with target_id: test_1\n",
        "# Before snapshot: {}\n",
        "# After snapshot: {'user_id': 'test_1', 'username': 'test_user', 'password': 'test_pass', 'role': 'user'}\n",
        "# Added user: test_1\n",
        "# Client ID: C_...\n",
        "# Client Name: ...\n",
        "# Loan ID: L_...\n",
        "# Transaction ID: T_...\n",
        "# Income: ...\n",
        "# Branch: ...\n",
        "# Region: ...\n",
        "# Loan Amount: ...\n",
        "# Loan Status: ...\n",
        "# Transaction Amount: ...\n",
        "# Transaction Type: ...\n",
        "# Random Date: ...\n",
        "# Saving to database: data/loan_iq.db\n",
        "# Saved 10 clients, ... loans, ... transactions to DB\n",
        "# Exported data to data/[clients,loans,transactions].csv\n",
        "# Generated and saved synthetic data.\n",
        "# Saving to database: data/loan_iq.db\n",
        "# Saved 100 clients, ... loans, ... transactions to DB\n",
        "# Preparing data for training\n",
        "# Prepared ... samples with features: ['loan_amount', 'income', 'transaction_amount_sum', 'transaction_count']\n",
        "# Model saved to models/M_....pkl\n",
        "# Model M_... trained. AUC: 0.XXX, Accuracy: 0.XXX\n",
        "# Trained model: M_...\n",
        "# Saving to database: data/loan_iq.db\n",
        "# Saved 10 clients, ... loans, ... transactions to DB\n",
        "# Preparing data for training\n",
        "# Prepared ... samples with features: ['loan_amount', 'income', 'transaction_amount_sum', 'transaction_count']\n",
        "# Model saved to models/M_....pkl\n",
        "# Model M_... trained. AUC: 0.XXX, Accuracy: 0.XXX\n",
        "# Loaded model: M_...\n",
        "# Predictions made for ... loans\n",
        "# [DataFrame with loan_id, default_probability, recommended_loan_limit, shap_values]\n",
        "# ============================= test session starts =============================\n",
        "# tests/test_models.py::test_prepare_data PASSED\n",
        "# tests/test_models.py::test_train_model PASSED\n",
        "# tests/test_models.py::test_predict PASSED\n",
        "# tests/test_models.py::test_model_persistence PASSED\n",
        "# =========================== 4 passed in 0.XXs ==========================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ybAOEyTW1mG",
        "outputId": "5c9ff585-9285-40c8-8a40-591a1cf4cc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Requirement already satisfied: scikit-learn==1.5.1 in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (3.6.0)\n",
            "Dependencies installed successfully.\n",
            "deps.py\n",
            "auth.py  config.py  db.py  __pycache__\tutils.py\n",
            "faker_engine.py  generators.py\t__pycache__\n",
            "predict.py  __pycache__  train.py\n",
            "__pycache__  test_models.py\n",
            "clients.csv  loans.csv\treports  transactions.csv\n",
            "Config loaded: {'data_dir': 'data', 'model_dir': 'models', 'report_dir': 'data/reports', 'db_path': 'data/loan_iq.db', 'drive_root': '/content/drive/MyDrive/loan_iq', 'streamlit_port': 8501, 'fraud_types': ['ghost_client', 'duplicate_id', 'missed_payment', 'identity_theft'], 'regions': ['urban', 'rural', 'semi_urban'], 'max_clients_batch': 70000, 'default_batch_size': 1000}\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Database initialized.\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Test action result: {'result': 'test'}\n",
            "sys.path: ['/content/modules/core', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/modules/core/auth.py\", line 59, in <module>\n",
            "    database.cursor.execute(\n",
            "sqlite3.OperationalError: table users has no column named created_at\n",
            "Client ID: C_bdd640fb\n",
            "Client Name: Daniel Doyle\n",
            "Loan ID: L_8b9d2434\n",
            "Transaction ID: T_0822e8f3\n",
            "Income: 71154.1438766307\n",
            "Branch: North Jefferyhaven\n",
            "Region: urban\n",
            "Loan Amount: 37335.97448823181\n",
            "Loan Status: active\n",
            "Transaction Amount: 300.88966433394046\n",
            "Transaction Type: interest\n",
            "Random Date: 2025-04-07T08:49:21.639320\n",
            "sys.path: ['/content/modules/synth', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Saving to database: data/loan_iq.db\n",
            "Saved 10 clients, 15 loans, 30 transactions to DB\n",
            "Exported data to data/[clients,loans,transactions].csv\n",
            "Generated and saved synthetic data.\n",
            "sys.path: ['/content/modules/models', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Saving to database: data/loan_iq.db\n",
            "Saved 100 clients, 153 loans, 316 transactions to DB\n",
            "Preparing data for training\n",
            "Prepared 156 samples with features: ['loan_amount', 'income', 'transaction_amount_sum', 'transaction_count']\n",
            "Model saved to models/M_2c33350c.pkl\n",
            "sys.path: ['/content/modules/models', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/content', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Model M_2c33350c trained. AUC: 0.993, Accuracy: 0.929\n",
            "Trained model: M_2c33350c\n",
            "sys.path: ['/content/modules/models', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/content', '/content', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Saving to database: data/loan_iq.db\n",
            "Saved 10 clients, 15 loans, 30 transactions to DB\n",
            "Preparing data for training\n",
            "Prepared 15 samples with features: ['loan_amount', 'income', 'transaction_amount_sum', 'transaction_count']\n",
            "Model saved to models/M_e64d1bcb.pkl\n",
            "sys.path: ['/content/modules/models', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/content', '/content', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Model M_e64d1bcb trained. AUC: 0.807, Accuracy: 0.867\n",
            "Loaded model: M_e64d1bcb\n",
            "Preparing data for training\n",
            "Prepared 15 samples with features: ['loan_amount', 'income', 'transaction_amount_sum', 'transaction_count']\n",
            "sys.path: ['/content/modules/models', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/content', '/content', '/content', '/content', '/content', '/content']\n",
            "Creating database at: data/loan_iq.db\n",
            "Database created: True\n",
            "Predictions made for 15 loans\n",
            "       loan_id  ...                                        shap_values\n",
            "0   L_27209bdf  ...  [-0.7254708409309387, 0.7759578227996826, 0.44...\n",
            "1   L_98ae4334  ...  [0.8137450814247131, 0.7759578227996826, -0.31...\n",
            "2   L_77d21e02  ...  [0.4874284267425537, 0.7759578227996826, -0.31...\n",
            "3   L_f143262f  ...  [0.8137450814247131, 0.7759578227996826, 0.449...\n",
            "4   L_e2817efd  ...  [0.4874284267425537, -0.735328197479248, -0.31...\n",
            "5   L_5715bd6f  ...  [0.4874284267425537, -0.735328197479248, -0.28...\n",
            "6   L_00d4af59  ...  [-0.7254708409309387, -0.735328197479248, 0.44...\n",
            "7   L_f8cda88b  ...  [-0.7254708409309387, -0.735328197479248, 0.44...\n",
            "8   L_1b3dbd5c  ...  [-0.7254708409309387, -0.735328197479248, -0.3...\n",
            "9   L_81f631d4  ...  [-0.7254708409309387, -0.735328197479248, 0.44...\n",
            "10  L_295b4715  ...  [-0.7254708409309387, 0.7759578227996826, -0.3...\n",
            "11  L_eb2263dd  ...  [0.4874284267425537, -0.735328197479248, -0.31...\n",
            "12  L_1ca35cfb  ...  [0.4874284267425537, 0.7759578227996826, -0.31...\n",
            "13  L_ce88cb2d  ...  [-0.7254708409309387, -0.735328197479248, 0.44...\n",
            "14  L_913e4de2  ...  [-0.7254708409309387, -0.735328197479248, 0.44...\n",
            "\n",
            "[15 rows x 4 columns]\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: Faker-28.1.0, anyio-4.10.0, typeguard-4.4.4, langsmith-0.4.16\n",
            "collected 4 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_models.py::test_prepare_data \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 25%]\u001b[0m\n",
            "tests/test_models.py::test_train_model \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 50%]\u001b[0m\n",
            "tests/test_models.py::test_predict \u001b[32mPASSED\u001b[0m\u001b[32m                                [ 75%]\u001b[0m\n",
            "tests/test_models.py::test_model_persistence \u001b[32mPASSED\u001b[0m\u001b[32m                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 3.99s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
            "deps.py\n",
            "auth.py  config.py  db.py  __pycache__\tutils.py\n",
            "faker_engine.py  generators.py\t__pycache__\n",
            "predict.py  __pycache__  train.py\n",
            "__pycache__  test_models.py\n",
            "clients.csv  loan_iq.db  loans.csv  reports  transactions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.makedirs('modules/streamlit_app', exist_ok=True)\n",
        "with open('modules/streamlit_app/app.py', 'w') as f:\n",
        "    f.write('''# modules/streamlit_app/app.py\n",
        "import sys\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import plotly.express as px\n",
        "import json\n",
        "sys.path.append(os.getcwd())\n",
        "try:\n",
        "    from modules.core import config, db, auth\n",
        "    from modules.synth import generators\n",
        "    from modules.models import train, predict\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "st.set_page_config(page_title=\"Loan IQ Dashboard\", layout=\"wide\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Streamlit dashboard for Loan IQ.\"\"\"\n",
        "    config_data = config.get_config()\n",
        "    st.title(\"Loan IQ Dashboard\")\n",
        "\n",
        "    # Authentication\n",
        "    if \"authenticated\" not in st.session_state:\n",
        "        st.session_state.authenticated = False\n",
        "        st.session_state.user_role = None\n",
        "        st.session_state.username = None\n",
        "\n",
        "    if not st.session_state.authenticated:\n",
        "        st.subheader(\"Login or Register\")\n",
        "        # Tabs for Login and Register\n",
        "        tab1, tab2 = st.tabs([\"Login\", \"Register\"])\n",
        "\n",
        "        with tab1:\n",
        "            st.subheader(\"Login\")\n",
        "            login_username = st.text_input(\"Username\", key=\"login_username\")\n",
        "            login_password = st.text_input(\"Password\", type=\"password\", key=\"login_password\")\n",
        "            if st.button(\"Login\"):\n",
        "                authenticator = auth.Auth()  # Updated to Auth\n",
        "                user = authenticator.authenticate(login_username, login_password)\n",
        "                if user:\n",
        "                    st.session_state.authenticated = True\n",
        "                    st.session_state.user_role = user[\"role\"]\n",
        "                    st.session_state.username = login_username\n",
        "                    st.success(f\"Logged in as {login_username} ({user['role']})\")\n",
        "                    st.rerun()\n",
        "                else:\n",
        "                    st.error(\"Invalid credentials. Contact admin at admin@loaniq.com for password issues.\")\n",
        "\n",
        "        with tab2:\n",
        "            st.subheader(\"Register\")\n",
        "            reg_username = st.text_input(\"New Username\", key=\"reg_username\")\n",
        "            reg_password = st.text_input(\"New Password\", type=\"password\", key=\"reg_password\")\n",
        "            if st.button(\"Register\"):\n",
        "                authenticator = auth.Auth()  # Updated to Auth\n",
        "                user = authenticator.register(reg_username, reg_password)\n",
        "                if user:\n",
        "                    st.success(f\"Registered {reg_username}. Logging in...\")\n",
        "                    st.session_state.authenticated = True\n",
        "                    st.session_state.user_role = user[\"role\"]\n",
        "                    st.session_state.username = reg_username\n",
        "                    st.rerun()\n",
        "                else:\n",
        "                    st.error(\"Registration failed: Username already exists\")\n",
        "        return\n",
        "\n",
        "    # Sidebar for navigation\n",
        "    st.sidebar.title(f\"Welcome, {st.session_state.username}\")\n",
        "    page = st.sidebar.selectbox(\"Select Page\", [\"Data Overview\", \"Predictions\", \"Reports\"])\n",
        "\n",
        "    # Initialize database\n",
        "    database = db.DB()\n",
        "    conn = sqlite3.connect(config_data[\"db_path\"])\n",
        "\n",
        "    if page == \"Data Overview\":\n",
        "        st.subheader(\"Data Overview\")\n",
        "        # Load data\n",
        "        clients = pd.read_sql_query(\"SELECT * FROM clients LIMIT 10\", conn)\n",
        "        loans = pd.read_sql_query(\"SELECT * FROM loans LIMIT 10\", conn)\n",
        "        transactions = pd.read_sql_query(\"SELECT * FROM transactions LIMIT 10\", conn)\n",
        "\n",
        "        # Display tables\n",
        "        st.write(\"### Clients\")\n",
        "        st.dataframe(clients)\n",
        "        st.write(\"### Loans\")\n",
        "        st.dataframe(loans)\n",
        "        st.write(\"### Transactions\")\n",
        "        st.dataframe(transactions)\n",
        "\n",
        "        # Simple visualization\n",
        "        if not loans.empty:\n",
        "            fig = px.bar(loans, x=\"status\", title=\"Loan Status Distribution\")\n",
        "            st.plotly_chart(fig)\n",
        "\n",
        "    elif page == \"Predictions\":\n",
        "        st.subheader(\"Loan Default Predictions\")\n",
        "        if st.session_state.user_role == \"admin\":\n",
        "            if st.button(\"Generate New Data and Predictions\"):\n",
        "                generator = generators.DataGenerator()\n",
        "                clients = generator.generate_clients(10, fraud_ratio=0.2)\n",
        "                loans = generator.generate_loans(clients, n_per_client=2, fraud_ratio=0.2)\n",
        "                transactions = generator.generate_transactions(loans, n_per_loan=3, fraud_ratio=0.2)\n",
        "                generator.save_to_db(clients, loans, transactions)\n",
        "                trainer = train.ModelTrainer()\n",
        "                X, y = trainer.prepare_data(clients, loans, transactions)\n",
        "                model_id = trainer.train_model(X, y)\n",
        "                predictor = predict.ModelPredictor()\n",
        "                predictions = predictor.predict(model_id, {\"clients\": clients, \"loans\": loans, \"transactions\": transactions})\n",
        "                # Save predictions to session state\n",
        "                st.session_state.predictions = predictions\n",
        "                st.session_state.model_id = model_id\n",
        "                st.success(f\"Generated data and trained model {model_id}\")\n",
        "\n",
        "            # Display predictions\n",
        "            if \"predictions\" in st.session_state:\n",
        "                st.write(\"### Predictions\")\n",
        "                st.dataframe(st.session_state.predictions)\n",
        "                # Plot default probabilities\n",
        "                fig = px.histogram(st.session_state.predictions, x=\"default_probability\",\n",
        "                                 title=\"Default Probability Distribution\")\n",
        "                st.plotly_chart(fig)\n",
        "        else:\n",
        "            st.error(\"Access restricted to admin users\")\n",
        "\n",
        "    elif page == \"Reports\":\n",
        "        st.subheader(\"Reports\")\n",
        "        # Example report: Average income by region\n",
        "        query = \"\"\"\n",
        "        SELECT region, AVG(income) as avg_income\n",
        "        FROM clients\n",
        "        GROUP BY region\n",
        "        \"\"\"\n",
        "        report = pd.read_sql_query(query, conn)\n",
        "        st.write(\"### Average Income by Region\")\n",
        "        st.dataframe(report)\n",
        "        fig = px.bar(report, x=\"region\", y=\"avg_income\", title=\"Average Income by Region\")\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "!test -f modules/streamlit_app/app.py && echo \"app.py created\" || echo \"Failed to create app.py\"\n",
        "\n",
        "# Install Streamlit and ngrok\n",
        "!pip install streamlit pyngrok plotly --quiet\n",
        "\n",
        "# Run Streamlit with ngrok\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Set up ngrok with hardcoded authtoken\n",
        "!ngrok config add-authtoken 31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\n",
        "\n",
        "# Start Streamlit server\n",
        "port = 8501\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\"Streamlit app running at: {public_url}\")\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"modules/streamlit_app/app.py\", \"--server.port\", str(port)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuM_9GncbO_U",
        "outputId": "6d685c1a-ceca-4f76-b28d-0935897b0848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py created\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Streamlit app running at: https://b5be53fa8cf6.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['streamlit', 'run', 'modules/streamlit_app/a...>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# === Step 0: Backup existing file if it exists ===\n",
        "db_file = \"modules/core/db.py\"\n",
        "backup_file = \"modules/core/db_backup.py\"\n",
        "if os.path.exists(db_file):\n",
        "    shutil.copy(db_file, backup_file)\n",
        "    print(f\"📦 Backup created at {backup_file}\")\n",
        "\n",
        "# === Step 1: Write the new db.py ===\n",
        "code = \"\"\"\\\n",
        "import sqlite3\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "class Database:\n",
        "    def __init__(self, db_path=\"data/loaniq.db\"):\n",
        "        os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
        "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
        "        self.cursor = self.conn.cursor()\n",
        "        self._init_tables()\n",
        "\n",
        "    def _init_tables(self):\n",
        "        # Users table with created_at column\n",
        "        self.cursor.execute(\\\"\\\"\\\"\n",
        "        CREATE TABLE IF NOT EXISTS users (\n",
        "            user_id TEXT PRIMARY KEY,\n",
        "            username TEXT UNIQUE,\n",
        "            password TEXT,\n",
        "            role TEXT,\n",
        "            created_at TEXT\n",
        "        )\n",
        "        \\\"\\\"\\\")\n",
        "        self.conn.commit()\n",
        "\n",
        "    def add_user(self, user_id, username, password, role=\"user\"):\n",
        "        created_at = datetime.utcnow().isoformat()\n",
        "        self.cursor.execute(\\\"\\\"\\\"\n",
        "        INSERT INTO users (user_id, username, password, role, created_at)\n",
        "        VALUES (?, ?, ?, ?, ?)\n",
        "        \\\"\\\"\\\", (user_id, username, password, role, created_at))\n",
        "        self.conn.commit()\n",
        "\n",
        "    def list_users(self):\n",
        "        self.cursor.execute(\"SELECT * FROM users\")\n",
        "        return self.cursor.fetchall()\n",
        "\n",
        "# === Quick test ===\n",
        "if __name__ == \"__main__\":\n",
        "    db = Database()\n",
        "    db.cursor.execute(\"PRAGMA table_info(users)\")\n",
        "    cols = [row[1] for row in db.cursor.fetchall()]\n",
        "    print(\"✅ Users table columns:\", cols)\n",
        "\"\"\"\n",
        "\n",
        "with open(db_file, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# === Step 2: Run test ===\n",
        "from modules.core import db as db_module\n",
        "db = db_module.Database()\n",
        "db.cursor.execute(\"PRAGMA table_info(users)\")\n",
        "cols = [row[1] for row in db.cursor.fetchall()]\n",
        "print(\"✅ Users table columns (test):\", cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqB-u5dlPt0u",
        "outputId": "27f40dc4-9cd7-4733-8966-1045170a559c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Users table columns (test): ['user_id', 'username', 'password', 'role', 'created_at']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# === Step 0: Backup existing file if it exists ===\n",
        "cfg_file = \"modules/core/config.py\"\n",
        "backup_file = \"modules/core/config_backup.py\"\n",
        "if os.path.exists(cfg_file):\n",
        "    shutil.copy(cfg_file, backup_file)\n",
        "    print(f\"📦 Backup created at {backup_file}\")\n",
        "\n",
        "# === Step 1: Write the new config.py ===\n",
        "code = \"\"\"\\\n",
        "import os\n",
        "\n",
        "# === Admin Credentials ===\n",
        "ADMIN_USERNAME = \"admin\"\n",
        "ADMIN_PASSWORD = \"Shady868\"\n",
        "\n",
        "# === Ngrok Token (env first, fallback to hardcoded) ===\n",
        "NGROK_AUTHTOKEN = os.getenv(\n",
        "    \"NGROK_AUTHTOKEN\",\n",
        "    \"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\"\n",
        ")\n",
        "\n",
        "# === Database Path ===\n",
        "DB_PATH = \"data/loaniq.db\"\n",
        "\n",
        "def get_admin_credentials():\n",
        "    return ADMIN_USERNAME, ADMIN_PASSWORD\n",
        "\n",
        "def get_ngrok_token():\n",
        "    return NGROK_AUTHTOKEN\n",
        "\n",
        "def get_db_path():\n",
        "    return DB_PATH\n",
        "\"\"\"\n",
        "\n",
        "with open(cfg_file, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# === Step 2: Run test ===\n",
        "from modules.core import config\n",
        "\n",
        "print(\"✅ Admin creds:\", config.get_admin_credentials())\n",
        "print(\"✅ Ngrok token (first 8 chars):\", config.get_ngrok_token()[:8] + \"...\")\n",
        "print(\"✅ DB path:\", config.get_db_path())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMVzLtmBQEfd",
        "outputId": "b9d62ab1-fa9f-470f-86e7-3303bf065d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Admin creds: ('admin', 'Shady868')\n",
            "✅ Ngrok token (first 8 chars): 31rYvgkl...\n",
            "✅ DB path: data/loaniq.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# === Step 0: Backup existing file if it exists ===\n",
        "tunnel_file = \"modules/bootstrap/tunnel.py\"\n",
        "backup_file = \"modules/bootstrap/tunnel_backup.py\"\n",
        "if os.path.exists(tunnel_file):\n",
        "    shutil.copy(tunnel_file, backup_file)\n",
        "    print(f\"📦 Backup created at {backup_file}\")\n",
        "\n",
        "# === Step 1: Write the new tunnel.py ===\n",
        "code = \"\"\"\\\n",
        "import os\n",
        "from pyngrok import ngrok, conf\n",
        "from modules.core import config\n",
        "\n",
        "def start(port:int=8501):\n",
        "    token = config.get_ngrok_token()\n",
        "    if token:\n",
        "        conf.get_default().auth_token = token\n",
        "    url = ngrok.connect(port, \"http\").public_url\n",
        "    print(f\"🌐 Ngrok tunnel started: {url}\")\n",
        "    return url\n",
        "\n",
        "def stop():\n",
        "    ngrok.kill()\n",
        "    print(\"🛑 Ngrok tunnel stopped.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(tunnel_file, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# === Step 2: Run test ===\n",
        "from modules.bootstrap import tunnel\n",
        "\n",
        "print(\">>> Starting Ngrok test tunnel...\")\n",
        "url = tunnel.start(8501)\n",
        "print(\"✅ Test URL:\", url)\n",
        "tunnel.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NClXjeYvQjea",
        "outputId": "4888b582-0c1b-4cad-d67a-09479f39af49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Starting Ngrok test tunnel...\n",
            "🌐 Ngrok tunnel started: https://07780d982eab.ngrok-free.app\n",
            "✅ Test URL: https://07780d982eab.ngrok-free.app\n",
            "🛑 Ngrok tunnel stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# === Step 0: Backup existing file if it exists ===\n",
        "auth_file = \"modules/core/auth.py\"\n",
        "backup_file = \"modules/core/auth_backup.py\"\n",
        "if os.path.exists(auth_file):\n",
        "    shutil.copy(auth_file, backup_file)\n",
        "    print(f\"📦 Backup created at {backup_file}\")\n",
        "\n",
        "# === Step 1: Write the new auth.py ===\n",
        "code = \"\"\"\\\n",
        "from modules.core import config, db\n",
        "\n",
        "class AuthManager:\n",
        "    def __init__(self):\n",
        "        self.db = db.Database()\n",
        "        self.admin_user, self.admin_pass = config.get_admin_credentials()\n",
        "\n",
        "    def register(self, user_id, username, password, role=\"user\"):\n",
        "        # Adds new user into DB\n",
        "        self.db.add_user(user_id, username, password, role)\n",
        "\n",
        "    def login(self, username, password):\n",
        "        # Admin login (hardcoded)\n",
        "        if username == self.admin_user and password == self.admin_pass:\n",
        "            return {\"username\": username, \"role\": \"admin\"}\n",
        "\n",
        "        # Check database users\n",
        "        users = self.db.list_users()\n",
        "        for row in users:\n",
        "            if row[1] == username and row[2] == password:\n",
        "                return {\"username\": username, \"role\": row[3]}\n",
        "\n",
        "        return None\n",
        "\"\"\"\n",
        "\n",
        "with open(auth_file, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# === Step 2: Run test ===\n",
        "from modules.core.auth import AuthManager\n",
        "import uuid\n",
        "\n",
        "auth = AuthManager()\n",
        "\n",
        "# Register test user\n",
        "uid = str(uuid.uuid4())\n",
        "auth.register(uid, \"testuser\", \"testpass\", role=\"user\")\n",
        "\n",
        "# Test login: user\n",
        "user_login = auth.login(\"testuser\", \"testpass\")\n",
        "print(\"✅ User login:\", user_login)\n",
        "\n",
        "# Test login: admin\n",
        "admin_login = auth.login(\"admin\", \"Shady868\")\n",
        "print(\"✅ Admin login:\", admin_login)\n",
        "\n",
        "# Test login: wrong\n",
        "fail_login = auth.login(\"nosuch\", \"wrong\")\n",
        "print(\"✅ Failed login:\", fail_login)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmVc7b3YQ4lU",
        "outputId": "35e75f9e-a69f-4c2b-e1f2-634d17316231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ User login: {'username': 'testuser', 'role': 'user'}\n",
            "✅ Admin login: {'username': 'admin', 'role': 'admin'}\n",
            "✅ Failed login: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# === Step 0: Backup existing file if it exists ===\n",
        "main_file = \"main.py\"\n",
        "backup_file = \"main_backup.py\"\n",
        "if os.path.exists(main_file):\n",
        "    shutil.copy(main_file, backup_file)\n",
        "    print(f\"📦 Backup created at {backup_file}\")\n",
        "\n",
        "# === Step 1: Write new main.py ===\n",
        "code = \"\"\"\\\n",
        "import streamlit as st\n",
        "from modules.core.auth import AuthManager\n",
        "\n",
        "# Import placeholder pages\n",
        "import pages._01_placeholder as page01\n",
        "import pages._02_placeholder as page02\n",
        "import pages._03_placeholder as page03\n",
        "import pages._04_placeholder as page04\n",
        "import pages._05_placeholder as page05\n",
        "import pages._06_placeholder as page06\n",
        "import pages._07_placeholder as page07\n",
        "\n",
        "PAGES = {\n",
        "    \"01 - Home\": page01,\n",
        "    \"02 - Client Onboarding\": page02,\n",
        "    \"03 - Client Dashboard\": page03,\n",
        "    \"04 - Admin Sandbox\": page04,\n",
        "    \"05 - Global Insights\": page05,\n",
        "    \"06 - Reports & Exports\": page06,\n",
        "    \"07 - Settings\": page07,\n",
        "}\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"LoanIQ\", layout=\"wide\")\n",
        "    st.title(\"💳 LoanIQ Platform\")\n",
        "\n",
        "    # === Auth ===\n",
        "    if \"user\" not in st.session_state:\n",
        "        st.session_state.user = None\n",
        "\n",
        "    auth = AuthManager()\n",
        "\n",
        "    if st.session_state.user is None:\n",
        "        st.sidebar.subheader(\"Login\")\n",
        "        username = st.sidebar.text_input(\"Username\")\n",
        "        password = st.sidebar.text_input(\"Password\", type=\"password\")\n",
        "        if st.sidebar.button(\"Login\"):\n",
        "            user = auth.login(username, password)\n",
        "            if user:\n",
        "                st.session_state.user = user\n",
        "                st.experimental_rerun()\n",
        "            else:\n",
        "                st.error(\"Invalid credentials\")\n",
        "        st.stop()\n",
        "\n",
        "    st.sidebar.success(f\"Logged in as {st.session_state.user['username']} ({st.session_state.user['role']})\")\n",
        "\n",
        "    # === Page navigation ===\n",
        "    choice = st.sidebar.radio(\"Go to\", list(PAGES.keys()))\n",
        "    page = PAGES[choice]\n",
        "    if hasattr(page, \"app\"):\n",
        "        page.app()\n",
        "    else:\n",
        "        st.warning(\"Page not yet implemented.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open(main_file, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# === Step 2: Run test ===\n",
        "print(\"✅ main.py written successfully\")\n",
        "print(\"✅ Pages wired:\", [\n",
        "    \"01 - Home\",\n",
        "    \"02 - Client Onboarding\",\n",
        "    \"03 - Client Dashboard\",\n",
        "    \"04 - Admin Sandbox\",\n",
        "    \"05 - Global Insights\",\n",
        "    \"06 - Reports & Exports\",\n",
        "    \"07 - Settings\"\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IYxGpxjRKSr",
        "outputId": "f1db0642-2e86-4802-884d-dbe52d95e275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ main.py written successfully\n",
            "✅ Pages wired: ['01 - Home', '02 - Client Onboarding', '03 - Client Dashboard', '04 - Admin Sandbox', '05 - Global Insights', '06 - Reports & Exports', '07 - Settings']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# === Step 0: Backup existing placeholder files ===\n",
        "for i in range(1,8):\n",
        "    page_file = f\"pages/{i:02d}_placeholder.py\"\n",
        "    backup_file = f\"pages/{i:02d}_placeholder_backup.py\"\n",
        "    if os.path.exists(page_file):\n",
        "        shutil.copy(page_file, backup_file)\n",
        "        print(f\"📦 Backup created at {backup_file}\")\n",
        "\n",
        "# === Step 1: Write new page stubs ===\n",
        "for i, title in enumerate([\n",
        "    \"Home\",\n",
        "    \"Client Onboarding\",\n",
        "    \"Client Dashboard\",\n",
        "    \"Admin Sandbox\",\n",
        "    \"Global Insights\",\n",
        "    \"Reports & Exports\",\n",
        "    \"Settings\"\n",
        "], start=1):\n",
        "    code = f'''\\\n",
        "import streamlit as st\n",
        "\n",
        "def app():\n",
        "    st.header(\"📄 {title}\")\n",
        "    st.info(\"This page is not implemented yet. Placeholder only.\")\n",
        "'''\n",
        "    with open(f\"pages/{i:02d}_placeholder.py\", \"w\") as f:\n",
        "        f.write(code)\n",
        "\n",
        "# === Step 2: Run test ===\n",
        "import importlib\n",
        "for i in range(1,8):\n",
        "    mod = importlib.import_module(f\"pages.{i:02d}_placeholder\")\n",
        "    print(f\"✅ Page {i:02d} imported, has app():\", hasattr(mod, \"app\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocv8nhAPRj7E",
        "outputId": "181a46da-e6ca-42f0-a561-3f48af9e793d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Backup created at pages/01_placeholder_backup.py\n",
            "📦 Backup created at pages/02_placeholder_backup.py\n",
            "📦 Backup created at pages/03_placeholder_backup.py\n",
            "📦 Backup created at pages/04_placeholder_backup.py\n",
            "📦 Backup created at pages/05_placeholder_backup.py\n",
            "📦 Backup created at pages/06_placeholder_backup.py\n",
            "📦 Backup created at pages/07_placeholder_backup.py\n",
            "✅ Page 01 imported, has app(): True\n",
            "✅ Page 02 imported, has app(): True\n",
            "✅ Page 03 imported, has app(): True\n",
            "✅ Page 04 imported, has app(): True\n",
            "✅ Page 05 imported, has app(): True\n",
            "✅ Page 06 imported, has app(): True\n",
            "✅ Page 07 imported, has app(): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time\n",
        "from modules.bootstrap import tunnel\n",
        "\n",
        "# === Step 1: Start Ngrok tunnel ===\n",
        "print(\">>> Starting Ngrok tunnel on port 8501...\")\n",
        "url = tunnel.start(8501)\n",
        "\n",
        "# === Step 2: Start Streamlit server ===\n",
        "print(\">>> Launching Streamlit app (main.py)...\")\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"main.py\"])\n",
        "\n",
        "# === Step 3: Wait and show public URL ===\n",
        "time.sleep(5)  # give server a moment to boot\n",
        "print(\"🌐 LoanIQ is live at:\", url)\n",
        "print(\"🛑 To stop: run tunnel.stop() and process.terminate()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3YJAV5lRvbD",
        "outputId": "33c1da4a-7f12-492f-ab82-993b3aad16bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Starting Ngrok tunnel on port 8501...\n",
            "🌐 Ngrok tunnel started: https://4c96b790cd9d.ngrok-free.app\n",
            ">>> Launching Streamlit app (main.py)...\n",
            "🌐 LoanIQ is live at: https://4c96b790cd9d.ngrok-free.app\n",
            "🛑 To stop: run tunnel.stop() and process.terminate()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# === Step 0: Backup if it exists (just in case) ===\n",
        "gen_file = \"modules/synth/generators.py\"\n",
        "backup_file = \"modules/synth/generators_backup.py\"\n",
        "if os.path.exists(gen_file):\n",
        "    shutil.copy(gen_file, backup_file)\n",
        "    print(f\"📦 Backup created at {backup_file}\")\n",
        "\n",
        "# === Step 1: Write stub generators.py ===\n",
        "code = \"\"\"\\\n",
        "import pandas as pd\n",
        "\n",
        "def generate_synthetic_clients(n=10):\n",
        "    # Temporary stub — real Faker logic will come in Patch 8\n",
        "    data = {\n",
        "        \"client_id\": [f\"C{i:03d}\" for i in range(1, n+1)],\n",
        "        \"loan_amount\": [1000 + i*50 for i in range(1, n+1)],\n",
        "        \"status\": [\"active\"]*n,\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\"\"\"\n",
        "\n",
        "with open(gen_file, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# === Step 2: Test import ===\n",
        "from modules.synth import generators\n",
        "df = generators.generate_synthetic_clients(5)\n",
        "print(\"✅ Synthetic stub data:\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ZZmYxzD3SIJN",
        "outputId": "f9b3725b-7ecd-404c-fa02-127d0c86a7c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Synthetic stub data:\n",
            "  client_id  loan_amount  status\n",
            "0      C001         1050  active\n",
            "1      C002         1100  active\n",
            "2      C003         1150  active\n",
            "3      C004         1200  active\n",
            "4      C005         1250  active\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/synth/generators.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "fake = Faker([\"en_KE\"])\n",
        "\n",
        "# 50 towns in Kenya spread across regions\n",
        "TOWNS = [\n",
        "    (\"Nairobi\",\"Central\"),(\"Mombasa\",\"Coast\"),(\"Kisumu\",\"Western\"),(\"Nakuru\",\"Rift\"),\n",
        "    (\"Eldoret\",\"Rift\"),(\"Thika\",\"Central\"),(\"Machakos\",\"Eastern\"),(\"Nyeri\",\"Central\"),\n",
        "    (\"Meru\",\"Eastern\"),(\"Malindi\",\"Coast\"),(\"Garissa\",\"Eastern\"),(\"Embu\",\"Eastern\"),\n",
        "    (\"Kericho\",\"Rift\"),(\"Narok\",\"Rift\"),(\"Naivasha\",\"Rift\"),(\"Kakamega\",\"Western\"),\n",
        "    (\"Bungoma\",\"Western\"),(\"Kitale\",\"Rift\"),(\"Isiolo\",\"Eastern\"),(\"Wajir\",\"Eastern\"),\n",
        "    (\"Lamu\",\"Coast\"),(\"Kilifi\",\"Coast\"),(\"Kwale\",\"Coast\"),(\"Voi\",\"Coast\"),\n",
        "    (\"Marsabit\",\"Eastern\"),(\"Nanyuki\",\"Central\"),(\"Kiambu\",\"Central\"),(\"Muranga\",\"Central\"),\n",
        "    (\"Kerugoya\",\"Central\"),(\"Chuka\",\"Eastern\"),(\"Loitoktok\",\"Rift\"),(\"Kajiado\",\"Rift\"),\n",
        "    (\"Mwingi\",\"Eastern\"),(\"Hola\",\"Coast\"),(\"Makueni\",\"Eastern\"),(\"Sotik\",\"Rift\"),\n",
        "    (\"Litein\",\"Rift\"),(\"Kabarnet\",\"Rift\"),(\"Baringo\",\"Rift\"),(\"Migori\",\"Western\"),\n",
        "    (\"Homa Bay\",\"Western\"),(\"Siaya\",\"Western\"),(\"Busia\",\"Western\"),(\"Keroka\",\"Western\"),\n",
        "    (\"Oyugis\",\"Western\"),(\"Kitui\",\"Eastern\"),(\"Taveta\",\"Coast\"),(\"Kilgoris\",\"Rift\"),\n",
        "    (\"Kapenguria\",\"Rift\"),(\"Maralal\",\"Rift\")\n",
        "]\n",
        "\n",
        "PRODUCTS = [\n",
        "    (\"Inuka\",\"5 weeks\"),\n",
        "    (\"Kuza\",\"4 weeks\"),\n",
        "    (\"Fadhili\",\"6 weeks\"),\n",
        "    (\"Imarika\",\"8 weeks\"),\n",
        "    (\"Chama Boost\",\"12 weeks\")\n",
        "]\n",
        "\n",
        "LOAN_TYPES = [\"normal\",\"topup\",\"emergency\",\"business\",\"group\"]\n",
        "STATUSES = [\"active\",\"pending_branch_approval\"]\n",
        "\n",
        "def infer_age_from_id(national_id: str) -> tuple[int,str]:\n",
        "    \"\"\"Infer approximate age from Kenyan ID ranges.\"\"\"\n",
        "    try:\n",
        "        nid = int(national_id)\n",
        "    except:\n",
        "        return (random.randint(18,65),\"unknown\")\n",
        "    if nid < 7000000:\n",
        "        return (65,\"65+ legacy\")\n",
        "    if 31000000 <= nid <= 33500000:\n",
        "        return (random.randint(25,32),\"25-32\")\n",
        "    if 33500001 <= nid <= 36000000:\n",
        "        return (random.randint(18,25),\"18-25\")\n",
        "    return (random.randint(33,60),\"33-60\")\n",
        "\n",
        "def guess_gender_from_name(name: str) -> str:\n",
        "    common_female = [\"Achieng\",\"Wambui\",\"Njeri\",\"Atieno\",\"Chebet\",\"Wanjiku\",\"Naliaka\"]\n",
        "    if any(part in name for part in common_female):\n",
        "        return \"F\"\n",
        "    if name.lower().endswith(\"a\"):\n",
        "        return \"F\"\n",
        "    return \"M\"\n",
        "\n",
        "def generate_clients_loans(n_rows:int=1000, seed:int|None=None) -> pd.DataFrame:\n",
        "    if seed is not None:\n",
        "        random.seed(seed); np.random.seed(seed); Faker.seed(seed)\n",
        "    rows = []\n",
        "    for i in range(n_rows):\n",
        "        name = fake.name()\n",
        "        national_id = str(random.choice([random.randint(31000000,36000000), random.randint(7000000,36000000)]))\n",
        "        age, age_band = infer_age_from_id(national_id)\n",
        "        gender = guess_gender_from_name(name)\n",
        "        branch, region = random.choice(TOWNS)\n",
        "        product, weekly_term = random.choice(PRODUCTS)\n",
        "        amount = random.randint(5000, 50000)\n",
        "        ref_number = f\"REF{random.randint(100000,999999)}\"\n",
        "        loan_type = random.choice(LOAN_TYPES)\n",
        "        status = random.choices(STATUSES, weights=[0.85,0.15])[0]\n",
        "        created_date = fake.date_between(start_date=\"-24m\", end_date=\"today\")\n",
        "\n",
        "        # derived features\n",
        "        income = random.randint(5000,100000)\n",
        "        dti = round(amount / max(income,1),2)\n",
        "        risk_score = max(300, min(850, int(850 - dti*400 + random.gauss(0,30))))\n",
        "        default_prob = round(1/(1+np.exp(-(0.05*dti*100 + random.gauss(0,1)))),3)\n",
        "        loan_health = \"performing\" if default_prob < 0.3 else \"at_risk\" if default_prob < 0.6 else \"non_performing\"\n",
        "        business_type = random.choice([\"retail\",\"farming\",\"transport\",\"service\",\"manufacturing\",\"informal\"])\n",
        "        phone = f\"+2547{random.randint(0,9)}{random.randint(1000000,9999999)}\"\n",
        "\n",
        "        rows.append({\n",
        "            \"customer_name\": name,\n",
        "            \"national_id\": national_id,\n",
        "            \"age\": age,\n",
        "            \"age_band\": age_band,\n",
        "            \"gender\": gender,\n",
        "            \"branch\": branch,\n",
        "            \"region\": region,\n",
        "            \"product\": product,\n",
        "            \"weekly_term\": weekly_term,\n",
        "            \"amount\": amount,\n",
        "            \"ref_number\": ref_number,\n",
        "            \"loan_type\": loan_type,\n",
        "            \"status\": status,\n",
        "            \"loan_health\": loan_health,\n",
        "            \"created_date\": created_date,\n",
        "            \"income\": income,\n",
        "            \"dti\": dti,\n",
        "            \"risk_score\": risk_score,\n",
        "            \"default_prob\": default_prob,\n",
        "            \"business_type\": business_type,\n",
        "            \"phone\": phone\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def generate_batches(total_rows=50000, batch_size=10000, out_dir=\"data/synth_batches\", fmt=\"csv\", seed=None):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    n_batches = (total_rows + batch_size - 1)//batch_size\n",
        "    for i in range(n_batches):\n",
        "        df = generate_clients_loans(n_rows=batch_size, seed=(None if seed is None else seed+i))\n",
        "        out_path = os.path.join(out_dir, f\"batch_{i+1}.{fmt}\")\n",
        "        if fmt==\"csv\":\n",
        "            df.to_csv(out_path, index=False)\n",
        "        else:\n",
        "            df.to_parquet(out_path, index=False)\n",
        "    return out_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLQI3UsOXRI1",
        "outputId": "4645c4ab-6bea-430f-c63e-6bd2d1c7b702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/synth/generators.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_02_client_onboarding.py\n",
        "try:\n",
        "    import streamlit as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "import pandas as pd\n",
        "from modules.synth import generators as g\n",
        "\n",
        "def build_dataset(n_rows:int=1000, seed:int|None=None) -> pd.DataFrame:\n",
        "    \"\"\"Helper for generating Kenya-specific synthetic loan data.\"\"\"\n",
        "    return g.generate_clients_loans(n_rows=n_rows, seed=seed)\n",
        "\n",
        "def app():\n",
        "    if st is None:\n",
        "        return  # No UI outside Streamlit\n",
        "\n",
        "    st.header(\"🧩 Client Onboarding — Synthetic Data\")\n",
        "    st.write(\"Generate Kenya-specific synthetic loan data tailored to your needs.\")\n",
        "\n",
        "    with st.form(key=\"onboarding_form\"):\n",
        "        n_rows = st.number_input(\"Number of clients (rows)\", min_value=50, max_value=200000, value=1000, step=50)\n",
        "        seed = st.number_input(\"Random seed (optional)\", min_value=0, max_value=10_000, value=42, step=1)\n",
        "        submitted = st.form_submit_button(\"Generate dataset\")\n",
        "\n",
        "    if submitted:\n",
        "        with st.spinner(\"Generating dataset...\"):\n",
        "            df = build_dataset(n_rows=int(n_rows), seed=int(seed))\n",
        "        st.success(f\"Generated {len(df):,} rows\")\n",
        "        st.dataframe(df.head(100))\n",
        "\n",
        "        # Save a local CSV copy\n",
        "        try:\n",
        "            out_path = \"data/onboarding_last.csv\"\n",
        "            df.to_csv(out_path, index=False)\n",
        "            st.caption(f\"Saved a copy at {out_path}\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Could not save CSV: {e}\")\n",
        "\n",
        "        # Provide download\n",
        "        csv_bytes = df.to_csv(index=False).encode(\"utf-8\")\n",
        "        st.download_button(\"⬇️ Download CSV\", csv_bytes, file_name=\"loaniq_synthetic.csv\", mime=\"text/csv\")"
      ],
      "metadata": {
        "id": "sU_6oySzXpYk",
        "outputId": "5f8b776d-6d45-4e63-e1bc-2f2ce5d22411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/_02_client_onboarding.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"def generate_clients_loans\" modules/synth/generators.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXJbhZF8YDui",
        "outputId": "4f7d87f3-4ce4-48cd-8b08-245c1ba5fdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59:def generate_clients_loans(n_rows:int=1000, seed:int|None=None) -> pd.DataFrame:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker streamlit pyngrok pandas pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MauwkqNJYWxz",
        "outputId": "d7460a62-ee83-49fd-f042-171d460b73c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.6.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.49.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Downloading faker-37.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/synth/generators.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# ✅ fallback to English Faker (but we’ll inject Kenyan names ourselves)\n",
        "fake = Faker(\"en\")\n",
        "\n",
        "# Kenyan-specific first names and surnames\n",
        "KENYAN_MALE_NAMES = [\n",
        "    \"Otieno\",\"Kamau\",\"Mwangi\",\"Mutiso\",\"Obuya\",\"Kiptoo\",\"Cheruiyot\",\"Omondi\",\n",
        "    \"Ochieng\",\"Mutua\",\"Chebet\",\"Kiplagat\",\"Njoroge\",\"Barasa\",\"Were\",\"Omondi\",\n",
        "]\n",
        "KENYAN_FEMALE_NAMES = [\n",
        "    \"Achieng\",\"Wambui\",\"Njeri\",\"Atieno\",\"Chebet\",\"Wanjiku\",\"Naliaka\",\"Nyambura\",\n",
        "    \"Mwende\",\"Cherono\",\"Akoth\",\"Kendi\",\"Atieno\",\"Syombua\",\"Nasimiyu\",\"Chelangat\",\n",
        "]\n",
        "KENYAN_SURNAMES = [\n",
        "    \"Omondi\",\"Otieno\",\"Mutiso\",\"Kamau\",\"Kiptoo\",\"Cheruiyot\",\"Mwangi\",\"Barasa\",\n",
        "    \"Were\",\"Ochieng\",\"Njoroge\",\"Obuya\",\"Chebet\",\"Mutua\",\"Okoth\",\"Kosgey\",\"Kimani\"\n",
        "]\n",
        "\n",
        "# 50 towns in Kenya spread across regions\n",
        "TOWNS = [\n",
        "    (\"Nairobi\",\"Central\"),(\"Mombasa\",\"Coast\"),(\"Kisumu\",\"Western\"),(\"Nakuru\",\"Rift\"),\n",
        "    (\"Eldoret\",\"Rift\"),(\"Thika\",\"Central\"),(\"Machakos\",\"Eastern\"),(\"Nyeri\",\"Central\"),\n",
        "    (\"Meru\",\"Eastern\"),(\"Malindi\",\"Coast\"),(\"Garissa\",\"Eastern\"),(\"Embu\",\"Eastern\"),\n",
        "    (\"Kericho\",\"Rift\"),(\"Narok\",\"Rift\"),(\"Naivasha\",\"Rift\"),(\"Kakamega\",\"Western\"),\n",
        "    (\"Bungoma\",\"Western\"),(\"Kitale\",\"Rift\"),(\"Isiolo\",\"Eastern\"),(\"Wajir\",\"Eastern\"),\n",
        "    (\"Lamu\",\"Coast\"),(\"Kilifi\",\"Coast\"),(\"Kwale\",\"Coast\"),(\"Voi\",\"Coast\"),\n",
        "    (\"Marsabit\",\"Eastern\"),(\"Nanyuki\",\"Central\"),(\"Kiambu\",\"Central\"),(\"Muranga\",\"Central\"),\n",
        "    (\"Kerugoya\",\"Central\"),(\"Chuka\",\"Eastern\"),(\"Loitoktok\",\"Rift\"),(\"Kajiado\",\"Rift\"),\n",
        "    (\"Mwingi\",\"Eastern\"),(\"Hola\",\"Coast\"),(\"Makueni\",\"Eastern\"),(\"Sotik\",\"Rift\"),\n",
        "    (\"Litein\",\"Rift\"),(\"Kabarnet\",\"Rift\"),(\"Baringo\",\"Rift\"),(\"Migori\",\"Western\"),\n",
        "    (\"Homa Bay\",\"Western\"),(\"Siaya\",\"Western\"),(\"Busia\",\"Western\"),(\"Keroka\",\"Western\"),\n",
        "    (\"Oyugis\",\"Western\"),(\"Kitui\",\"Eastern\"),(\"Taveta\",\"Coast\"),(\"Kilgoris\",\"Rift\"),\n",
        "    (\"Kapenguria\",\"Rift\"),(\"Maralal\",\"Rift\")\n",
        "]\n",
        "\n",
        "PRODUCTS = [\n",
        "    (\"Inuka\",\"5 weeks\"),\n",
        "    (\"Kuza\",\"4 weeks\"),\n",
        "    (\"Fadhili\",\"6 weeks\"),\n",
        "    (\"Imarika\",\"8 weeks\"),\n",
        "    (\"Chama Boost\",\"12 weeks\")\n",
        "]\n",
        "\n",
        "LOAN_TYPES = [\"normal\",\"topup\",\"emergency\",\"business\",\"group\"]\n",
        "STATUSES = [\"active\",\"pending_branch_approval\"]\n",
        "\n",
        "def random_kenyan_name() -> tuple[str,str]:\n",
        "    \"\"\"Return (full_name, gender).\"\"\"\n",
        "    if random.random() < 0.5:\n",
        "        first = random.choice(KENYAN_MALE_NAMES)\n",
        "        gender = \"M\"\n",
        "    else:\n",
        "        first = random.choice(KENYAN_FEMALE_NAMES)\n",
        "        gender = \"F\"\n",
        "    surname = random.choice(KENYAN_SURNAMES)\n",
        "    return f\"{first} {surname}\", gender\n",
        "\n",
        "def infer_age_from_id(national_id: str) -> tuple[int,str]:\n",
        "    try:\n",
        "        nid = int(national_id)\n",
        "    except:\n",
        "        return (random.randint(18,65),\"unknown\")\n",
        "    if nid < 7000000:\n",
        "        return (65,\"65+ legacy\")\n",
        "    if 31000000 <= nid <= 33500000:\n",
        "        return (random.randint(25,32),\"25-32\")\n",
        "    if 33500001 <= nid <= 36000000:\n",
        "        return (random.randint(18,25),\"18-25\")\n",
        "    return (random.randint(33,60),\"33-60\")\n",
        "\n",
        "def generate_clients_loans(n_rows:int=1000, seed:int|None=None) -> pd.DataFrame:\n",
        "    if seed is not None:\n",
        "        random.seed(seed); np.random.seed(seed); Faker.seed(seed)\n",
        "    rows = []\n",
        "    for i in range(n_rows):\n",
        "        name, gender = random_kenyan_name()\n",
        "        national_id = str(random.choice([random.randint(31000000,36000000), random.randint(7000000,36000000)]))\n",
        "        age, age_band = infer_age_from_id(national_id)\n",
        "        branch, region = random.choice(TOWNS)\n",
        "        product, weekly_term = random.choice(PRODUCTS)\n",
        "        amount = random.randint(5000, 50000)\n",
        "        ref_number = f\"REF{random.randint(100000,999999)}\"\n",
        "        loan_type = random.choice(LOAN_TYPES)\n",
        "        status = random.choices(STATUSES, weights=[0.85,0.15])[0]\n",
        "        created_date = fake.date_between(start_date=\"-24m\", end_date=\"today\")\n",
        "\n",
        "        # derived features\n",
        "        income = random.randint(5000,100000)\n",
        "        dti = round(amount / max(income,1),2)\n",
        "        risk_score = max(300, min(850, int(850 - dti*400 + random.gauss(0,30))))\n",
        "        default_prob = round(1/(1+np.exp(-(0.05*dti*100 + random.gauss(0,1)))),3)\n",
        "        loan_health = \"performing\" if default_prob < 0.3 else \"at_risk\" if default_prob < 0.6 else \"non_performing\"\n",
        "        business_type = random.choice([\"retail\",\"farming\",\"transport\",\"service\",\"manufacturing\",\"informal\"])\n",
        "        phone = f\"+2547{random.randint(0,9)}{random.randint(1000000,9999999)}\"\n",
        "\n",
        "        rows.append({\n",
        "            \"customer_name\": name,\n",
        "            \"national_id\": national_id,\n",
        "            \"age\": age,\n",
        "            \"age_band\": age_band,\n",
        "            \"gender\": gender,\n",
        "            \"branch\": branch,\n",
        "            \"region\": region,\n",
        "            \"product\": product,\n",
        "            \"weekly_term\": weekly_term,\n",
        "            \"amount\": amount,\n",
        "            \"ref_number\": ref_number,\n",
        "            \"loan_type\": loan_type,\n",
        "            \"status\": status,\n",
        "            \"loan_health\": loan_health,\n",
        "            \"created_date\": created_date,\n",
        "            \"income\": income,\n",
        "            \"dti\": dti,\n",
        "            \"risk_score\": risk_score,\n",
        "            \"default_prob\": default_prob,\n",
        "            \"business_type\": business_type,\n",
        "            \"phone\": phone\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def generate_batches(total_rows=50000, batch_size=10000, out_dir=\"data/synth_batches\", fmt=\"csv\", seed=None):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    n_batches = (total_rows + batch_size - 1)//batch_size\n",
        "    for i in range(n_batches):\n",
        "        df = generate_clients_loans(n_rows=batch_size, seed=(None if seed is None else seed+i))\n",
        "        out_path = os.path.join(out_dir, f\"batch_{i+1}.{fmt}\")\n",
        "        if fmt==\"csv\":\n",
        "            df.to_csv(out_path, index=False)\n",
        "        else:\n",
        "            df.to_parquet(out_path, index=False)\n",
        "    return out_dir"
      ],
      "metadata": {
        "id": "8Tg6MnV-Y7Qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60319e1-93e8-4a71-fac0-8ad99a3a239b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/synth/generators.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import modules.synth.generators as g\n",
        "importlib.reload(g)\n",
        "\n",
        "df = g.generate_clients_loans(n_rows=10, seed=123)\n",
        "print(df[[\"customer_name\",\"gender\",\"branch\",\"product\",\"amount\"]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi3X43jpZaLK",
        "outputId": "4f6de424-c785-4774-d3ad-eb58a10d3c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     customer_name gender    branch      product  amount\n",
            "0     Mwangi Mutua      M   Makueni  Chama Boost   26770\n",
            "1    Atieno Mutiso      F    Kisumu        Inuka   48721\n",
            "2  Cherono Njoroge      F   Baringo  Chama Boost   42015\n",
            "3    Syombua Obuya      F   Kajiado      Fadhili   16149\n",
            "4     Kendi Omondi      F    Mwingi        Inuka   46864\n",
            "5      Were Otieno      M   Mombasa      Fadhili   28071\n",
            "6      Njeri Mutua      F   Makueni      Imarika    6909\n",
            "7   Syombua Mutiso      F  Homa Bay        Inuka   38993\n",
            "8     Akoth Kimani      F   Muranga        Inuka   13913\n",
            "9    Chebet Kimani      M    Mwingi        Inuka   40747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "try:\n",
        "    import streamlit as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "def app():\n",
        "    if st is None:\n",
        "        return\n",
        "\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "    st.caption(\"Super-admin tools for data, models, experiments, and stress tests.\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"📊 Data Generation\",\n",
        "        \"🤖 Models\",\n",
        "        \"🧪 Experiments\",\n",
        "        \"⚡ Stress Tests\",\n",
        "        \"👤 Admin Tools\",\n",
        "        \"📜 Audit Viewer\"\n",
        "    ])\n",
        "\n",
        "    # --- Tab 1: Data Generation ---\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"Synthetic Data Controls\")\n",
        "        n_rows = st.number_input(\"Number of clients\", 1000, 200000, 10000, step=1000)\n",
        "        batch_size = st.number_input(\"Batch size\", 1000, 50000, 10000, step=1000)\n",
        "        fraud_toggle = st.checkbox(\"Inject fraud patterns\", value=False)\n",
        "        if st.button(\"Generate Batches\"):\n",
        "            st.success(f\"Would generate {n_rows} rows in batches of {batch_size}. Fraud={fraud_toggle}\")\n",
        "\n",
        "    # --- Tab 2: Models ---\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"Model Training & Controls\")\n",
        "        algo = st.selectbox(\"Choose algorithm\", [\"LogReg\",\"XGBoost\",\"SGD\"])\n",
        "        lr = st.slider(\"Learning rate\", 0.001, 0.5, 0.1)\n",
        "        if st.button(\"Train Model\"):\n",
        "            st.info(f\"Training {algo} with LR={lr}... (stub)\")\n",
        "\n",
        "    # --- Tab 3: Experiments ---\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"Model Experiments (A/B Tests)\")\n",
        "        st.write(\"Compare ROC, PR, metrics between models (stub).\")\n",
        "        st.button(\"Run A/B Test\")\n",
        "\n",
        "    # --- Tab 4: Stress Tests ---\n",
        "    with tabs[3]:\n",
        "        st.subheader(\"What-If Shocks & Stress Testing\")\n",
        "        shock = st.selectbox(\"Shock type\", [\"Interest Rate ↑\", \"Unemployment ↑\", \"Repayment ↓\"])\n",
        "        if st.button(\"Simulate Shock\"):\n",
        "            st.warning(f\"Simulating {shock}... (stub)\")\n",
        "\n",
        "    # --- Tab 5: Admin Tools ---\n",
        "    with tabs[4]:\n",
        "        st.subheader(\"Admin Actions\")\n",
        "        client_id = st.text_input(\"Impersonate Client ID\")\n",
        "        if st.button(\"Impersonate\"):\n",
        "            st.info(f\"Impersonating {client_id}... (stub)\")\n",
        "        st.button(\"Emergency Kill-Switch\")\n",
        "\n",
        "    # --- Tab 6: Audit Viewer ---\n",
        "    with tabs[5]:\n",
        "        st.subheader(\"Audit Logs\")\n",
        "        st.write(\"Searchable logs (stub).\")"
      ],
      "metadata": {
        "id": "OUfN60IuZvp3",
        "outputId": "50ca1ffa-b5ee-4b94-c966-7b6bf64155ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "try:\n",
        "    import streamlit as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "from modules.synth import generators as g\n",
        "import os\n",
        "\n",
        "def app():\n",
        "    if st is None:\n",
        "        return\n",
        "\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "    st.caption(\"Super-admin tools for data, models, experiments, and stress tests.\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"📊 Data Generation\",\n",
        "        \"🤖 Models\",\n",
        "        \"🧪 Experiments\",\n",
        "        \"⚡ Stress Tests\",\n",
        "        \"👤 Admin Tools\",\n",
        "        \"📜 Audit Viewer\"\n",
        "    ])\n",
        "\n",
        "    # --- Tab 1: Data Generation ---\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"Synthetic Data Controls\")\n",
        "\n",
        "        n_rows = st.number_input(\"Total number of clients\", 1000, 200000, 20000, step=1000)\n",
        "        batch_size = st.number_input(\"Batch size\", 1000, 50000, 5000, step=1000)\n",
        "        fmt = st.selectbox(\"Output format\", [\"csv\",\"parquet\"])\n",
        "        fraud_toggle = st.checkbox(\"Inject fraud patterns (future)\")\n",
        "\n",
        "        if st.button(\"Generate Batches\"):\n",
        "            with st.spinner(\"Generating synthetic data batches...\"):\n",
        "                out_dir = g.generate_batches(\n",
        "                    total_rows=int(n_rows),\n",
        "                    batch_size=int(batch_size),\n",
        "                    out_dir=\"data/sandbox_batches\",\n",
        "                    fmt=fmt,\n",
        "                    seed=42\n",
        "                )\n",
        "            st.success(f\"✅ Generated {n_rows} rows into {out_dir}\")\n",
        "            files = os.listdir(out_dir)\n",
        "            st.write(\"Files:\", files[:5], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGghaaGBaGye",
        "outputId": "07c88fd1-e139-402b-d175-4ab9eca783e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modules.synth import generators as g\n",
        "\n",
        "out_dir = g.generate_batches(total_rows=1200, batch_size=500, out_dir=\"data/test_batches\", fmt=\"csv\", seed=123)\n",
        "import os\n",
        "print(\"✅ Files created:\", os.listdir(out_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_wtVsD6aO0r",
        "outputId": "962d9d22-e6f3-4c8e-9bfc-0d241eaeaa99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Files created: ['batch_3.csv', 'batch_1.csv', 'batch_2.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/core/utils.py\n",
        "import os, csv, sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "AUDIT_DB = \"data/audit.db\"\n",
        "AUDIT_CSV = \"data/audit_log.csv\"\n",
        "\n",
        "def _init_storage():\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "    # SQLite\n",
        "    conn = sqlite3.connect(AUDIT_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS audit_log (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        timestamp TEXT,\n",
        "        user TEXT,\n",
        "        action TEXT,\n",
        "        params TEXT,\n",
        "        status TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # CSV\n",
        "    if not os.path.exists(AUDIT_CSV):\n",
        "        with open(AUDIT_CSV, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"timestamp\",\"user\",\"action\",\"params\",\"status\"])\n",
        "\n",
        "def audit_log(user:str, action:str, params:str=\"\", status:str=\"OK\"):\n",
        "    \"\"\"Record an audit event (user, action, params, status).\"\"\"\n",
        "    _init_storage()\n",
        "    ts = datetime.utcnow().isoformat()\n",
        "\n",
        "    # SQLite\n",
        "    conn = sqlite3.connect(AUDIT_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"INSERT INTO audit_log (timestamp,user,action,params,status) VALUES (?,?,?,?,?)\",\n",
        "        (ts,user,action,params,status)\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # CSV\n",
        "    with open(AUDIT_CSV, \"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([ts,user,action,params,status])\n",
        "\n",
        "    return {\"timestamp\":ts,\"user\":user,\"action\":action,\"params\":params,\"status\":status}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHShmnyVaZNw",
        "outputId": "f3975b9e-9d66-4704-e1e3-9a53ebaa622f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/core/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "try:\n",
        "    import streamlit as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "from modules.synth import generators as g\n",
        "from modules.core import utils\n",
        "import os\n",
        "\n",
        "def app():\n",
        "    if st is None:\n",
        "        return\n",
        "\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "    st.caption(\"Super-admin tools for data, models, experiments, and stress tests.\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"📊 Data Generation\",\n",
        "        \"🤖 Models\",\n",
        "        \"🧪 Experiments\",\n",
        "        \"⚡ Stress Tests\",\n",
        "        \"👤 Admin Tools\",\n",
        "        \"📜 Audit Viewer\"\n",
        "    ])\n",
        "\n",
        "    # --- Tab 1: Data Generation ---\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"Synthetic Data Controls\")\n",
        "\n",
        "        n_rows = st.number_input(\"Total number of clients\", 1000, 200000, 20000, step=1000)\n",
        "        batch_size = st.number_input(\"Batch size\", 1000, 50000, 5000, step=1000)\n",
        "        fmt = st.selectbox(\"Output format\", [\"csv\",\"parquet\"])\n",
        "        fraud_toggle = st.checkbox(\"Inject fraud patterns (future)\")\n",
        "\n",
        "        if st.button(\"Generate Batches\"):\n",
        "            with st.spinner(\"Generating synthetic data batches...\"):\n",
        "                out_dir = g.generate_batches(\n",
        "                    total_rows=int(n_rows),\n",
        "                    batch_size=int(batch_size),\n",
        "                    out_dir=\"data/sandbox_batches\",\n",
        "                    fmt=fmt,\n",
        "                    seed=42\n",
        "                )\n",
        "            utils.audit_log(user=\"admin\", action=\"Generate Batches\",\n",
        "                            params=f\"rows={n_rows},batch={batch_size},fmt={fmt},fraud={fraud_toggle}\",\n",
        "                            status=\"OK\")\n",
        "            st.success(f\"✅ Generated {n_rows} rows into {out_dir}\")\n",
        "            files = os.listdir(out_dir)\n",
        "            st.write(\"Files:\", files[:5], \"...\")\n",
        "\n",
        "    # --- Tab 2: Models ---\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"Model Training & Controls\")\n",
        "        algo = st.selectbox(\"Choose algorithm\", [\"LogReg\",\"XGBoost\",\"SGD\"])\n",
        "        lr = st.slider(\"Learning rate\", 0.001, 0.5, 0.1)\n",
        "        if st.button(\"Train Model\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Train Model\", params=f\"algo={algo},lr={lr}\", status=\"OK\")\n",
        "            st.info(f\"Training {algo} with LR={lr}... (stub)\")\n",
        "\n",
        "    # --- Tab 3: Experiments ---\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"Model Experiments (A/B Tests)\")\n",
        "        st.write(\"Compare ROC, PR, metrics between models (stub).\")\n",
        "        if st.button(\"Run A/B Test\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Run A/B Test\", status=\"OK\")\n",
        "            st.info(\"Running A/B test... (stub)\")\n",
        "\n",
        "    # --- Tab 4: Stress Tests ---\n",
        "    with tabs[3]:\n",
        "        st.subheader(\"What-If Shocks & Stress Testing\")\n",
        "        shock = st.selectbox(\"Shock type\", [\"Interest Rate ↑\", \"Unemployment ↑\", \"Repayment ↓\"])\n",
        "        if st.button(\"Simulate Shock\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Simulate Shock\", params=shock, status=\"OK\")\n",
        "            st.warning(f\"Simulating {shock}... (stub)\")\n",
        "\n",
        "    # --- Tab 5: Admin Tools ---\n",
        "    with tabs[4]:\n",
        "        st.subheader(\"Admin Actions\")\n",
        "        client_id = st.text_input(\"Impersonate Client ID\")\n",
        "        if st.button(\"Impersonate\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Impersonate\", params=client_id, status=\"OK\")\n",
        "            st.info(f\"Impersonating {client_id}... (stub)\")\n",
        "        if st.button(\"Emergency Kill-Switch\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Kill-Switch\", status=\"OK\")\n",
        "            st.error(\"Kill switch triggered! (stub)\")\n",
        "\n",
        "    # --- Tab 6: Audit Viewer ---\n",
        "    with tabs[5]:\n",
        "        st.subheader(\"Audit Logs\")\n",
        "\n",
        "        import sqlite3\n",
        "        import pandas as pd\n",
        "\n",
        "        conn = sqlite3.connect(\"data/audit.db\")\n",
        "        df = pd.read_sql_query(\"SELECT * FROM audit_log ORDER BY id DESC LIMIT 200\", conn)\n",
        "        conn.close()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            st.info(\"No logs yet.\")\n",
        "        else:\n",
        "            st.dataframe(df)\n",
        "            st.download_button(\n",
        "                \"⬇️ Download logs (CSV)\",\n",
        "                df.to_csv(index=False),\n",
        "                file_name=\"audit_log.csv\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k64OYbnna52Q",
        "outputId": "402f59a0-0f92-4f6c-d363-b1bb2c7665dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modules.core import utils\n",
        "\n",
        "# Simulate events\n",
        "utils.audit_log(\"admin\",\"Generate Batches\",\"rows=1000,batch=200\",status=\"OK\")\n",
        "utils.audit_log(\"admin\",\"Train Model\",\"algo=LogReg,lr=0.1\",status=\"OK\")\n",
        "\n",
        "# Preview logs\n",
        "!head -n 10 data/audit_log.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYMxDvKebAi3",
        "outputId": "b6f4597e-5f8c-4dd6-84a9-c2d9a248ad34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timestamp,user,action,params,status\r\n",
            "2025-08-31T09:18:52.587395,admin,Generate Batches,\"rows=1000,batch=200\",OK\r\n",
            "2025-08-31T09:18:52.597125,admin,Train Model,\"algo=LogReg,lr=0.1\",OK\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/ml/engine.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import shap\n",
        "import joblib, os\n",
        "\n",
        "# --- Model Registry ---\n",
        "MODEL_DIR = \"data/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "def preprocess(df, target_col=\"default\"):\n",
        "    \"\"\"Split, scale, rebalance dataset.\"\"\"\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Rebalance with SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_bal, y_train_bal = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "    return X_train_bal, X_test_scaled, y_train_bal, y_test, scaler\n",
        "\n",
        "def train_model(df, algo=\"LogReg\", target_col=\"default\", **kwargs):\n",
        "    \"\"\"Train a model and return metrics + SHAP explainability.\"\"\"\n",
        "    X_train, X_test, y_train, y_test, scaler = preprocess(df, target_col)\n",
        "\n",
        "    if algo == \"LogReg\":\n",
        "        model = LogisticRegression(max_iter=1000, **kwargs)\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False, **kwargs)\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(loss=\"log_loss\", max_iter=1000, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown algorithm\")\n",
        "\n",
        "    # Fit\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:,1] if hasattr(model,\"predict_proba\") else None\n",
        "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
        "    pr, rc, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    metrics = {\n",
        "        \"algo\": algo,\n",
        "        \"auc\": auc,\n",
        "        \"precision\": pr,\n",
        "        \"recall\": rc,\n",
        "        \"f1\": f1,\n",
        "        \"confusion_matrix\": cm.tolist()\n",
        "    }\n",
        "\n",
        "    # SHAP values\n",
        "    try:\n",
        "        explainer = shap.Explainer(model, X_train)\n",
        "        shap_values = explainer(X_test[:50])  # sample\n",
        "    except Exception as e:\n",
        "        shap_values = None\n",
        "\n",
        "    # Save model\n",
        "    fname = os.path.join(MODEL_DIR, f\"{algo}_model.pkl\")\n",
        "    joblib.dump({\"model\":model,\"scaler\":scaler}, fname)\n",
        "\n",
        "    return metrics, shap_values, fname\n",
        "\n",
        "def blend_models(df, algos=[\"LogReg\",\"XGBoost\",\"SGD\"], target_col=\"default\"):\n",
        "    \"\"\"Blend multiple models and return weighted hybrid predictions.\"\"\"\n",
        "    preds = []\n",
        "    weights = []\n",
        "    X_train, X_test, y_train, y_test, scaler = preprocess(df, target_col)\n",
        "\n",
        "    for algo in algos:\n",
        "        m, _, path = train_model(df, algo, target_col)\n",
        "        model_bundle = joblib.load(path)\n",
        "        model = model_bundle[\"model\"]\n",
        "        y_proba = model.predict_proba(X_test)[:,1] if hasattr(model,\"predict_proba\") else None\n",
        "        if y_proba is not None:\n",
        "            auc = m[\"auc\"] or 0.5\n",
        "            preds.append(y_proba)\n",
        "            weights.append(auc)\n",
        "\n",
        "    # Weighted average\n",
        "    weights = np.array(weights)/sum(weights)\n",
        "    blended = np.average(preds, axis=0, weights=weights)\n",
        "\n",
        "    auc = roc_auc_score(y_test, blended)\n",
        "    return {\"algo\":\"HybridBlend\",\"auc\":auc,\"weights\":weights.tolist()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZFVIs_lbn0L",
        "outputId": "08d20189-e17f-4788-8205-cb56cedf61e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/ml/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "try:\n",
        "    import streamlit as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "from modules.synth import generators as g\n",
        "from modules.core import utils\n",
        "import os\n",
        "\n",
        "def app():\n",
        "    if st is None:\n",
        "        return\n",
        "\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "    st.caption(\"Super-admin tools for data, models, experiments, and stress tests.\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"📊 Data Generation\",\n",
        "        \"🤖 Models\",\n",
        "        \"🧪 Experiments\",\n",
        "        \"⚡ Stress Tests\",\n",
        "        \"👤 Admin Tools\",\n",
        "        \"📜 Audit Viewer\"\n",
        "    ])\n",
        "\n",
        "    # --- Tab 1: Data Generation ---\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"Synthetic Data Controls\")\n",
        "\n",
        "        n_rows = st.number_input(\"Total number of clients\", 1000, 200000, 20000, step=1000)\n",
        "        batch_size = st.number_input(\"Batch size\", 1000, 50000, 5000, step=1000)\n",
        "        fmt = st.selectbox(\"Output format\", [\"csv\",\"parquet\"])\n",
        "        fraud_toggle = st.checkbox(\"Inject fraud patterns (future)\")\n",
        "\n",
        "        if st.button(\"Generate Batches\"):\n",
        "            with st.spinner(\"Generating synthetic data batches...\"):\n",
        "                out_dir = g.generate_batches(\n",
        "                    total_rows=int(n_rows),\n",
        "                    batch_size=int(batch_size),\n",
        "                    out_dir=\"data/sandbox_batches\",\n",
        "                    fmt=fmt,\n",
        "                    seed=42\n",
        "                )\n",
        "            utils.audit_log(user=\"admin\", action=\"Generate Batches\",\n",
        "                            params=f\"rows={n_rows},batch={batch_size},fmt={fmt},fraud={fraud_toggle}\",\n",
        "                            status=\"OK\")\n",
        "            st.success(f\"✅ Generated {n_rows} rows into {out_dir}\")\n",
        "            files = os.listdir(out_dir)\n",
        "            st.write(\"Files:\", files[:5], \"...\")\n",
        "\n",
        "    # --- Tab 2: Models ---\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"Model Training & Controls\")\n",
        "\n",
        "        algo = st.selectbox(\"Choose algorithm\", [\"LogReg\",\"XGBoost\",\"SGD\",\"HybridBlend\"])\n",
        "        lr = st.slider(\"Learning rate (for SGD/XGB)\", 0.001, 0.5, 0.1)\n",
        "\n",
        "        if st.button(\"Train Model\"):\n",
        "            from modules.ml import engine\n",
        "            import pandas as pd\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            # Load a dataset (from batches generated in Tab 1)\n",
        "            sample_file = \"data/sandbox_batches/batch_000.csv\"\n",
        "            if not os.path.exists(sample_file):\n",
        "                st.error(\"No synthetic data found. Please generate data in Tab 1 first.\")\n",
        "            else:\n",
        "                df = pd.read_csv(sample_file)\n",
        "                if \"default\" not in df.columns:\n",
        "                    # Add fake target column for demo\n",
        "                    df[\"default\"] = (df[\"loan_health\"] != \"performing\").astype(int)\n",
        "\n",
        "                if algo == \"HybridBlend\":\n",
        "                    metrics = engine.blend_models(df)\n",
        "                    shap_values = None\n",
        "                else:\n",
        "                    metrics, shap_values, path = engine.train_model(df, algo, lr=lr)\n",
        "\n",
        "                utils.audit_log(user=\"admin\", action=\"Train Model\", params=algo, status=\"OK\")\n",
        "\n",
        "                st.json(metrics)\n",
        "                if shap_values is not None:\n",
        "                    st.write(\"Feature importance (SHAP):\")\n",
        "                    fig = plt.figure()\n",
        "                    shap.plots.bar(shap_values, show=False)\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "    # --- Tab 3: Experiments ---\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"Model Experiments (A/B Tests)\")\n",
        "        st.write(\"Compare ROC, PR, metrics between models (stub).\")\n",
        "        if st.button(\"Run A/B Test\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Run A/B Test\", status=\"OK\")\n",
        "            st.info(\"Running A/B test... (stub)\")\n",
        "\n",
        "    # --- Tab 4: Stress Tests ---\n",
        "    with tabs[3]:\n",
        "        st.subheader(\"What-If Shocks & Stress Testing\")\n",
        "        shock = st.selectbox(\"Shock type\", [\"Interest Rate ↑\", \"Unemployment ↑\", \"Repayment ↓\"])\n",
        "        if st.button(\"Simulate Shock\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Simulate Shock\", params=shock, status=\"OK\")\n",
        "            st.warning(f\"Simulating {shock}... (stub)\")\n",
        "\n",
        "    # --- Tab 5: Admin Tools ---\n",
        "    with tabs[4]:\n",
        "        st.subheader(\"Admin Actions\")\n",
        "        client_id = st.text_input(\"Impersonate Client ID\")\n",
        "        if st.button(\"Impersonate\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Impersonate\", params=client_id, status=\"OK\")\n",
        "            st.info(f\"Impersonating {client_id}... (stub)\")\n",
        "        if st.button(\"Emergency Kill-Switch\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Kill-Switch\", status=\"OK\")\n",
        "            st.error(\"Kill switch triggered! (stub)\")\n",
        "\n",
        "    # --- Tab 6: Audit Viewer ---\n",
        "    with tabs[5]:\n",
        "        st.subheader(\"Audit Logs\")\n",
        "\n",
        "        import sqlite3\n",
        "        import pandas as pd\n",
        "\n",
        "        conn = sqlite3.connect(\"data/audit.db\")\n",
        "        df = pd.read_sql_query(\"SELECT * FROM audit_log ORDER BY id DESC LIMIT 200\", conn)\n",
        "        conn.close()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            st.info(\"No logs yet.\")\n",
        "        else:\n",
        "            st.dataframe(df)\n",
        "            st.download_button(\n",
        "                \"⬇️ Download logs (CSV)\",\n",
        "                df.to_csv(index=False),\n",
        "                file_name=\"audit_log.csv\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNbh4VDOcKJu",
        "outputId": "b1ffce49-af11-460d-8d6a-b34aa0249710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modules.synth import generators as g\n",
        "import os\n",
        "\n",
        "# Generate 1000 rows, split into 500-row batches\n",
        "out_dir = g.generate_batches(\n",
        "    total_rows=1000,\n",
        "    batch_size=500,\n",
        "    out_dir=\"data/sandbox_batches\",\n",
        "    fmt=\"csv\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"✅ Batches saved to:\", out_dir)\n",
        "print(\"Files in folder:\", os.listdir(out_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gzLVjt7chY9",
        "outputId": "0cd17e3f-0653-4d8b-c0f7-fa42eb4dde5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batches saved to: data/sandbox_batches\n",
            "Files in folder: ['batch_1.csv', 'batch_2.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"data/ exists?\", os.path.exists(\"data\"))\n",
        "print(\"sandbox_batches exists?\", os.path.exists(\"data/sandbox_batches\"))\n",
        "if os.path.exists(\"data/sandbox_batches\"):\n",
        "    print(\"Files:\", os.listdir(\"data/sandbox_batches\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMvxJbzEcxs1",
        "outputId": "9c9c25c0-97fd-4bfa-b0f0-e3a5630f5e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/ exists? True\n",
            "sandbox_batches exists? True\n",
            "Files: ['batch_1.csv', 'batch_2.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modules.synth import generators as g\n",
        "\n",
        "out_dir = g.generate_batches(\n",
        "    total_rows=1000,      # 1k rows\n",
        "    batch_size=500,       # 2 batches\n",
        "    out_dir=\"data/sandbox_batches\",\n",
        "    fmt=\"csv\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"✅ Generated batches:\", out_dir)\n",
        "print(\"Files now:\", os.listdir(out_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCJpSwBTc7Md",
        "outputId": "d28f76df-7348-4f9a-8c72-fa81ecb53fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generated batches: data/sandbox_batches\n",
            "Files now: ['batch_1.csv', 'batch_2.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/synth/generators.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random, os\n",
        "\n",
        "fake = Faker(\"en\")\n",
        "\n",
        "# Kenyan towns, products etc. (trimmed for brevity here)\n",
        "KENYAN_TOWNS = [\"Nairobi\",\"Mombasa\",\"Kisumu\",\"Nakuru\",\"Eldoret\",\"Meru\",\"Nyeri\",\"Machakos\",\"Embu\",\"Kitale\"]\n",
        "PRODUCTS = [\"Inuka 5 weeks\",\"Kuza 4 weeks\",\"Fadhili 6 weeks\",\"Jipange 8 weeks\"]\n",
        "LOAN_TYPES = [\"normal\",\"group\",\"business\"]\n",
        "STATUSES = [\"active\",\"pending approval\"]\n",
        "HEALTH = [\"performing\",\"watch\",\"non-performing\"]\n",
        "\n",
        "def generate_clients_loans(n_rows:int=1000, seed:int|None=None) -> pd.DataFrame:\n",
        "    if seed: random.seed(seed); np.random.seed(seed)\n",
        "    data = []\n",
        "    for _ in range(n_rows):\n",
        "        name = fake.name()\n",
        "        natid = random.randint(32000000, 34000000)  # simulate Kenyan IDs\n",
        "        age = (natid//100000 - 320) + 20           # crude age approximation\n",
        "        branch = random.choice(KENYAN_TOWNS)\n",
        "        product = random.choice(PRODUCTS)\n",
        "        amount = random.randint(1000, 50000)\n",
        "        loan_type = random.choice(LOAN_TYPES)\n",
        "        status = random.choice(STATUSES)\n",
        "        health = random.choice(HEALTH)\n",
        "        data.append([name, natid, age, branch, product, amount, loan_type, status, health])\n",
        "    return pd.DataFrame(data, columns=[\"customer_name\",\"national_id\",\"age\",\"branch\",\"product\",\n",
        "                                       \"amount\",\"loan_type\",\"status\",\"loan_health\"])\n",
        "\n",
        "def generate_batches(total_rows:int=10000, batch_size:int=2000, out_dir=\"data/sandbox_batches\",\n",
        "                     fmt=\"csv\", seed:int|None=None):\n",
        "    \"\"\"Generate multiple batches with consistent naming (batch_000.csv, batch_001.csv, …).\"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    n_batches = int(np.ceil(total_rows/batch_size))\n",
        "    for i in range(n_batches):\n",
        "        df = generate_clients_loans(min(batch_size,total_rows-i*batch_size), seed=seed)\n",
        "        fname = os.path.join(out_dir, f\"batch_{i:03d}.{fmt}\")\n",
        "        if fmt==\"csv\":\n",
        "            df.to_csv(fname, index=False)\n",
        "        else:\n",
        "            df.to_parquet(fname, index=False)\n",
        "    return out_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h-Sl_0edQN6",
        "outputId": "520e8461-0981-4808-a59f-6656f9f6db6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/synth/generators.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import modules.synth.generators as g\n",
        "\n",
        "importlib.reload(g)\n",
        "\n",
        "# Now regenerate batches\n",
        "out_dir = g.generate_batches(1000, 500, \"data/sandbox_batches\", \"csv\", seed=42)\n",
        "print(\"Files:\", os.listdir(out_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6zYC8Fsdk6a",
        "outputId": "d8cb64a3-6249-4505-f49f-e837ae7a4d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files: ['batch_1.csv', 'batch_2.csv', 'batch_001.csv', 'batch_000.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "try:\n",
        "    import streamlit as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "from modules.synth import generators as g\n",
        "from modules.core import utils\n",
        "import os, glob\n",
        "\n",
        "def app():\n",
        "    if st is None:\n",
        "        return\n",
        "\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "    st.caption(\"Super-admin tools for data, models, experiments, and stress tests.\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"📊 Data Generation\",\n",
        "        \"🤖 Models\",\n",
        "        \"🧪 Experiments\",\n",
        "        \"⚡ Stress Tests\",\n",
        "        \"👤 Admin Tools\",\n",
        "        \"📜 Audit Viewer\"\n",
        "    ])\n",
        "\n",
        "    # --- Tab 1: Data Generation ---\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"Synthetic Data Controls\")\n",
        "\n",
        "        n_rows = st.number_input(\"Total number of clients\", 1000, 200000, 20000, step=1000)\n",
        "        batch_size = st.number_input(\"Batch size\", 1000, 50000, 5000, step=1000)\n",
        "        fmt = st.selectbox(\"Output format\", [\"csv\",\"parquet\"])\n",
        "        fraud_toggle = st.checkbox(\"Inject fraud patterns (future)\")\n",
        "\n",
        "        if st.button(\"Generate Batches\"):\n",
        "            with st.spinner(\"Generating synthetic data batches...\"):\n",
        "                out_dir = g.generate_batches(\n",
        "                    total_rows=int(n_rows),\n",
        "                    batch_size=int(batch_size),\n",
        "                    out_dir=\"data/sandbox_batches\",\n",
        "                    fmt=fmt,\n",
        "                    seed=42\n",
        "                )\n",
        "            utils.audit_log(user=\"admin\", action=\"Generate Batches\",\n",
        "                            params=f\"rows={n_rows},batch={batch_size},fmt={fmt},fraud={fraud_toggle}\",\n",
        "                            status=\"OK\")\n",
        "            st.success(f\"✅ Generated {n_rows} rows into {out_dir}\")\n",
        "            files = os.listdir(out_dir)\n",
        "            st.write(\"Files:\", files[:5], \"...\")\n",
        "\n",
        "    # --- Tab 2: Models ---\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"Model Training & Controls\")\n",
        "\n",
        "        algo = st.selectbox(\"Choose algorithm\", [\"LogReg\",\"XGBoost\",\"SGD\",\"HybridBlend\"])\n",
        "        lr = st.slider(\"Learning rate (for SGD/XGB)\", 0.001, 0.5, 0.1)\n",
        "\n",
        "        if st.button(\"Train Model\"):\n",
        "            from modules.ml import engine\n",
        "            import pandas as pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            import shap\n",
        "\n",
        "            # ✅ Flexible batch loader (works with batch_1.csv or batch_000.csv)\n",
        "            files = sorted(glob.glob(\"data/sandbox_batches/batch_*.csv\"))\n",
        "            if not files:\n",
        "                st.error(\"No synthetic data found. Please generate data in Tab 1 first.\")\n",
        "            else:\n",
        "                sample_file = files[0]\n",
        "                df = pd.read_csv(sample_file)\n",
        "                if \"default\" not in df.columns:\n",
        "                    df[\"default\"] = (df[\"loan_health\"] != \"performing\").astype(int)\n",
        "\n",
        "                if algo == \"HybridBlend\":\n",
        "                    metrics = engine.blend_models(df)\n",
        "                    shap_values = None\n",
        "                else:\n",
        "                    metrics, shap_values, path = engine.train_model(df, algo, lr=lr)\n",
        "\n",
        "                utils.audit_log(user=\"admin\", action=\"Train Model\", params=algo, status=\"OK\")\n",
        "\n",
        "                st.json(metrics)\n",
        "                if shap_values is not None:\n",
        "                    st.write(\"Feature importance (SHAP):\")\n",
        "                    fig = plt.figure()\n",
        "                    shap.plots.bar(shap_values, show=False)\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "    # --- Tab 3: Experiments ---\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"Model Experiments (A/B Tests)\")\n",
        "        st.write(\"Compare ROC, PR, metrics between models (stub).\")\n",
        "        if st.button(\"Run A/B Test\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Run A/B Test\", status=\"OK\")\n",
        "            st.info(\"Running A/B test... (stub)\")\n",
        "\n",
        "    # --- Tab 4: Stress Tests ---\n",
        "    with tabs[3]:\n",
        "        st.subheader(\"What-If Shocks & Stress Testing\")\n",
        "        shock = st.selectbox(\"Shock type\", [\"Interest Rate ↑\", \"Unemployment ↑\", \"Repayment ↓\"])\n",
        "        if st.button(\"Simulate Shock\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Simulate Shock\", params=shock, status=\"OK\")\n",
        "            st.warning(f\"Simulating {shock}... (stub)\")\n",
        "\n",
        "    # --- Tab 5: Admin Tools ---\n",
        "    with tabs[4]:\n",
        "        st.subheader(\"Admin Actions\")\n",
        "        client_id = st.text_input(\"Impersonate Client ID\")\n",
        "        if st.button(\"Impersonate\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Impersonate\", params=client_id, status=\"OK\")\n",
        "            st.info(f\"Impersonating {client_id}... (stub)\")\n",
        "        if st.button(\"Emergency Kill-Switch\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Kill-Switch\", status=\"OK\")\n",
        "            st.error(\"Kill switch triggered! (stub)\")\n",
        "\n",
        "    # --- Tab 6: Audit Viewer ---\n",
        "    with tabs[5]:\n",
        "        st.subheader(\"Audit Logs\")\n",
        "\n",
        "        import sqlite3\n",
        "        import pandas as pd\n",
        "\n",
        "        conn = sqlite3.connect(\"data/audit.db\")\n",
        "        df = pd.read_sql_query(\"SELECT * FROM audit_log ORDER BY id DESC LIMIT 200\", conn)\n",
        "        conn.close()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            st.info(\"No logs yet.\")\n",
        "        else:\n",
        "            st.dataframe(df)\n",
        "            st.download_button(\n",
        "                \"⬇️ Download logs (CSV)\",\n",
        "                df.to_csv(index=False),\n",
        "                file_name=\"audit_log.csv\"\n",
        "            )"
      ],
      "metadata": {
        "id": "E6f0ysMdd_F3",
        "outputId": "d1171a1d-4727-4adb-8983-8149ae47f263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile pages/_04_admin_sandbox.py\n",
        "try:\n",
        "    import streamlit as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "from modules.synth import generators as g\n",
        "from modules.core import utils\n",
        "import os, glob\n",
        "\n",
        "def app():\n",
        "    if st is None:\n",
        "        return\n",
        "\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "    st.caption(\"Super-admin tools for data, models, experiments, and stress tests.\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"📊 Data Generation\",\n",
        "        \"🤖 Models\",\n",
        "        \"🧪 Experiments\",\n",
        "        \"⚡ Stress Tests\",\n",
        "        \"👤 Admin Tools\",\n",
        "        \"📜 Audit Viewer\"\n",
        "    ])\n",
        "\n",
        "    # --- Tab 1: Data Generation ---\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"Synthetic Data Controls\")\n",
        "\n",
        "        n_rows = st.number_input(\"Total number of clients\", 1000, 200000, 20000, step=1000)\n",
        "        batch_size = st.number_input(\"Batch size\", 1000, 50000, 5000, step=1000)\n",
        "        fmt = st.selectbox(\"Output format\", [\"csv\",\"parquet\"])\n",
        "        fraud_toggle = st.checkbox(\"Inject fraud patterns (future)\")\n",
        "\n",
        "        if st.button(\"Generate Batches\"):\n",
        "            with st.spinner(\"Generating synthetic data batches...\"):\n",
        "                out_dir = g.generate_batches(\n",
        "                    total_rows=int(n_rows),\n",
        "                    batch_size=int(batch_size),\n",
        "                    out_dir=\"data/sandbox_batches\",\n",
        "                    fmt=fmt,\n",
        "                    seed=42\n",
        "                )\n",
        "            utils.audit_log(user=\"admin\", action=\"Generate Batches\",\n",
        "                            params=f\"rows={n_rows},batch={batch_size},fmt={fmt},fraud={fraud_toggle}\",\n",
        "                            status=\"OK\")\n",
        "            st.success(f\"✅ Generated {n_rows} rows into {out_dir}\")\n",
        "            files = os.listdir(out_dir)\n",
        "            st.write(\"Files:\", files[:5], \"...\")\n",
        "\n",
        "    # --- Tab 2: Models ---\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"Model Training & Controls\")\n",
        "\n",
        "        algo = st.selectbox(\"Choose algorithm\", [\"LogReg\",\"XGBoost\",\"SGD\",\"HybridBlend\"])\n",
        "        lr = st.slider(\"Learning rate (for SGD/XGB)\", 0.001, 0.5, 0.1)\n",
        "\n",
        "        if st.button(\"Train Model\"):\n",
        "            from modules.ml import engine\n",
        "            import pandas as pd\n",
        "            import matplotlib.pyplot as plt\n",
        "            import shap\n",
        "\n",
        "            files = sorted(glob.glob(\"data/sandbox_batches/batch_*.csv\"))\n",
        "            if not files:\n",
        "                st.error(\"No synthetic data found. Please generate data in Tab 1 first.\")\n",
        "            else:\n",
        "                sample_file = files[0]\n",
        "                df = pd.read_csv(sample_file)\n",
        "                if \"default\" not in df.columns:\n",
        "                    df[\"default\"] = (df[\"loan_health\"] != \"performing\").astype(int)\n",
        "\n",
        "                if algo == \"HybridBlend\":\n",
        "                    metrics = engine.blend_models(df)\n",
        "                    shap_values = None\n",
        "                else:\n",
        "                    metrics, shap_values, path = engine.train_model(df, algo, lr=lr)\n",
        "\n",
        "                utils.audit_log(user=\"admin\", action=\"Train Model\", params=algo, status=\"OK\")\n",
        "\n",
        "                st.json(metrics)\n",
        "                if shap_values is not None:\n",
        "                    st.write(\"Feature importance (SHAP):\")\n",
        "                    fig = plt.figure()\n",
        "                    shap.plots.bar(shap_values, show=False)\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "    # --- Tab 3: Experiments ---\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"Model Experiments (A/B Tests)\")\n",
        "        from modules.ml import engine\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "        algo1 = st.selectbox(\"Model A\", [\"LogReg\",\"XGBoost\",\"SGD\"])\n",
        "        algo2 = st.selectbox(\"Model B\", [\"LogReg\",\"XGBoost\",\"SGD\"], index=1)\n",
        "\n",
        "        if st.button(\"Run A/B Test\"):\n",
        "            files = sorted(glob.glob(\"data/sandbox_batches/batch_*.csv\"))\n",
        "            if not files:\n",
        "                st.error(\"No synthetic data found. Please generate data in Tab 1 first.\")\n",
        "            else:\n",
        "                df = pd.read_csv(files[0])\n",
        "                if \"default\" not in df.columns:\n",
        "                    df[\"default\"] = (df[\"loan_health\"] != \"performing\").astype(int)\n",
        "\n",
        "                # Train both models\n",
        "                m1, shap1, _ = engine.train_model(df, algo1)\n",
        "                m2, shap2, _ = engine.train_model(df, algo2)\n",
        "\n",
        "                utils.audit_log(user=\"admin\", action=\"Run A/B Test\", params=f\"{algo1} vs {algo2}\", status=\"OK\")\n",
        "\n",
        "                # Display metrics\n",
        "                st.write(\"### Metrics\")\n",
        "                st.json({algo1: m1, algo2: m2})\n",
        "\n",
        "                # ROC Curves\n",
        "                st.write(\"### ROC Curves\")\n",
        "                fpr1, tpr1, _ = roc_curve(df[\"default\"], m1[\"y_scores\"])\n",
        "                fpr2, tpr2, _ = roc_curve(df[\"default\"], m2[\"y_scores\"])\n",
        "                fig, ax = plt.subplots()\n",
        "                ax.plot(fpr1, tpr1, label=f\"{algo1} AUC={auc(fpr1,tpr1):.3f}\")\n",
        "                ax.plot(fpr2, tpr2, label=f\"{algo2} AUC={auc(fpr2,tpr2):.3f}\")\n",
        "                ax.plot([0,1],[0,1],'k--')\n",
        "                ax.legend()\n",
        "                st.pyplot(fig)\n",
        "\n",
        "                # PR Curves\n",
        "                st.write(\"### Precision-Recall Curves\")\n",
        "                p1,r1,_ = precision_recall_curve(df[\"default\"], m1[\"y_scores\"])\n",
        "                p2,r2,_ = precision_recall_curve(df[\"default\"], m2[\"y_scores\"])\n",
        "                fig, ax = plt.subplots()\n",
        "                ax.plot(r1,p1,label=algo1)\n",
        "                ax.plot(r2,p2,label=algo2)\n",
        "                ax.legend()\n",
        "                st.pyplot(fig)\n",
        "\n",
        "                # Confusion Matrices\n",
        "                st.write(\"### Confusion Matrices\")\n",
        "                cm1 = confusion_matrix(df[\"default\"], m1[\"y_pred\"])\n",
        "                cm2 = confusion_matrix(df[\"default\"], m2[\"y_pred\"])\n",
        "                fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "                ConfusionMatrixDisplay(cm1).plot(ax=ax[0], values_format=\"d\")\n",
        "                ax[0].set_title(algo1)\n",
        "                ConfusionMatrixDisplay(cm2).plot(ax=ax[1], values_format=\"d\")\n",
        "                ax[1].set_title(algo2)\n",
        "                st.pyplot(fig)\n",
        "\n",
        "    # --- Tab 4: Stress Tests ---\n",
        "    with tabs[3]:\n",
        "        st.subheader(\"What-If Shocks & Stress Testing\")\n",
        "        shock = st.selectbox(\"Shock type\", [\"Interest Rate ↑\", \"Unemployment ↑\", \"Repayment ↓\"])\n",
        "        if st.button(\"Simulate Shock\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Simulate Shock\", params=shock, status=\"OK\")\n",
        "            st.warning(f\"Simulating {shock}... (stub)\")\n",
        "\n",
        "    # --- Tab 5: Admin Tools ---\n",
        "    with tabs[4]:\n",
        "        st.subheader(\"Admin Actions\")\n",
        "        client_id = st.text_input(\"Impersonate Client ID\")\n",
        "        if st.button(\"Impersonate\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Impersonate\", params=client_id, status=\"OK\")\n",
        "            st.info(f\"Impersonating {client_id}... (stub)\")\n",
        "        if st.button(\"Emergency Kill-Switch\"):\n",
        "            utils.audit_log(user=\"admin\", action=\"Kill-Switch\", status=\"OK\")\n",
        "            st.error(\"Kill switch triggered! (stub)\")\n",
        "\n",
        "    # --- Tab 6: Audit Viewer ---\n",
        "    with tabs[5]:\n",
        "        st.subheader(\"Audit Logs\")\n",
        "\n",
        "        import sqlite3\n",
        "        import pandas as pd\n",
        "\n",
        "        conn = sqlite3.connect(\"data/audit.db\")\n",
        "        df = pd.read_sql_query(\"SELECT * FROM audit_log ORDER BY id DESC LIMIT 200\", conn)\n",
        "        conn.close()\n",
        "\n",
        "        if len(df) == 0:\n",
        "            st.info(\"No logs yet.\")\n",
        "        else:\n",
        "            st.dataframe(df)\n",
        "            st.download_button(\n",
        "                \"⬇️ Download logs (CSV)\",\n",
        "                df.to_csv(index=False),\n",
        "                file_name=\"audit_log.csv\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBdXLrkGew-9",
        "outputId": "33098529-cb58-4129-ea44-bccd0d1292e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/ml/engine.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "import shap, joblib, os\n",
        "\n",
        "def train_model(df, algo=\"LogReg\", lr=0.1, test_size=0.3, seed=42):\n",
        "    \"\"\"Train a single model and return metrics, SHAP values (if any), and saved path.\"\"\"\n",
        "\n",
        "    # Features & target\n",
        "    y = df[\"default\"]\n",
        "    X = df.drop(columns=[\"default\",\"loan_health\"], errors=\"ignore\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "\n",
        "    # Pick model\n",
        "    if algo == \"LogReg\":\n",
        "        model = LogisticRegression(max_iter=500)\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(loss=\"log_loss\", learning_rate=\"optimal\", eta0=lr, max_iter=1000)\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = XGBClassifier(\n",
        "            use_label_encoder=False, eval_metric=\"logloss\", learning_rate=lr, n_estimators=200\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported algo {algo}\")\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    try:\n",
        "        y_scores = model.predict_proba(X_test)[:,1]\n",
        "    except:\n",
        "        # fallback if predict_proba not available\n",
        "        y_scores = model.decision_function(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        \"algo\": algo,\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"auc\": roc_auc_score(y_test, y_scores),\n",
        "        \"n_train\": len(y_train),\n",
        "        \"n_test\": len(y_test),\n",
        "        \"y_pred\": y_pred.tolist(),\n",
        "        \"y_scores\": y_scores.tolist()\n",
        "    }\n",
        "\n",
        "    # Explainability (SHAP)\n",
        "    shap_values = None\n",
        "    try:\n",
        "        explainer = shap.Explainer(model, X_test)\n",
        "        shap_values = explainer(X_test)\n",
        "    except Exception as e:\n",
        "        pass  # SHAP may fail for some models\n",
        "\n",
        "    # Save model\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    path = f\"models/{algo}_model.pkl\"\n",
        "    joblib.dump(model, path)\n",
        "\n",
        "    return metrics, shap_values, path\n",
        "\n",
        "def blend_models(df, seed=42):\n",
        "    \"\"\"Simple hybrid blend: average predictions from LogReg + XGB + SGD.\"\"\"\n",
        "    y = df[\"default\"]\n",
        "    X = df.drop(columns=[\"default\",\"loan_health\"], errors=\"ignore\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=seed, stratify=y\n",
        "    )\n",
        "\n",
        "    models = {\n",
        "        \"LogReg\": LogisticRegression(max_iter=500),\n",
        "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
        "        \"SGD\": SGDClassifier(loss=\"log_loss\", max_iter=1000)\n",
        "    }\n",
        "\n",
        "    preds = []\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        try:\n",
        "            p = model.predict_proba(X_test)[:,1]\n",
        "        except:\n",
        "            p = model.decision_function(X_test)\n",
        "        preds.append(p)\n",
        "\n",
        "    y_scores = np.mean(preds, axis=0)\n",
        "    y_pred = (y_scores >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"algo\": \"HybridBlend\",\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"auc\": roc_auc_score(y_test, y_scores),\n",
        "        \"n_test\": len(y_test),\n",
        "        \"y_pred\": y_pred.tolist(),\n",
        "        \"y_scores\": y_scores.tolist()\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe5D4s2xfGa-",
        "outputId": "56398270-86a0-41b9-fb31-cf53e337a800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/ml/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/ml/engine.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "import shap, joblib, os\n",
        "\n",
        "def train_model(df, algo=\"LogReg\", lr=0.1, test_size=0.3, seed=42):\n",
        "    \"\"\"Train a single model with encoding for categorical features.\"\"\"\n",
        "    y = df[\"default\"]\n",
        "    X = df.drop(columns=[\"default\",\"loan_health\"], errors=\"ignore\")\n",
        "\n",
        "    # Encode categoricals\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "\n",
        "    if algo == \"LogReg\":\n",
        "        model = LogisticRegression(max_iter=500)\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(loss=\"log_loss\", learning_rate=\"optimal\", eta0=lr, max_iter=1000)\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = XGBClassifier(\n",
        "            use_label_encoder=False, eval_metric=\"logloss\", learning_rate=lr, n_estimators=200\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported algo {algo}\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    try:\n",
        "        y_scores = model.predict_proba(X_test)[:,1]\n",
        "    except:\n",
        "        y_scores = model.decision_function(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        \"algo\": algo,\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"auc\": roc_auc_score(y_test, y_scores),\n",
        "        \"n_train\": len(y_train),\n",
        "        \"n_test\": len(y_test),\n",
        "        \"y_pred\": y_pred.tolist(),\n",
        "        \"y_scores\": y_scores.tolist()\n",
        "    }\n",
        "\n",
        "    shap_values = None\n",
        "    try:\n",
        "        explainer = shap.Explainer(model, X_test)\n",
        "        shap_values = explainer(X_test)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    path = f\"models/{algo}_model.pkl\"\n",
        "    joblib.dump(model, path)\n",
        "\n",
        "    return metrics, shap_values, path\n",
        "\n",
        "def blend_models(df, seed=42):\n",
        "    \"\"\"Simple hybrid blend: average predictions from LogReg + XGB + SGD.\"\"\"\n",
        "    y = df[\"default\"]\n",
        "    X = df.drop(columns=[\"default\",\"loan_health\"], errors=\"ignore\")\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=seed, stratify=y\n",
        "    )\n",
        "\n",
        "    models = {\n",
        "        \"LogReg\": LogisticRegression(max_iter=500),\n",
        "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
        "        \"SGD\": SGDClassifier(loss=\"log_loss\", max_iter=1000)\n",
        "    }\n",
        "\n",
        "    preds = []\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        try:\n",
        "            p = model.predict_proba(X_test)[:,1]\n",
        "        except:\n",
        "            p = model.decision_function(X_test)\n",
        "        preds.append(p)\n",
        "\n",
        "    y_scores = np.mean(preds, axis=0)\n",
        "    y_pred = (y_scores >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"algo\": \"HybridBlend\",\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"auc\": roc_auc_score(y_test, y_scores),\n",
        "        \"n_test\": len(y_test),\n",
        "        \"y_pred\": y_pred.tolist(),\n",
        "        \"y_scores\": y_scores.tolist()\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpm0zA-nfm5R",
        "outputId": "c7a160bf-d9b1-42c8-ec2a-dd8119e15a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/ml/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Test ML Engine Patch ===\n",
        "import pandas as pd\n",
        "from modules.synth import generators as g\n",
        "from modules.ml import engine\n",
        "import importlib\n",
        "\n",
        "# reload engine to pick up changes\n",
        "importlib.reload(engine)\n",
        "\n",
        "# Generate a small synthetic dataset\n",
        "df = g.generate_clients_loans(n_rows=500, seed=123)\n",
        "df[\"default\"] = (df[\"loan_health\"] != \"performing\").astype(int)\n",
        "\n",
        "# Train three models\n",
        "for algo in [\"LogReg\", \"SGD\", \"XGBoost\"]:\n",
        "    metrics, shap_values, path = engine.train_model(df, algo, lr=0.1)\n",
        "    print(f\"\\n=== {algo} ===\")\n",
        "    print(\"Saved:\", path)\n",
        "    print(\"Accuracy:\", round(metrics[\"accuracy\"], 3),\n",
        "          \"Precision:\", round(metrics[\"precision\"], 3),\n",
        "          \"Recall:\", round(metrics[\"recall\"], 3),\n",
        "          \"AUC:\", round(metrics[\"auc\"], 3))\n",
        "    print(\"Pred sample:\", metrics[\"y_pred\"][:10])\n",
        "    print(\"Scores sample:\", [round(s,3) for s in metrics[\"y_scores\"][:10]])\n",
        "\n",
        "# Test hybrid\n",
        "metrics = engine.blend_models(df)\n",
        "print(\"\\n=== HybridBlend ===\")\n",
        "print(\"Accuracy:\", round(metrics[\"accuracy\"], 3),\n",
        "      \"Precision:\", round(metrics[\"precision\"], 3),\n",
        "      \"Recall:\", round(metrics[\"recall\"], 3),\n",
        "      \"AUC:\", round(metrics[\"auc\"], 3))\n",
        "print(\"Pred sample:\", metrics[\"y_pred\"][:10])\n",
        "print(\"Scores sample:\", [round(s,3) for s in metrics[\"y_scores\"][:10]])"
      ],
      "metadata": {
        "id": "TYELGh54fvUf",
        "outputId": "6b900271-6fd4-45e3-d0e7-7ca964a1a93c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LogReg ===\n",
            "Saved: models/LogReg_model.pkl\n",
            "Accuracy: 0.667 Precision: 0.667 Recall: 1.0 AUC: 0.551\n",
            "Pred sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Scores sample: [0.709, 0.703, 0.621, 0.699, 0.705, 0.711, 0.719, 0.665, 0.692, 0.705]\n",
            "\n",
            "=== SGD ===\n",
            "Saved: models/SGD_model.pkl\n",
            "Accuracy: 0.333 Precision: 0.0 Recall: 0.0 AUC: 0.5\n",
            "Pred sample: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Scores sample: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [09:39:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== XGBoost ===\n",
            "Saved: models/XGBoost_model.pkl\n",
            "Accuracy: 0.58 Precision: 0.664 Recall: 0.75 AUC: 0.494\n",
            "Pred sample: [1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "Scores sample: [0.813, 0.535, 0.603, 0.893, 0.625, 0.576, 0.355, 0.794, 0.974, 0.767]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [09:39:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== HybridBlend ===\n",
            "Accuracy: 0.513 Precision: 0.708 Recall: 0.46 AUC: 0.524\n",
            "Pred sample: [1, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n",
            "Scores sample: [0.53, 0.41, 0.437, 0.511, 0.445, 0.407, 0.367, 0.533, 0.561, 0.495]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/synth/generators.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "faker = Faker(\"en_US\")\n",
        "\n",
        "# Realistic Kenyan towns (~70 branches across regions)\n",
        "KENYAN_TOWNS = [\n",
        "    # Nairobi & Central\n",
        "    \"Nairobi\", \"Thika\", \"Kiambu\", \"Murang'a\", \"Nyeri\", \"Embu\", \"Meru\",\n",
        "    # Coast\n",
        "    \"Mombasa\", \"Kilifi\", \"Lamu\", \"Malindi\", \"Kwale\", \"Voi\", \"Taita\",\n",
        "    # Rift Valley\n",
        "    \"Nakuru\", \"Naivasha\", \"Kericho\", \"Eldoret\", \"Kitale\", \"Bomet\", \"Narok\",\n",
        "    # Eastern\n",
        "    \"Machakos\", \"Kitui\", \"Makueni\", \"Isiolo\", \"Marsabit\",\n",
        "    # Western\n",
        "    \"Kakamega\", \"Bungoma\", \"Vihiga\", \"Busia\",\n",
        "    # Nyanza\n",
        "    \"Kisumu\", \"Siaya\", \"Homa Bay\", \"Migori\", \"Kisii\", \"Nyamira\",\n",
        "]\n",
        "\n",
        "LOAN_PRODUCTS = {\n",
        "    \"Inuka\": 5,\n",
        "    \"Kuza\": 4,\n",
        "    \"Fadhili\": 6,\n",
        "    \"Imara\": 8,\n",
        "    \"SME\": 12\n",
        "}\n",
        "\n",
        "def derive_age_from_id(gov_id: str) -> int:\n",
        "    \"\"\"Estimate age from Kenyan ID pattern (rough approximation).\"\"\"\n",
        "    try:\n",
        "        prefix = int(str(gov_id)[:2])\n",
        "        if prefix < 20:\n",
        "            return random.randint(55, 70)\n",
        "        elif prefix < 30:\n",
        "            return random.randint(40, 55)\n",
        "        elif prefix < 33:\n",
        "            return random.randint(30, 40)\n",
        "        elif prefix < 35:\n",
        "            return random.randint(25, 30)\n",
        "        else:\n",
        "            return random.randint(20, 25)\n",
        "    except:\n",
        "        return random.randint(22, 55)\n",
        "\n",
        "def guess_gender(name: str) -> str:\n",
        "    \"\"\"Guess gender from name (very rough).\"\"\"\n",
        "    female_markers = [\"a\", \"ah\", \"na\", \"trice\", \"lyn\", \"mary\", \"rose\", \"cynthia\", \"grace\"]\n",
        "    if any(name.lower().endswith(suf) for suf in female_markers):\n",
        "        return \"F\"\n",
        "    return random.choices([\"F\",\"M\"], weights=[0.65,0.35])[0]\n",
        "\n",
        "def generate_clients_loans(\n",
        "    n_rows:int=1000,\n",
        "    default_rate:float=0.2,\n",
        "    gender_ratio:float=0.65,\n",
        "    loan_mean:int=20000,\n",
        "    branches:int=70,\n",
        "    product_weeks:int=6,\n",
        "    region_bias:str=\"balanced\",\n",
        "    seed:int|None=None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Generate realistic synthetic microfinance loan data.\"\"\"\n",
        "    if seed: random.seed(seed); np.random.seed(seed)\n",
        "\n",
        "    towns = random.sample(KENYAN_TOWNS, min(branches, len(KENYAN_TOWNS)))\n",
        "    products = list(LOAN_PRODUCTS.keys())\n",
        "\n",
        "    rows = []\n",
        "    for i in range(n_rows):\n",
        "        cust_name = faker.name()\n",
        "        gov_id = str(random.randint(20000000, 39999999))\n",
        "        age = derive_age_from_id(gov_id)\n",
        "\n",
        "        # Gender bias\n",
        "        if random.random() < gender_ratio:\n",
        "            gender = \"F\"\n",
        "        else:\n",
        "            gender = \"M\"\n",
        "        # backup from name guess\n",
        "        if not gender:\n",
        "            gender = guess_gender(cust_name)\n",
        "\n",
        "        branch = random.choice(towns)\n",
        "        product = random.choice(products)\n",
        "        weeks = LOAN_PRODUCTS[product]\n",
        "\n",
        "        # Loan size around mean ± variance\n",
        "        amount = max(1000, int(np.random.normal(loan_mean, loan_mean*0.3)))\n",
        "\n",
        "        # Loan type\n",
        "        loan_type = random.choice([\"normal\", \"group\", \"SME\"])\n",
        "\n",
        "        # Status\n",
        "        status = random.choice([\"active\", \"pending approval\"])\n",
        "\n",
        "        # Loan health based on default_rate\n",
        "        if random.random() < default_rate:\n",
        "            loan_health = random.choice([\"delinquent\", \"default\"])\n",
        "        else:\n",
        "            loan_health = \"performing\"\n",
        "\n",
        "        # Derived features\n",
        "        dti = round(random.uniform(0.1, 0.8), 2)  # debt-to-income\n",
        "        created_date = datetime.now() - timedelta(days=random.randint(0, 365))\n",
        "\n",
        "        rows.append({\n",
        "            \"customer_id\": f\"CUST{i+1:06d}\",\n",
        "            \"customer_name\": cust_name,\n",
        "            \"gov_id\": gov_id,\n",
        "            \"age\": age,\n",
        "            \"gender\": gender,\n",
        "            \"branch\": branch,\n",
        "            \"product\": product,\n",
        "            \"product_weeks\": weeks,\n",
        "            \"loan_amount\": amount,\n",
        "            \"loan_type\": loan_type,\n",
        "            \"status\": status,\n",
        "            \"loan_health\": loan_health,\n",
        "            \"debt_to_income\": dti,\n",
        "            \"created_date\": created_date.strftime(\"%Y-%m-%d\")\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def generate_batches(n_clients:int, batch_size:int, out_dir:str=\"data/sandbox_batches\", fmt:str=\"csv\", seed:int=1):\n",
        "    \"\"\"Generate multiple batch files for sandbox use.\"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    n_batches = max(1, n_clients // batch_size)\n",
        "    files = []\n",
        "    for b in range(1, n_batches+1):\n",
        "        df = generate_clients_loans(batch_size, seed=seed+b)\n",
        "        fname = os.path.join(out_dir, f\"batch_{b}.csv\")\n",
        "        df.to_csv(fname, index=False)\n",
        "        files.append(fname)\n",
        "    return files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrAebMF2hkV4",
        "outputId": "e05fe6d5-7f9e-4af9-c3ad-c9817867b674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/synth/generators.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import modules.synth.generators as g\n",
        "\n",
        "# reload to pick up the new code we just wrote\n",
        "importlib.reload(g)\n",
        "\n",
        "# Now test again\n",
        "df = g.generate_clients_loans(\n",
        "    n_rows=20,\n",
        "    default_rate=0.2,\n",
        "    gender_ratio=0.7,\n",
        "    loan_mean=25000,\n",
        "    branches=15,\n",
        "    product_weeks=6,\n",
        "    seed=42\n",
        ")\n",
        "print(df.head())\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Loan health counts:\\n\", df['loan_health'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLUMc4woh6lj",
        "outputId": "c7213a84-097e-427c-b94a-4ed6f2f8b97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  customer_id     customer_name    gov_id  age gender    branch  product  \\\n",
            "0  CUST000001     Paul Gonzalez  20999828   42      F  Machakos      SME   \n",
            "1  CUST000002     Scott Sanchez  27396759   54      F    Kitale    Inuka   \n",
            "2  CUST000003      Laura Walker  23429607   42      F     Nyeri  Fadhili   \n",
            "3  CUST000004       Leah Abbott  24188470   52      F    Nakuru      SME   \n",
            "4  CUST000005  Kristina Jackson  29710248   42      M     Taita    Inuka   \n",
            "\n",
            "   product_weeks  loan_amount loan_type            status loan_health  \\\n",
            "0             12        28725    normal            active  performing   \n",
            "1              5        23963    normal  pending approval  performing   \n",
            "2              6        29857       SME  pending approval  performing   \n",
            "3             12        36422     group            active  performing   \n",
            "4              5        23243     group  pending approval  performing   \n",
            "\n",
            "   debt_to_income created_date  \n",
            "0            0.59   2025-01-29  \n",
            "1            0.21   2025-03-12  \n",
            "2            0.61   2024-11-30  \n",
            "3            0.13   2025-05-07  \n",
            "4            0.68   2025-06-09  \n",
            "Shape: (20, 14)\n",
            "Loan health counts:\n",
            " loan_health\n",
            "performing    16\n",
            "delinquent     3\n",
            "default        1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import modules.synth.generators as g\n",
        "\n",
        "df = g.generate_clients_loans(\n",
        "    n_rows=20,\n",
        "    default_rate=0.2,\n",
        "    gender_ratio=0.7,\n",
        "    loan_mean=25000,\n",
        "    branches=15,\n",
        "    product_weeks=6,\n",
        "    seed=42\n",
        ")\n",
        "print(df.head())\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Loan health counts:\\n\", df['loan_health'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVoyMRywiDYY",
        "outputId": "1daa4c5c-c272-4e2b-f8b5-df825646e30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  customer_id    customer_name    gov_id  age gender    branch  product  \\\n",
            "0  CUST000001    Kayla Watkins  20999828   42      F  Machakos      SME   \n",
            "1  CUST000002   Joshua Shannon  27396759   54      F    Kitale    Inuka   \n",
            "2  CUST000003     Adam Marquez  23429607   42      F     Nyeri  Fadhili   \n",
            "3  CUST000004      David Hardy  24188470   52      F    Nakuru      SME   \n",
            "4  CUST000005  Charles Johnson  29710248   42      M     Taita    Inuka   \n",
            "\n",
            "   product_weeks  loan_amount loan_type            status loan_health  \\\n",
            "0             12        28725    normal            active  performing   \n",
            "1              5        23963    normal  pending approval  performing   \n",
            "2              6        29857       SME  pending approval  performing   \n",
            "3             12        36422     group            active  performing   \n",
            "4              5        23243     group  pending approval  performing   \n",
            "\n",
            "   debt_to_income created_date  \n",
            "0            0.59   2025-01-29  \n",
            "1            0.21   2025-03-12  \n",
            "2            0.61   2024-11-30  \n",
            "3            0.13   2025-05-07  \n",
            "4            0.68   2025-06-09  \n",
            "Shape: (20, 14)\n",
            "Loan health counts:\n",
            " loan_health\n",
            "performing    16\n",
            "delinquent     3\n",
            "default        1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import importlib\n",
        "from modules.synth import generators as g\n",
        "\n",
        "importlib.reload(g)  # always reload latest generator\n",
        "\n",
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    st.markdown(\"### 🔹 Data Generation Controls\")\n",
        "\n",
        "    # --- Sliders for 7 parameters ---\n",
        "    n_rows = st.slider(\"Number of clients\", 100, 10000, 1000, step=100)\n",
        "    default_rate = st.slider(\"Default rate (%)\", 0, 50, 20, step=1) / 100.0\n",
        "    gender_ratio = st.slider(\"Female ratio (%)\", 0, 100, 70, step=5) / 100.0\n",
        "    loan_mean = st.number_input(\"Average loan amount (KES)\", 5000, 100000, 25000, step=1000)\n",
        "    branches = st.slider(\"Number of branches\", 5, 70, 20, step=1)\n",
        "    product_weeks = st.slider(\"Typical product cycle (weeks)\", 4, 12, 6, step=1)\n",
        "    region_bias = st.selectbox(\"Region bias\", [\"balanced\", \"Eastern\", \"Coast\", \"Central\", \"Rift\", \"Nairobi\", \"Western\"])\n",
        "\n",
        "    # --- Button to generate dataset ---\n",
        "    if st.button(\"🚀 Generate Dataset\"):\n",
        "        df = g.generate_clients_loans(\n",
        "            n_rows=n_rows,\n",
        "            default_rate=default_rate,\n",
        "            gender_ratio=gender_ratio,\n",
        "            loan_mean=loan_mean,\n",
        "            branches=branches,\n",
        "            product_weeks=product_weeks,\n",
        "            region_bias=region_bias,\n",
        "            seed=42\n",
        "        )\n",
        "        st.success(f\"✅ Generated dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "        st.dataframe(df.head(20))\n",
        "\n",
        "        # Save to sandbox_batches\n",
        "        os.makedirs(\"data/sandbox_batches\", exist_ok=True)\n",
        "        fname = f\"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        df.to_csv(fname, index=False)\n",
        "        st.info(f\"📂 Saved to {fname}\")\n",
        "\n",
        "        # Show quick stats\n",
        "        st.write(\"### Stats\")\n",
        "        st.write(df[\"loan_health\"].value_counts(normalize=True).round(2))\n",
        "        st.write(df[\"gender\"].value_counts(normalize=True).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z9rjo60iRYO",
        "outputId": "b2fd0062-7bac-4afb-a6e8-f34628e51261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pages._04_admin_sandbox as sandbox\n",
        "\n",
        "importlib.reload(sandbox)\n",
        "\n",
        "# Run the app function (simulates Streamlit run)\n",
        "sandbox.app()\n",
        "print(\"✅ Sandbox app loaded (UI should appear in Streamlit)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shcOUiN8iXlI",
        "outputId": "c764c17c-261b-4a68-a154-f79d1ccfbcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 09:51:00.771 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.116 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-08-31 09:51:01.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.145 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.148 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.163 Session state does not function when running a script without `streamlit run`\n",
            "2025-08-31 09:51:01.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.165 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.185 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.187 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.187 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.195 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.196 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.202 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.202 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:51:01.209 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sandbox app loaded (UI should appear in Streamlit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import importlib\n",
        "from modules.synth import generators as g\n",
        "from modules.ml import engine  # we'll use your ml engine\n",
        "\n",
        "importlib.reload(g)\n",
        "importlib.reload(engine)\n",
        "\n",
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    # === TAB SETUP ===\n",
        "    tabs = st.tabs([\"Data Generation\", \"Model Training\", \"Stress Testing\", \"Audit Logs\"])\n",
        "\n",
        "    # ---------------- TAB 1: Data Generation ----------------\n",
        "    with tabs[0]:\n",
        "        st.markdown(\"### 🔹 Data Generation Controls\")\n",
        "\n",
        "        n_rows = st.slider(\"Number of clients\", 100, 10000, 1000, step=100)\n",
        "        default_rate = st.slider(\"Default rate (%)\", 0, 50, 20, step=1) / 100.0\n",
        "        gender_ratio = st.slider(\"Female ratio (%)\", 0, 100, 70, step=5) / 100.0\n",
        "        loan_mean = st.number_input(\"Average loan amount (KES)\", 5000, 100000, 25000, step=1000)\n",
        "        branches = st.slider(\"Number of branches\", 5, 70, 20, step=1)\n",
        "        product_weeks = st.slider(\"Typical product cycle (weeks)\", 4, 12, 6, step=1)\n",
        "        region_bias = st.selectbox(\"Region bias\", [\"balanced\", \"Eastern\", \"Coast\", \"Central\", \"Rift\", \"Nairobi\", \"Western\"])\n",
        "\n",
        "        if st.button(\"🚀 Generate Dataset\"):\n",
        "            df = g.generate_clients_loans(\n",
        "                n_rows=n_rows,\n",
        "                default_rate=default_rate,\n",
        "                gender_ratio=gender_ratio,\n",
        "                loan_mean=loan_mean,\n",
        "                branches=branches,\n",
        "                product_weeks=product_weeks,\n",
        "                region_bias=region_bias,\n",
        "                seed=42\n",
        "            )\n",
        "            st.success(f\"✅ Generated dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "            st.dataframe(df.head(20))\n",
        "\n",
        "            os.makedirs(\"data/sandbox_batches\", exist_ok=True)\n",
        "            fname = f\"data/sandbox_batches/sandbox_latest.csv\"\n",
        "            df.to_csv(fname, index=False)\n",
        "            st.info(f\"📂 Saved to {fname}\")\n",
        "\n",
        "            st.write(\"### Stats\")\n",
        "            st.write(df[\"loan_health\"].value_counts(normalize=True).round(2))\n",
        "            st.write(df[\"gender\"].value_counts(normalize=True).round(2))\n",
        "\n",
        "    # ---------------- TAB 2: Model Training ----------------\n",
        "    with tabs[1]:\n",
        "        st.markdown(\"### 🔹 Train Models\")\n",
        "\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            st.success(f\"Loaded {fname} with {df.shape[0]} rows\")\n",
        "\n",
        "            algo = st.selectbox(\"Choose algorithm\", [\"LogReg\", \"SGD\", \"XGBoost\", \"HybridBlend\"])\n",
        "            if st.button(\"⚡ Train Model\"):\n",
        "                metrics, shap_values, model_path = engine.train_model(df, algo)\n",
        "                st.success(f\"✅ {algo} trained and saved to {model_path}\")\n",
        "\n",
        "                st.write(\"### Metrics\")\n",
        "                st.json(metrics)\n",
        "\n",
        "                st.write(\"### Prediction Sample\")\n",
        "                preds, scores = engine.predict_sample(df, algo)\n",
        "                st.write(preds[:10])\n",
        "                st.write(scores[:10])\n",
        "        else:\n",
        "            st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWENHm-giknu",
        "outputId": "2fc2bebd-34ae-4436-a16c-f62553d60ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pages._04_admin_sandbox as sandbox\n",
        "\n",
        "importlib.reload(sandbox)\n",
        "sandbox.app()\n",
        "print(\"✅ Sandbox with Model Training (Tab 2) loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnRdjytXirDi",
        "outputId": "6d0881cb-6605-4e6a-f007-acf865a736cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 09:52:21.043 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.049 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.056 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.058 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.061 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.063 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.068 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.071 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.075 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.077 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.080 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.085 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.087 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.097 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.098 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.099 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.099 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.147 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.183 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.184 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.190 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:52:21.195 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sandbox with Model Training (Tab 2) loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/ml/engine.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, joblib\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Core training function ---\n",
        "def train_model(df, algo=\"LogReg\", lr=0.1, seed=42):\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "    # Features (numeric only)\n",
        "    features = [\"age\", \"loan_amount\", \"debt_to_income\", \"product_weeks\"]\n",
        "    X = df[features].values\n",
        "    y = (df[\"loan_health\"] != \"performing\").astype(int)\n",
        "\n",
        "    if algo == \"LogReg\":\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(loss=\"log_loss\", max_iter=1000)\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = xgb.XGBClassifier(\n",
        "            use_label_encoder=False,\n",
        "            eval_metric=\"logloss\",\n",
        "            learning_rate=lr,\n",
        "            n_estimators=50,\n",
        "            max_depth=4,\n",
        "            random_state=seed\n",
        "        )\n",
        "    elif algo == \"HybridBlend\":\n",
        "        # Load or train 3 base models\n",
        "        base_models = {}\n",
        "        for sub in [\"LogReg\", \"SGD\", \"XGBoost\"]:\n",
        "            path = f\"models/{sub}_model.pkl\"\n",
        "            if os.path.exists(path):\n",
        "                base_models[sub] = joblib.load(path)\n",
        "            else:\n",
        "                metrics, _, _ = train_model(df, sub, seed=seed)\n",
        "                base_models[sub] = joblib.load(path)\n",
        "\n",
        "        def hybrid_predict(X):\n",
        "            preds = []\n",
        "            for sub, weight in zip([\"LogReg\",\"SGD\",\"XGBoost\"], [0.3, 0.2, 0.5]):\n",
        "                try:\n",
        "                    p = base_models[sub].predict_proba(X)[:,1]\n",
        "                except:\n",
        "                    p = base_models[sub].predict(X)\n",
        "                preds.append(p * weight)\n",
        "            return np.sum(preds, axis=0)\n",
        "\n",
        "        model = base_models\n",
        "        preds = (hybrid_predict(X) > 0.5).astype(int)\n",
        "        scores = hybrid_predict(X)\n",
        "\n",
        "        metrics = {\n",
        "            \"Accuracy\": float(accuracy_score(y, preds)),\n",
        "            \"Precision\": float(precision_score(y, preds, zero_division=0)),\n",
        "            \"Recall\": float(recall_score(y, preds, zero_division=0)),\n",
        "            \"AUC\": float(roc_auc_score(y, scores))\n",
        "        }\n",
        "        path = \"models/HybridBlend_model.pkl\"\n",
        "        joblib.dump(model, path)\n",
        "        return metrics, None, path\n",
        "\n",
        "    # --- Train single model ---\n",
        "    model.fit(X, y)\n",
        "    preds = model.predict(X)\n",
        "    scores = model.predict_proba(X)[:,1] if hasattr(model, \"predict_proba\") else preds\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": float(accuracy_score(y, preds)),\n",
        "        \"Precision\": float(precision_score(y, preds, zero_division=0)),\n",
        "        \"Recall\": float(recall_score(y, preds, zero_division=0)),\n",
        "        \"AUC\": float(roc_auc_score(y, scores))\n",
        "    }\n",
        "\n",
        "    path = f\"models/{algo}_model.pkl\"\n",
        "    joblib.dump(model, path)\n",
        "\n",
        "    return metrics, scores, path\n",
        "\n",
        "def predict_sample(df, algo=\"LogReg\", n=10):\n",
        "    features = [\"age\", \"loan_amount\", \"debt_to_income\", \"product_weeks\"]\n",
        "    X = df[features].values\n",
        "    y = (df[\"loan_health\"] != \"performing\").astype(int)\n",
        "\n",
        "    model = joblib.load(f\"models/{algo}_model.pkl\")\n",
        "\n",
        "    if algo == \"HybridBlend\":\n",
        "        preds, scores = [], []\n",
        "        for sub, weight in zip([\"LogReg\",\"SGD\",\"XGBoost\"], [0.3, 0.2, 0.5]):\n",
        "            submodel = model[sub]\n",
        "            try:\n",
        "                p = submodel.predict_proba(X)[:,1]\n",
        "            except:\n",
        "                p = submodel.predict(X)\n",
        "            scores.append(p * weight)\n",
        "        score_final = np.sum(scores, axis=0)\n",
        "        preds_final = (score_final > 0.5).astype(int)\n",
        "        return preds_final[:n].tolist(), score_final[:n].round(3).tolist()\n",
        "\n",
        "    preds = model.predict(X)\n",
        "    scores = model.predict_proba(X)[:,1] if hasattr(model, \"predict_proba\") else preds\n",
        "    return preds[:n].tolist(), scores[:n].round(3).tolist()\n",
        "\n",
        "def stress_test(df, shock_type=\"default\", intensity=0.2):\n",
        "    \"\"\"Apply stress scenario to dataset.\"\"\"\n",
        "    df = df.copy()\n",
        "    if shock_type == \"default\":\n",
        "        # Flip some performing loans to default\n",
        "        mask = df[\"loan_health\"]==\"performing\"\n",
        "        flip_idx = df[mask].sample(frac=intensity, replace=False).index\n",
        "        df.loc[flip_idx,\"loan_health\"]=\"default\"\n",
        "    elif shock_type == \"loan_amount\":\n",
        "        df[\"loan_amount\"] = df[\"loan_amount\"] * (1+intensity)\n",
        "    elif shock_type == \"unemployment\":\n",
        "        # simulate by raising debt-to-income\n",
        "        df[\"debt_to_income\"] = np.minimum(1.0, df[\"debt_to_income\"]*(1+intensity))\n",
        "    return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tVaccrfi-RM",
        "outputId": "a06d23ba-a2d8-41a7-fbcd-6f75f32a3efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/ml/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import importlib\n",
        "from modules.synth import generators as g\n",
        "from modules.ml import engine\n",
        "\n",
        "# Always reload modules (fresh patches)\n",
        "importlib.reload(g)\n",
        "importlib.reload(engine)\n",
        "\n",
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"1️⃣ Data Generation\",\n",
        "        \"2️⃣ Model Training\",\n",
        "        \"3️⃣ Stress Testing\"\n",
        "    ])\n",
        "\n",
        "    # ---------------- TAB 1: Data Generation ----------------\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"🔹 Data Generation Controls\")\n",
        "\n",
        "        n_rows = st.slider(\"Number of clients\", 100, 10000, 1000, step=100)\n",
        "        default_rate = st.slider(\"Default rate (%)\", 0, 50, 20, step=1) / 100.0\n",
        "        gender_ratio = st.slider(\"Female ratio (%)\", 0, 100, 70, step=5) / 100.0\n",
        "        loan_mean = st.number_input(\"Average loan amount (KES)\", 5000, 100000, 25000, step=1000)\n",
        "        branches = st.slider(\"Number of branches\", 5, 70, 20, step=1)\n",
        "        product_weeks = st.slider(\"Typical product cycle (weeks)\", 4, 12, 6, step=1)\n",
        "        region_bias = st.selectbox(\"Region bias\", [\"balanced\",\"Eastern\",\"Coast\",\"Central\",\"Rift\",\"Nairobi\",\"Western\"])\n",
        "\n",
        "        if st.button(\"🚀 Generate Dataset\"):\n",
        "            df = g.generate_clients_loans(\n",
        "                n_rows=n_rows,\n",
        "                default_rate=default_rate,\n",
        "                gender_ratio=gender_ratio,\n",
        "                loan_mean=loan_mean,\n",
        "                branches=branches,\n",
        "                product_weeks=product_weeks,\n",
        "                region_bias=region_bias,\n",
        "                seed=42\n",
        "            )\n",
        "            st.success(f\"✅ Generated dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "            st.dataframe(df.head(20))\n",
        "\n",
        "            os.makedirs(\"data/sandbox_batches\", exist_ok=True)\n",
        "            fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "            df.to_csv(fname, index=False)\n",
        "            st.info(f\"📂 Saved to {fname}\")\n",
        "\n",
        "    # ---------------- TAB 2: Model Training ----------------\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"🔹 Model Training\")\n",
        "\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            st.success(f\"Loaded {fname}\")\n",
        "\n",
        "            if st.button(\"⚡ Train Models\"):\n",
        "                for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                    metrics, _, path = engine.train_model(df, algo)\n",
        "                    st.write(f\"### {algo}\")\n",
        "                    st.json(metrics)\n",
        "                    preds, scores = engine.predict_sample(df, algo)\n",
        "                    st.write(\"Sample preds:\", preds)\n",
        "                    st.write(\"Sample scores:\", scores)\n",
        "        else:\n",
        "            st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "\n",
        "    # ---------------- TAB 3: Stress Testing ----------------\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"🔹 Stress Testing\")\n",
        "\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            st.success(f\"Loaded {fname}\")\n",
        "\n",
        "            shock = st.selectbox(\"Shock type\", [\"default\",\"loan_amount\",\"unemployment\"])\n",
        "            intensity = st.slider(\"Shock intensity\", 0.0, 1.0, 0.2, step=0.05)\n",
        "\n",
        "            if st.button(\"🔥 Run Stress Test\"):\n",
        "                stressed_df = engine.stress_test(df, shock, intensity)\n",
        "                st.write(\"📊 Preview stressed dataset\")\n",
        "                st.dataframe(stressed_df.head(10))\n",
        "\n",
        "                for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                    metrics, _, _ = engine.train_model(stressed_df, algo)\n",
        "                    st.write(f\"### {algo}\")\n",
        "                    st.json(metrics)\n",
        "        else:\n",
        "            st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_zn_1r4j-F9",
        "outputId": "0d0fcc2f-23d7-4d10-a178-d60ae8b946f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pages._04_admin_sandbox as sandbox\n",
        "\n",
        "importlib.reload(sandbox)\n",
        "sandbox.app()\n",
        "print(\"✅ Sandbox Tabs 1–3 fully loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHBnsFl6kEwn",
        "outputId": "108201aa-29ef-4847-b17c-258760132248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 09:58:28.004 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.014 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.023 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.027 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.030 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.033 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.041 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.042 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.045 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.048 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.051 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.055 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.057 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.060 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.062 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.067 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.070 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.075 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.080 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.081 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.083 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.088 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.090 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.091 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.096 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.097 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.098 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.098 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.144 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.145 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.146 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.147 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.148 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.183 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.185 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.187 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 09:58:28.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sandbox Tabs 1–3 fully loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import importlib\n",
        "import sqlite3\n",
        "from modules.synth import generators as g\n",
        "from modules.ml import engine\n",
        "\n",
        "# Always reload modules\n",
        "importlib.reload(g)\n",
        "importlib.reload(engine)\n",
        "\n",
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"1️⃣ Data Generation\",\n",
        "        \"2️⃣ Model Training\",\n",
        "        \"3️⃣ Stress Testing\",\n",
        "        \"4️⃣ Audit Logs\"\n",
        "    ])\n",
        "\n",
        "    # ---------------- TAB 1: Data Generation ----------------\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"🔹 Data Generation Controls\")\n",
        "        n_rows = st.slider(\"Number of clients\", 100, 10000, 1000, step=100)\n",
        "        default_rate = st.slider(\"Default rate (%)\", 0, 50, 20, step=1) / 100.0\n",
        "        gender_ratio = st.slider(\"Female ratio (%)\", 0, 100, 70, step=5) / 100.0\n",
        "        loan_mean = st.number_input(\"Average loan amount (KES)\", 5000, 100000, 25000, step=1000)\n",
        "        branches = st.slider(\"Number of branches\", 5, 70, 20, step=1)\n",
        "        product_weeks = st.slider(\"Typical product cycle (weeks)\", 4, 12, 6, step=1)\n",
        "        region_bias = st.selectbox(\"Region bias\", [\"balanced\",\"Eastern\",\"Coast\",\"Central\",\"Rift\",\"Nairobi\",\"Western\"])\n",
        "\n",
        "        if st.button(\"🚀 Generate Dataset\"):\n",
        "            df = g.generate_clients_loans(\n",
        "                n_rows=n_rows,\n",
        "                default_rate=default_rate,\n",
        "                gender_ratio=gender_ratio,\n",
        "                loan_mean=loan_mean,\n",
        "                branches=branches,\n",
        "                product_weeks=product_weeks,\n",
        "                region_bias=region_bias,\n",
        "                seed=42\n",
        "            )\n",
        "            st.success(f\"✅ Generated dataset with {df.shape[0]} rows\")\n",
        "            st.dataframe(df.head(20))\n",
        "\n",
        "            os.makedirs(\"data/sandbox_batches\", exist_ok=True)\n",
        "            fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "            df.to_csv(fname, index=False)\n",
        "            st.info(f\"📂 Saved to {fname}\")\n",
        "\n",
        "    # ---------------- TAB 2: Model Training ----------------\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"🔹 Model Training\")\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            st.success(f\"Loaded {fname}\")\n",
        "\n",
        "            if st.button(\"⚡ Train Models\"):\n",
        "                for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                    metrics, _, path = engine.train_model(df, algo)\n",
        "                    st.write(f\"### {algo}\")\n",
        "                    st.json(metrics)\n",
        "                    preds, scores = engine.predict_sample(df, algo)\n",
        "                    st.write(\"Sample preds:\", preds)\n",
        "                    st.write(\"Sample scores:\", scores)\n",
        "        else:\n",
        "            st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "\n",
        "    # ---------------- TAB 3: Stress Testing ----------------\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"🔹 Stress Testing\")\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            st.success(f\"Loaded {fname}\")\n",
        "\n",
        "            shock = st.selectbox(\"Shock type\", [\"default\",\"loan_amount\",\"unemployment\"])\n",
        "            intensity = st.slider(\"Shock intensity\", 0.0, 1.0, 0.2, step=0.05)\n",
        "\n",
        "            if st.button(\"🔥 Run Stress Test\"):\n",
        "                stressed_df = engine.stress_test(df, shock, intensity)\n",
        "                st.write(\"📊 Preview stressed dataset\")\n",
        "                st.dataframe(stressed_df.head(10))\n",
        "\n",
        "                for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                    metrics, _, _ = engine.train_model(stressed_df, algo)\n",
        "                    st.write(f\"### {algo}\")\n",
        "                    st.json(metrics)\n",
        "        else:\n",
        "            st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "\n",
        "    # ---------------- TAB 4: Audit Logs ----------------\n",
        "    with tabs[3]:\n",
        "        st.subheader(\"🔹 Audit Logs\")\n",
        "\n",
        "        db_path = \"data/audit.db\"\n",
        "        if os.path.exists(db_path):\n",
        "            conn = sqlite3.connect(db_path)\n",
        "            df_audit = pd.read_sql(\"SELECT * FROM audit_logs ORDER BY created_at DESC LIMIT 200\", conn)\n",
        "            conn.close()\n",
        "\n",
        "            st.write(\"Latest audit entries:\")\n",
        "            st.dataframe(df_audit)\n",
        "\n",
        "            # Filters\n",
        "            user_filter = st.text_input(\"Filter by user\")\n",
        "            action_filter = st.text_input(\"Filter by action\")\n",
        "            if user_filter:\n",
        "                df_audit = df_audit[df_audit[\"user\"].str.contains(user_filter, case=False)]\n",
        "            if action_filter:\n",
        "                df_audit = df_audit[df_audit[\"action\"].str.contains(action_filter, case=False)]\n",
        "            st.write(\"Filtered results:\", df_audit.shape[0])\n",
        "            st.dataframe(df_audit)\n",
        "        else:\n",
        "            st.warning(\"⚠️ No audit database found at data/audit.db\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjorQh-tkWZd",
        "outputId": "081d688c-4bf3-4124-8e12-88cc6563f54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import importlib\n",
        "import sqlite3\n",
        "from modules.synth import generators as g\n",
        "from modules.ml import engine\n",
        "\n",
        "# Always reload modules\n",
        "importlib.reload(g)\n",
        "importlib.reload(engine)\n",
        "\n",
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"1️⃣ Data Generation\",\n",
        "        \"2️⃣ Model Training\",\n",
        "        \"3️⃣ Stress Testing\",\n",
        "        \"4️⃣ Audit Logs\"\n",
        "    ])\n",
        "\n",
        "    # ---------------- TAB 1: Data Generation ----------------\n",
        "    with tabs[0]:\n",
        "        st.subheader(\"🔹 Data Generation Controls\")\n",
        "        n_rows = st.slider(\"Number of clients\", 100, 10000, 1000, step=100)\n",
        "        default_rate = st.slider(\"Default rate (%)\", 0, 50, 20, step=1) / 100.0\n",
        "        gender_ratio = st.slider(\"Female ratio (%)\", 0, 100, 70, step=5) / 100.0\n",
        "        loan_mean = st.number_input(\"Average loan amount (KES)\", 5000, 100000, 25000, step=1000)\n",
        "        branches = st.slider(\"Number of branches\", 5, 70, 20, step=1)\n",
        "        product_weeks = st.slider(\"Typical product cycle (weeks)\", 4, 12, 6, step=1)\n",
        "        region_bias = st.selectbox(\"Region bias\", [\"balanced\",\"Eastern\",\"Coast\",\"Central\",\"Rift\",\"Nairobi\",\"Western\"])\n",
        "\n",
        "        if st.button(\"🚀 Generate Dataset\"):\n",
        "            df = g.generate_clients_loans(\n",
        "                n_rows=n_rows,\n",
        "                default_rate=default_rate,\n",
        "                gender_ratio=gender_ratio,\n",
        "                loan_mean=loan_mean,\n",
        "                branches=branches,\n",
        "                product_weeks=product_weeks,\n",
        "                region_bias=region_bias,\n",
        "                seed=42\n",
        "            )\n",
        "            st.success(f\"✅ Generated dataset with {df.shape[0]} rows\")\n",
        "            st.dataframe(df.head(20))\n",
        "\n",
        "            os.makedirs(\"data/sandbox_batches\", exist_ok=True)\n",
        "            fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "            df.to_csv(fname, index=False)\n",
        "            st.info(f\"📂 Saved to {fname}\")\n",
        "\n",
        "    # ---------------- TAB 2: Model Training ----------------\n",
        "    with tabs[1]:\n",
        "        st.subheader(\"🔹 Model Training\")\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            st.success(f\"Loaded {fname}\")\n",
        "\n",
        "            if st.button(\"⚡ Train Models\"):\n",
        "                for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                    metrics, _, path = engine.train_model(df, algo)\n",
        "                    st.write(f\"### {algo}\")\n",
        "                    st.json(metrics)\n",
        "                    preds, scores = engine.predict_sample(df, algo)\n",
        "                    st.write(\"Sample preds:\", preds)\n",
        "                    st.write(\"Sample scores:\", scores)\n",
        "        else:\n",
        "            st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "\n",
        "    # ---------------- TAB 3: Stress Testing ----------------\n",
        "    with tabs[2]:\n",
        "        st.subheader(\"🔹 Stress Testing\")\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df = pd.read_csv(fname)\n",
        "            st.success(f\"Loaded {fname}\")\n",
        "\n",
        "            shock = st.selectbox(\"Shock type\", [\"default\",\"loan_amount\",\"unemployment\"])\n",
        "            intensity = st.slider(\"Shock intensity\", 0.0, 1.0, 0.2, step=0.05)\n",
        "\n",
        "            if st.button(\"🔥 Run Stress Test\"):\n",
        "                stressed_df = engine.stress_test(df, shock, intensity)\n",
        "                st.write(\"📊 Preview stressed dataset\")\n",
        "                st.dataframe(stressed_df.head(10))\n",
        "\n",
        "                for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                    metrics, _, _ = engine.train_model(stressed_df, algo)\n",
        "                    st.write(f\"### {algo}\")\n",
        "                    st.json(metrics)\n",
        "        else:\n",
        "            st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "\n",
        "    # ---------------- TAB 4: Audit Logs ----------------\n",
        "    with tabs[3]:\n",
        "        st.subheader(\"🔹 Audit Logs\")\n",
        "\n",
        "        db_path = \"data/audit.db\"\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        # Ensure table exists\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audit_logs (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            user TEXT,\n",
        "            action TEXT,\n",
        "            reason TEXT,\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\n",
        "        \"\"\")\n",
        "        conn.commit()\n",
        "\n",
        "        # Seed with sample if empty\n",
        "        cur.execute(\"SELECT COUNT(*) FROM audit_logs\")\n",
        "        if cur.fetchone()[0] == 0:\n",
        "            cur.executemany(\n",
        "                \"INSERT INTO audit_logs (user, action, reason) VALUES (?, ?, ?)\",\n",
        "                [\n",
        "                    (\"admin\", \"create_dataset\", \"Initial synthetic data\"),\n",
        "                    (\"analyst\", \"train_model\", \"Ran baseline LogReg\"),\n",
        "                    (\"auditor\", \"view_logs\", \"Checked audit table\")\n",
        "                ]\n",
        "            )\n",
        "            conn.commit()\n",
        "\n",
        "        # Load table\n",
        "        df_audit = pd.read_sql(\"SELECT * FROM audit_logs ORDER BY created_at DESC LIMIT 200\", conn)\n",
        "        conn.close()\n",
        "\n",
        "        st.write(\"Latest audit entries:\")\n",
        "        st.dataframe(df_audit)\n",
        "\n",
        "        # Filters\n",
        "        user_filter = st.text_input(\"Filter by user\")\n",
        "        action_filter = st.text_input(\"Filter by action\")\n",
        "        if user_filter:\n",
        "            df_audit = df_audit[df_audit[\"user\"].str.contains(user_filter, case=False)]\n",
        "        if action_filter:\n",
        "            df_audit = df_audit[df_audit[\"action\"].str.contains(action_filter, case=False)]\n",
        "        st.write(\"Filtered results:\", df_audit.shape[0])\n",
        "        st.dataframe(df_audit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyglysf5lV9x",
        "outputId": "b623b9d4-eff7-4e41-d6f1-5e8b521a9c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pages._04_admin_sandbox as sandbox\n",
        "\n",
        "importlib.reload(sandbox)\n",
        "sandbox.app()\n",
        "print(\"✅ Tabs 1–4 working (Audit logs auto-created)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjgDKOdWll8S",
        "outputId": "674d4cbc-fc84-47e2-8a6c-e3e2d1d1e479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 10:05:07.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.695 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.700 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.701 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.701 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.723 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.723 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.729 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.730 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.730 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.732 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.733 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.734 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.735 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.735 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.736 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.736 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.737 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.738 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.738 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.742 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.742 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.744 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.745 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.747 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.749 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.749 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.751 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.756 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.757 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.757 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.758 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.759 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.759 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.760 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.760 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.761 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.793 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.799 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.889 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.890 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.891 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.892 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.893 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.902 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.907 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.910 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.911 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.911 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.917 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.918 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.918 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.920 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.921 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.927 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.928 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.928 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.931 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.934 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:05:07.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tabs 1–4 working (Audit logs auto-created)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/_04_admin_sandbox.py\n",
        "import streamlit as st\n",
        "import os, sqlite3, pandas as pd\n",
        "import modules.synth.generators as g\n",
        "import modules.ml.engine as engine\n",
        "\n",
        "# ---------------- TAB 1: Data Generation ----------------\n",
        "def render_tab1():\n",
        "    st.markdown(\"### 🔹 Data Generation\")\n",
        "    n_rows = st.slider(\"Number of rows\", 100, 5000, 1000, step=100)\n",
        "    default_rate = st.slider(\"Default rate\", 0.0, 0.5, 0.1, step=0.05)\n",
        "    gender_ratio = st.slider(\"Female ratio\", 0.0, 1.0, 0.6, step=0.1)\n",
        "    loan_mean = st.slider(\"Avg loan amount\", 5000, 100000, 20000, step=5000)\n",
        "    branches = st.slider(\"Branches\", 5, 50, 20)\n",
        "    product_weeks = st.slider(\"Product weeks\", 4, 12, 6)\n",
        "    region_bias = st.checkbox(\"Bias by region?\", value=True)\n",
        "\n",
        "    if st.button(\"🚀 Generate Dataset\"):\n",
        "        df = g.generate_clients_loans(\n",
        "            n_rows=n_rows,\n",
        "            default_rate=default_rate,\n",
        "            gender_ratio=gender_ratio,\n",
        "            loan_mean=loan_mean,\n",
        "            branches=branches,\n",
        "            product_weeks=product_weeks,\n",
        "            region_bias=region_bias,\n",
        "            seed=42\n",
        "        )\n",
        "        st.success(f\"✅ Generated dataset with {df.shape[0]} rows\")\n",
        "        st.dataframe(df.head(20))\n",
        "\n",
        "        os.makedirs(\"data/sandbox_batches\", exist_ok=True)\n",
        "        fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "        df.to_csv(fname, index=False)\n",
        "        st.info(f\"📂 Saved to {fname}\")\n",
        "\n",
        "# ---------------- TAB 2: Model Training ----------------\n",
        "def render_tab2():\n",
        "    st.markdown(\"### 🔹 Model Training\")\n",
        "    fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "    if os.path.exists(fname):\n",
        "        df = pd.read_csv(fname)\n",
        "        st.success(f\"Loaded {fname} with {df.shape[0]} rows\")\n",
        "\n",
        "        if st.button(\"⚡ Train Models\"):\n",
        "            for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                metrics, _, path = engine.train_model(df, algo)\n",
        "                st.write(f\"### {algo}\")\n",
        "                st.json(metrics)\n",
        "                preds, scores = engine.predict_sample(df, algo)\n",
        "                st.write(\"Sample preds:\", preds)\n",
        "                st.write(\"Sample scores:\", scores)\n",
        "    else:\n",
        "        st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "\n",
        "# ---------------- TAB 3: Stress Testing ----------------\n",
        "def render_tab3():\n",
        "    st.markdown(\"### 🔹 Stress Testing\")\n",
        "\n",
        "    fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "    if os.path.exists(fname):\n",
        "        df = pd.read_csv(fname)\n",
        "        st.success(f\"Loaded {fname}\")\n",
        "\n",
        "        shock = st.selectbox(\"Shock type\", [\"default\",\"loan_amount\",\"unemployment\"])\n",
        "        intensity = st.slider(\"Shock intensity\", 0.0, 1.0, 0.2, step=0.05)\n",
        "\n",
        "        if st.button(\"🔥 Run Stress Test\"):\n",
        "            stressed_df = engine.stress_test(df, shock, intensity)\n",
        "            st.write(\"📊 Preview stressed dataset\")\n",
        "            st.dataframe(stressed_df.head(10))\n",
        "\n",
        "            for algo in [\"LogReg\",\"SGD\",\"XGBoost\",\"HybridBlend\"]:\n",
        "                metrics, _, _ = engine.train_model(stressed_df, algo)\n",
        "                st.write(f\"### {algo}\")\n",
        "                st.json(metrics)\n",
        "    else:\n",
        "        st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "\n",
        "# ---------------- TAB 4: Audit Logs ----------------\n",
        "def render_tab4():\n",
        "    st.subheader(\"🔹 Audit Logs\")\n",
        "\n",
        "    db_path = \"data/audit.db\"\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS audit_logs (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        user TEXT,\n",
        "        action TEXT,\n",
        "        reason TEXT,\n",
        "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "    )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM audit_logs\")\n",
        "    if cur.fetchone()[0] == 0:\n",
        "        cur.executemany(\n",
        "            \"INSERT INTO audit_logs (user, action, reason) VALUES (?, ?, ?)\",\n",
        "            [\n",
        "                (\"admin\", \"create_dataset\", \"Initial synthetic data\"),\n",
        "                (\"analyst\", \"train_model\", \"Ran baseline LogReg\"),\n",
        "                (\"auditor\", \"view_logs\", \"Checked audit table\")\n",
        "            ]\n",
        "        )\n",
        "        conn.commit()\n",
        "\n",
        "    df_audit = pd.read_sql(\"SELECT * FROM audit_logs ORDER BY created_at DESC LIMIT 200\", conn)\n",
        "    conn.close()\n",
        "\n",
        "    st.write(\"Latest audit entries:\")\n",
        "    st.dataframe(df_audit)\n",
        "\n",
        "    user_filter = st.text_input(\"Filter by user\")\n",
        "    action_filter = st.text_input(\"Filter by action\")\n",
        "    if user_filter:\n",
        "        df_audit = df_audit[df_audit[\"user\"].str.contains(user_filter, case=False)]\n",
        "    if action_filter:\n",
        "        df_audit = df_audit[df_audit[\"action\"].str.contains(action_filter, case=False)]\n",
        "    st.write(\"Filtered results:\", df_audit.shape[0])\n",
        "    st.dataframe(df_audit)\n",
        "\n",
        "# ---------------- MAIN APP ----------------\n",
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"1️⃣ Data Generation\",\n",
        "        \"2️⃣ Model Training\",\n",
        "        \"3️⃣ Stress Testing\",\n",
        "        \"4️⃣ Audit Logs\"\n",
        "    ])\n",
        "\n",
        "    with tabs[0]: render_tab1()\n",
        "    with tabs[1]: render_tab2()\n",
        "    with tabs[2]: render_tab3()\n",
        "    with tabs[3]: render_tab4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcDPsc6Ymu3k",
        "outputId": "f0084a71-b5a6-4b03-f1d3-db55c478a4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/_04_admin_sandbox.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pages._04_admin_sandbox as sandbox\n",
        "\n",
        "importlib.reload(sandbox)\n",
        "sandbox.app()\n",
        "print(\"✅ Sandbox refactored: each tab is its own function\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InoaY8Snm3JG",
        "outputId": "7bcdc211-2f84-496f-adea-a70e6de311c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 10:10:39.597 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.598 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.599 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.600 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.607 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.609 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.611 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.627 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.630 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.632 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.634 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.635 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.635 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.637 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.642 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.646 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.653 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.656 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.659 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.660 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.665 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.665 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.676 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.680 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.684 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.697 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.700 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.701 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.702 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:10:39.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sandbox refactored: each tab is its own function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- TAB 5: A/B Experiments ----------------\n",
        "def render_tab5():\n",
        "    st.markdown(\"### 🔹 A/B Model Experiments\")\n",
        "\n",
        "    fname = \"data/sandbox_batches/sandbox_latest.csv\"\n",
        "    if not os.path.exists(fname):\n",
        "        st.warning(\"⚠️ Please generate a dataset in Tab 1 first.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(fname)\n",
        "    st.success(f\"Loaded {fname} with {df.shape[0]} rows\")\n",
        "\n",
        "    # Select models to compare\n",
        "    models = [\"LogReg\", \"SGD\", \"XGBoost\", \"HybridBlend\"]\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        model_a = st.selectbox(\"Select Model A\", models, index=0)\n",
        "    with col2:\n",
        "        model_b = st.selectbox(\"Select Model B\", models, index=2)\n",
        "\n",
        "    if st.button(\"⚔️ Run A/B Experiment\"):\n",
        "        metrics_a, _, _ = engine.train_model(df, model_a)\n",
        "        metrics_b, _, _ = engine.train_model(df, model_b)\n",
        "\n",
        "        st.write(f\"### {model_a}\")\n",
        "        st.json(metrics_a)\n",
        "\n",
        "        st.write(f\"### {model_b}\")\n",
        "        st.json(metrics_b)\n",
        "\n",
        "        # Quick comparison\n",
        "        comparison = pd.DataFrame([metrics_a, metrics_b], index=[model_a, model_b])\n",
        "        st.write(\"📊 Side-by-side comparison\")\n",
        "        st.dataframe(comparison)"
      ],
      "metadata": {
        "id": "qyPok_-6nTMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"1️⃣ Data Generation\",\n",
        "        \"2️⃣ Model Training\",\n",
        "        \"3️⃣ Stress Testing\",\n",
        "        \"4️⃣ Audit Logs\",\n",
        "        \"5️⃣ A/B Experiments\"\n",
        "    ])\n",
        "\n",
        "    with tabs[0]:\n",
        "        render_tab1()\n",
        "    with tabs[1]:\n",
        "        render_tab2()\n",
        "    with tabs[2]:\n",
        "        render_tab3()\n",
        "    with tabs[3]:\n",
        "        render_tab4()\n",
        "    with tabs[4]:\n",
        "        render_tab5()"
      ],
      "metadata": {
        "id": "JTv9rjqxnkzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pages._04_admin_sandbox as sandbox\n",
        "\n",
        "importlib.reload(sandbox)\n",
        "sandbox.app()\n",
        "print(\"✅ Sandbox app with Tab 5 registered cleanly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpXqsV7Unpyu",
        "outputId": "4a632ab8-6eb0-49b6-8cce-1f8b76a923dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-31 10:14:06.411 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.412 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.413 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.414 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.415 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.417 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.418 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.418 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.421 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.422 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.423 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.424 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.425 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.425 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.426 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.433 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.433 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.434 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.435 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.435 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.436 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.437 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.437 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.438 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.439 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.439 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.440 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.442 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.444 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.445 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.445 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.446 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.447 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.447 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.448 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.449 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.449 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.450 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.451 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.451 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.452 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.452 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.453 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.454 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.454 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.455 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.456 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.456 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.457 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.458 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.458 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.459 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.460 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.460 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.461 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.461 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.463 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.472 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.473 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.477 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.494 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.496 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.499 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.499 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.502 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.502 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.511 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-31 10:14:06.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sandbox app with Tab 5 registered cleanly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- TAB 6: User Management ----------------\n",
        "def render_tab6():\n",
        "    st.markdown(\"### 🔹 User Management (Impersonate / Edit / Delete)\")\n",
        "\n",
        "    db_path = \"data/audit.db\"\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create users table if not exists\n",
        "    cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS users (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        username TEXT UNIQUE,\n",
        "        role TEXT,\n",
        "        branch TEXT,\n",
        "        status TEXT DEFAULT 'active'\n",
        "    )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    # Seed users if empty\n",
        "    cur.execute(\"SELECT COUNT(*) FROM users\")\n",
        "    if cur.fetchone()[0] == 0:\n",
        "        cur.executemany(\n",
        "            \"INSERT OR IGNORE INTO users (username, role, branch) VALUES (?, ?, ?)\",\n",
        "            [\n",
        "                (\"admin\", \"superadmin\", \"HQ\"),\n",
        "                (\"analyst1\", \"analyst\", \"Nairobi\"),\n",
        "                (\"officer1\", \"loan_officer\", \"Mombasa\"),\n",
        "            ]\n",
        "        )\n",
        "        conn.commit()\n",
        "\n",
        "    # Load users\n",
        "    df_users = pd.read_sql(\"SELECT * FROM users\", conn)\n",
        "    st.dataframe(df_users)\n",
        "\n",
        "    # Select a user\n",
        "    selected_user = st.selectbox(\"Select a user\", df_users[\"username\"].tolist())\n",
        "    if selected_user:\n",
        "        user_data = df_users[df_users[\"username\"] == selected_user].iloc[0]\n",
        "        st.write(\"User details:\", user_data.to_dict())\n",
        "\n",
        "        # Impersonate\n",
        "        if st.button(f\"🎭 Impersonate {selected_user}\"):\n",
        "            st.session_state[\"active_user\"] = selected_user\n",
        "            cur.execute(\"INSERT INTO audit_logs (user, action, reason) VALUES (?, ?, ?)\",\n",
        "                        (\"admin\", f\"impersonate {selected_user}\", \"Admin action\"))\n",
        "            conn.commit()\n",
        "            st.success(f\"✅ Now impersonating {selected_user}\")\n",
        "\n",
        "        # Edit role & branch\n",
        "        new_role = st.text_input(\"Role\", value=user_data[\"role\"])\n",
        "        new_branch = st.text_input(\"Branch\", value=user_data[\"branch\"])\n",
        "        new_status = st.selectbox(\"Status\", [\"active\",\"suspended\",\"deleted\"], index=0)\n",
        "\n",
        "        if st.button(\"💾 Save Changes\"):\n",
        "            cur.execute(\n",
        "                \"UPDATE users SET role=?, branch=?, status=? WHERE username=?\",\n",
        "                (new_role, new_branch, new_status, selected_user)\n",
        "            )\n",
        "            cur.execute(\"INSERT INTO audit_logs (user, action, reason) VALUES (?, ?, ?)\",\n",
        "                        (\"admin\", f\"edit_user {selected_user}\", f\"Updated to {new_role}/{new_branch}/{new_status}\"))\n",
        "            conn.commit()\n",
        "            st.success(\"✅ User updated.\")\n",
        "\n",
        "        # Delete user\n",
        "        if st.button(f\"🗑️ Delete {selected_user}\"):\n",
        "            cur.execute(\"DELETE FROM users WHERE username=?\", (selected_user,))\n",
        "            cur.execute(\"INSERT INTO audit_logs (user, action, reason) VALUES (?, ?, ?)\",\n",
        "                        (\"admin\", f\"delete_user {selected_user}\", \"Admin action\"))\n",
        "            conn.commit()\n",
        "            st.warning(f\"⚠️ User {selected_user} deleted.\")\n",
        "\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "0HDVaD2Tn263"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def app():\n",
        "    st.title(\"🛠️ Admin Sandbox (Godmode)\")\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"1️⃣ Data Generation\",\n",
        "        \"2️⃣ Model Training\",\n",
        "        \"3️⃣ Stress Testing\",\n",
        "        \"4️⃣ Audit Logs\",\n",
        "        \"5️⃣ A/B Experiments\",\n",
        "        \"6️⃣ User Management\"\n",
        "    ])\n",
        "\n",
        "    with tabs[0]:\n",
        "        render_tab1()\n",
        "\n",
        "    with tabs[1]:\n",
        "        render_tab2()\n",
        "\n",
        "    with tabs[2]:\n",
        "        render_tab3()\n",
        "\n",
        "    with tabs[3]:\n",
        "        render_tab4()\n",
        "\n",
        "    with tabs[4]:\n",
        "        render_tab5()\n",
        "\n",
        "    with tabs[5]:\n",
        "        render_tab6()"
      ],
      "metadata": {
        "id": "-LOI--iNoPcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import pages._04_admin_sandbox as sandbox\n",
        "\n",
        "importlib.reload(sandbox)\n",
        "sandbox.app()\n",
        "print(\"✅ Sandbox with Tab 6 now runs cleanly\")"
      ],
      "metadata": {
        "id": "cpWrqhpSoUfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Install deps\n",
        "!pip -q install xgboost imbalanced-learn shap\n",
        "\n",
        "# Step 1: Wrap engine code\n",
        "engine_code = r'''\n",
        "# =========================================\n",
        "# modules/ml/engine.py\n",
        "# =========================================\n",
        "\n",
        "# 👉 # =========================================\n",
        "# ONE-CELL PATCH: modules/ml/engine.py\n",
        "# - Installs deps\n",
        "# - Writes full engine.py (rigorous training)\n",
        "# - Runs a quick self-test with your generator\n",
        "# =========================================\n",
        "\n",
        "# ---- Step 0: Deps (quiet) ----\n",
        "!pip -q install xgboost imbalanced-learn shap\n",
        "\n",
        "# ---- Step 1: Write engine.py ----\n",
        "%%writefile modules/ml/engine.py\n",
        "import os, time, json, joblib, warnings\n",
        "from dataclasses import dataclass, asdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import shap\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "TARGET_CANDIDATES = [\"default\", \"label\", \"y\", \"is_default\"]\n",
        "\n",
        "# -------------------------\n",
        "# Utilities / dataclasses\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class TrainMetrics:\n",
        "    algo: str\n",
        "    rows: int\n",
        "    features: int\n",
        "    accuracy: float\n",
        "    precision: float\n",
        "    recall: float\n",
        "    f1: float\n",
        "    auc: float\n",
        "    tn: int\n",
        "    fp: int\n",
        "    fn: int\n",
        "    tp: int\n",
        "    model_path: str\n",
        "    timestamp: str\n",
        "\n",
        "def _find_or_build_target(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Ensure df contains a binary target column named 'default'.\"\"\"\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    if \"default\" in cols:\n",
        "        # Just ensure int\n",
        "        df[\"default\"] = df[df.columns[cols.index(\"default\")]].astype(int)\n",
        "        return df\n",
        "\n",
        "    # Common pattern in your synthetic data\n",
        "    if \"loan_health\" in df.columns:\n",
        "        df = df.copy()\n",
        "        df[\"default\"] = (df[\"loan_health\"].str.lower() != \"performing\").astype(int)\n",
        "        return df\n",
        "\n",
        "    # Fallback: if any candidate exists, map to default\n",
        "    for c in TARGET_CANDIDATES:\n",
        "        if c in df.columns:\n",
        "            df = df.copy()\n",
        "            df[\"default\"] = df[c].astype(int)\n",
        "            return df\n",
        "\n",
        "    raise ValueError(\"No target column found. Provide 'default' or a 'loan_health' to infer from.\")\n",
        "\n",
        "def _split_features(df: pd.DataFrame):\n",
        "    \"\"\"Split columns into numeric vs categorical, dropping strong identifiers.\"\"\"\n",
        "    drop_cols = set([\n",
        "        \"customer_name\",\"name\",\"client_name\",\n",
        "        \"national_id\",\"id_number\",\"client_id\",\"ref_number\",\"ref\",\"reference\",\n",
        "        \"created_date\",\"created_at\",\"approval_date\"\n",
        "    ])\n",
        "    base_cols = [c for c in df.columns if c not in drop_cols and c != \"default\"]\n",
        "\n",
        "    # Identify numeric vs categorical\n",
        "    numeric_cols, cat_cols = [], []\n",
        "    for c in base_cols:\n",
        "        if pd.api.types.is_numeric_dtype(df[c]):\n",
        "            numeric_cols.append(c)\n",
        "        else:\n",
        "            cat_cols.append(c)\n",
        "    return numeric_cols, cat_cols\n",
        "\n",
        "def _build_preprocessor(numeric_cols, cat_cols):\n",
        "    \"\"\"ColumnTransformer with dense output to allow SMOTE.\"\"\"\n",
        "    numeric_t = ImbPipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "    cat_t = ImbPipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "    ])\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_t, numeric_cols),\n",
        "            (\"cat\", cat_t, cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.0\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "def _confmat_parts(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "    return tn, fp, fn, tp\n",
        "\n",
        "def _calc_metrics(algo, y_true, y_prob, y_pred, n_feats, model_path, rows):\n",
        "    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.5\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    tn, fp, fn, tp = _confmat_parts(y_true, y_pred)\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    return TrainMetrics(\n",
        "        algo=algo, rows=rows, features=n_feats,\n",
        "        accuracy=acc, precision=prec, recall=rec, f1=f1, auc=auc,\n",
        "        tn=tn, fp=fp, fn=fn, tp=tp, model_path=model_path, timestamp=ts\n",
        "    )\n",
        "\n",
        "def _save_registry_row(metrics: TrainMetrics):\n",
        "    reg_path = \"models/registry.csv\"\n",
        "    row = asdict(metrics)\n",
        "    df_row = pd.DataFrame([row])\n",
        "    if os.path.exists(reg_path):\n",
        "        df_row.to_csv(reg_path, mode=\"a\", header=False, index=False)\n",
        "    else:\n",
        "        df_row.to_csv(reg_path, index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Public API\n",
        "# -------------------------\n",
        "def train_model(df: pd.DataFrame, algo: str = \"LogReg\",\n",
        "                test_size: float = 0.2, val_size: float = 0.1, random_state: int = 42,\n",
        "                smote_k_neighbors: int = 5, max_train_rows: int | None = None,\n",
        "                lr: float = 0.05):\n",
        "    \"\"\"\n",
        "    Train a model on df and persist:\n",
        "      - Preprocessing (imputer/onehot/scaler)\n",
        "      - SMOTE for class imbalance\n",
        "      - Classifier (LogReg / SGD / XGBoost / HybridBlend)\n",
        "    Returns: (metrics:dict, shap_values:dict|None, model_path:str)\n",
        "    \"\"\"\n",
        "    if max_train_rows is not None and df.shape[0] > max_train_rows:\n",
        "        df = df.sample(n=max_train_rows, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    df = _find_or_build_target(df)\n",
        "    # Drop rows with missing target\n",
        "    df = df[df[\"default\"].isin([0,1])].copy()\n",
        "\n",
        "    # Train/val/test split (stratified)\n",
        "    X = df.drop(columns=[\"default\"])\n",
        "    y = df[\"default\"].astype(int)\n",
        "\n",
        "    strat = y if y.nunique() == 2 else None\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
        "    )\n",
        "    # carve validation from train_full\n",
        "    val_rel = val_size / (1 - test_size)\n",
        "    strat_tr = y_train_full if y_train_full.nunique() == 2 else None\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=val_rel, random_state=random_state, stratify=strat_tr\n",
        "    )\n",
        "\n",
        "    # Columns\n",
        "    num_cols, cat_cols = _split_features(pd.concat([X_train, X_val, X_test], axis=0))\n",
        "    pre = _build_preprocessor(num_cols, cat_cols)\n",
        "\n",
        "    # Classifier\n",
        "    if algo == \"LogReg\":\n",
        "        clf = LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=2.0, class_weight=\"balanced\")\n",
        "    elif algo == \"SGD\":\n",
        "        clf = SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\", alpha=1e-4, l1_ratio=0.15, class_weight=\"balanced\", max_iter=2000)\n",
        "    elif algo == \"XGBoost\":\n",
        "        clf = XGBClassifier(\n",
        "            n_estimators=400, max_depth=5, learning_rate=0.08, subsample=0.9, colsample_bytree=0.9,\n",
        "            reg_lambda=1.0, reg_alpha=0.0, tree_method=\"hist\", eval_metric=\"logloss\", random_state=random_state\n",
        "        )\n",
        "    elif algo == \"HybridBlend\":\n",
        "        # Train bases and return blended metrics\n",
        "        m_lr, _, p_lr = train_model(df.copy(), \"LogReg\", test_size, val_size, random_state, smote_k_neighbors, max_train_rows, lr)\n",
        "        m_sgd, _, p_sgd = train_model(df.copy(), \"SGD\", test_size, val_size, random_state, smote_k_neighbors, max_train_rows, lr)\n",
        "        m_xgb, _, p_xgb = train_model(df.copy(), \"XGBoost\", test_size, val_size, random_state, smote_k_neighbors, max_train_rows, lr)\n",
        "\n",
        "        # Load predictions on test for blending\n",
        "        model_lr = joblib.load(p_lr)\n",
        "        model_sgd = joblib.load(p_sgd)\n",
        "        model_xgb = joblib.load(p_xgb)\n",
        "\n",
        "        # Build a unified preprocessor to transform test set for all models safely:\n",
        "        # (Our saved pipelines already include preprocessing; just call predict_proba)\n",
        "        # Pick the model with the best validation AUC and give it more weight\n",
        "        w_lr = 0.25\n",
        "        w_sgd = 0.25\n",
        "        w_xgb = 0.50\n",
        "\n",
        "        # Need to reconstruct test from df splits; do a fresh split with same seed\n",
        "        df2 = _find_or_build_target(df.copy())\n",
        "        X2 = df2.drop(columns=[\"default\"])\n",
        "        y2 = df2[\"default\"].astype(int)\n",
        "        Xtr_full, Xte, ytr_full, yte = train_test_split(\n",
        "            X2, y2, test_size=test_size, random_state=random_state, stratify=(y2 if y2.nunique()==2 else None)\n",
        "        )\n",
        "\n",
        "        proba_lr = model_lr.predict_proba(Xte)[:,1]\n",
        "        proba_sgd = model_sgd.predict_proba(Xte)[:,1]\n",
        "        proba_xgb = model_xgb.predict_proba(Xte)[:,1]\n",
        "        blended = w_lr*proba_lr + w_sgd*proba_sgd + w_xgb*proba_xgb\n",
        "        y_pred = (blended >= 0.5).astype(int)\n",
        "\n",
        "        # Save blend metadata\n",
        "        ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        path = f\"models/HybridBlend_{ts}.pkl\"\n",
        "        meta = {\n",
        "            \"type\": \"HybridBlend\",\n",
        "            \"weights\": {\"LogReg\": w_lr, \"SGD\": w_sgd, \"XGBoost\": w_xgb},\n",
        "            \"components\": {\"LogReg\": p_lr, \"SGD\": p_sgd, \"XGBoost\": p_xgb},\n",
        "            \"created_at\": ts\n",
        "        }\n",
        "        joblib.dump(meta, path)\n",
        "\n",
        "        metrics = _calc_metrics(\"HybridBlend\", yte, blended, y_pred, n_feats=-1, model_path=path, rows=df.shape[0])\n",
        "        _save_registry_row(metrics)\n",
        "        return asdict(metrics), None, path\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown algo: {algo}\")\n",
        "\n",
        "    # Full pipeline: preprocess -> SMOTE -> classifier\n",
        "    pipe = ImbPipeline(steps=[\n",
        "        (\"pre\", pre),\n",
        "        (\"smote\", SMOTE(k_neighbors=smote_k_neighbors, random_state=random_state)),\n",
        "        (\"clf\", clf)\n",
        "    ])\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "    # Evaluate on test\n",
        "    y_prob = pipe.predict_proba(X_test)[:,1]\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_path = f\"models/{algo}_{ts}.pkl\"\n",
        "    joblib.dump(pipe, model_path)\n",
        "\n",
        "    # SHAP (lightweight): use a small background sample\n",
        "    shap_values = None\n",
        "    try:\n",
        "        # Sample 200 rows from test to compute SHAP\n",
        "        X_small = X_test.sample(min(200, X_test.shape[0]), random_state=0)\n",
        "        # Transform features\n",
        "        Xt_small = pipe.named_steps[\"pre\"].transform(X_small)\n",
        "        # Choose explainer type\n",
        "        if algo == \"XGBoost\":\n",
        "            explainer = shap.TreeExplainer(pipe.named_steps[\"clf\"])\n",
        "        else:\n",
        "            # KernelExplainer over decision function/proba; use dense background\n",
        "            background = shap.sample(Xt_small, 100)\n",
        "            # Define a prediction function that accepts pre-transformed arrays\n",
        "            model = pipe.named_steps[\"clf\"]\n",
        "            pred_fn = lambda A: model.predict_proba(A)[:,1]\n",
        "            explainer = shap.KernelExplainer(pred_fn, background)\n",
        "        # Compute SHAP on small set\n",
        "        sv = explainer.shap_values(pipe.named_steps[\"pre\"].transform(X_small))\n",
        "        # Store only summary stats (mean |shap|), to keep object small\n",
        "        mean_abs = np.mean(np.abs(sv), axis=0).tolist() if isinstance(sv, np.ndarray) else None\n",
        "        shap_values = {\"mean_abs_shap\": mean_abs}\n",
        "    except Exception as e:\n",
        "        shap_values = None  # SHAP is best-effort; never fail training\n",
        "\n",
        "    metrics = _calc_metrics(algo, y_test, y_prob, y_pred, n_feats=pipe.named_steps[\"pre\"].transform(X_train).shape[1],\n",
        "                            model_path=model_path, rows=df.shape[0])\n",
        "    _save_registry_row(metrics)\n",
        "    return asdict(metrics), shap_values, model_path\n",
        "\n",
        "def predict_sample(df: pd.DataFrame, algo: str = \"LogReg\", k: int = 10):\n",
        "    \"\"\"Load latest saved model for algo and return preds/scores on head(k).\"\"\"\n",
        "    if algo == \"HybridBlend\":\n",
        "        # Load meta and compute with components\n",
        "        metas = sorted([f for f in os.listdir(\"models\") if f.startswith(\"HybridBlend_\")])\n",
        "        if not metas:\n",
        "            raise FileNotFoundError(\"No HybridBlend model found. Train it first.\")\n",
        "        meta = joblib.load(os.path.join(\"models\", metas[-1]))\n",
        "        comp = meta[\"components\"]\n",
        "        w = meta[\"weights\"]\n",
        "\n",
        "        # Prepare sample\n",
        "        df2 = _find_or_build_target(df.copy())\n",
        "        X = df2.drop(columns=[\"default\"])\n",
        "        Xs = X.head(k)\n",
        "\n",
        "        pipe_lr  = joblib.load(comp[\"LogReg\"])\n",
        "        pipe_sgd = joblib.load(comp[\"SGD\"])\n",
        "        pipe_xgb = joblib.load(comp[\"XGBoost\"])\n",
        "\n",
        "        pr_lr  = pipe_lr.predict_proba(Xs)[:,1]\n",
        "        pr_sgd = pipe_sgd.predict_proba(Xs)[:,1]\n",
        "        pr_xgb = pipe_xgb.predict_proba(Xs)[:,1]\n",
        "        blended = w[\"LogReg\"]*pr_lr + w[\"SGD\"]*pr_sgd + w[\"XGBoost\"]*pr_xgb\n",
        "        preds = (blended >= 0.5).astype(int).tolist()\n",
        "        scores = blended.round(3).tolist()\n",
        "        return preds, scores\n",
        "\n",
        "    # else: single algo\n",
        "    files = sorted([f for f in os.listdir(\"models\") if f.startswith(f\"{algo}_\")])\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No saved model for {algo}. Train it first.\")\n",
        "    pipe = joblib.load(os.path.join(\"models\", files[-1]))\n",
        "\n",
        "    df2 = _find_or_build_target(df.copy())\n",
        "    Xs = df2.drop(columns=[\"default\"]).head(k)\n",
        "    scores = pipe.predict_proba(Xs)[:,1]\n",
        "    preds = (scores >= 0.5).astype(int).tolist()\n",
        "    return preds, scores.round(3).tolist()\n",
        "\n",
        "def stress_test(df: pd.DataFrame, shock: str = \"default\", intensity: float = 0.2) -> pd.DataFrame:\n",
        "    \"\"\"Simple stressors to create counterfactual datasets.\"\"\"\n",
        "    df = df.copy()\n",
        "    if shock == \"default\":\n",
        "        # Flip some performing to non-performing\n",
        "        mask = df[\"loan_health\"].str.lower() == \"performing\"\n",
        "        idx = df[mask].sample(frac=min(intensity, 0.9), random_state=1).index\n",
        "        df.loc[idx, \"loan_health\"] = \"nonperforming\"\n",
        "    elif shock == \"loan_amount\":\n",
        "        df[\"amount\"] = (df[\"amount\"] * (1.0 + intensity)).round(0)\n",
        "    elif shock == \"unemployment\":\n",
        "        # Proxy: reduce income stability and increase defaults\n",
        "        if \"income_stability\" in df.columns:\n",
        "            df[\"income_stability\"] = np.clip(df[\"income_stability\"] - intensity*0.5, 0, 1)\n",
        "        if \"loan_health\" in df.columns:\n",
        "            idx = df.sample(frac=min(0.1 + intensity*0.4, 0.9), random_state=2).index\n",
        "            df.loc[idx, \"loan_health\"] = \"nonperforming\"\n",
        "    return df\n",
        "\n",
        "# ---- End file ----\n",
        "\n",
        "# ---- Step 2: Quick self-test on synthetic data ----\n",
        "import importlib\n",
        "import pandas as pd\n",
        "from modules.synth import generators as g\n",
        "import modules.ml.engine as engine\n",
        "\n",
        "importlib.reload(g)\n",
        "importlib.reload(engine)\n",
        "\n",
        "# Generate a realistic dataset (you can tweak these)\n",
        "df = g.generate_clients_loans(\n",
        "    n_rows=8000,\n",
        "    default_rate=0.18,\n",
        "    gender_ratio=0.7,\n",
        "    loan_mean=28000,\n",
        "    branches=40,\n",
        "    product_weeks=6,\n",
        "    region_bias=\"balanced\",\n",
        "    seed=2\n",
        ")\n",
        "\n",
        "# Train all models, print headline metrics\n",
        "for algo in [\"LogReg\", \"SGD\", \"XGBoost\", \"HybridBlend\"]:\n",
        "    metrics, shap_info, path = engine.train_model(df, algo, random_state=7, smote_k_neighbors=5)\n",
        "    print(f\"\\n=== {algo} ===\")\n",
        "    print(\"Saved:\", path)\n",
        "    print(\"Accuracy:\", round(metrics[\"accuracy\"], 3),\n",
        "          \"Precision:\", round(metrics[\"precision\"], 3),\n",
        "          \"Recall:\", round(metrics[\"recall\"], 3),\n",
        "          \"AUC:\", round(metrics[\"auc\"], 3))\n",
        "    preds, scores = engine.predict_sample(df, algo, k=10)\n",
        "    print(\"Pred sample:\", preds)\n",
        "    print(\"Scores sample:\", scores)\n",
        "\n",
        "# Show last 5 registry rows (if any)\n",
        "reg_path = \"models/registry.csv\"\n",
        "if os.path.exists(reg_path):\n",
        "    tail = pd.read_csv(reg_path).tail(5)\n",
        "    print(\"\\n📒 Registry tail:\")\n",
        "    print(tail[[\"algo\",\"rows\",\"accuracy\",\"recall\",\"auc\",\"model_path\"]].to_string(index=False))\n",
        "else:\n",
        "    print(\"\\n(no registry yet)\")\n",
        "# (from the very first line \"import ...\" down to the very last line)\n",
        "# Do not delete its internal \"\"\"docstrings\"\"\"\n",
        "# Just drop the code here as-is\n",
        "\n",
        "# =========================================\n",
        "'''\n",
        "\n",
        "# Step 2: Save to file\n",
        "with open(\"modules/ml/engine.py\", \"w\") as f:\n",
        "    f.write(engine_code)\n",
        "\n",
        "print(\"✅ engine.py overwritten successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp4ebgc4v2x4",
        "outputId": "610ebfc4-bd79-4cb7-e90a-62f9d302abf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ engine.py overwritten successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import modules.synth.generators as g\n",
        "from modules.ml import engine\n",
        "\n",
        "# === Step 1: Generate synthetic dataset ===\n",
        "df = g.generate_clients_loans(\n",
        "    n_rows=5000,         # you can scale this up for realism\n",
        "    default_rate=0.15,   # 15% default, typical for microfinance\n",
        "    gender_ratio=0.65,   # 65% women borrowers\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"✅ Synthetic dataset ready:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# === Step 2: Train models ===\n",
        "results = {}\n",
        "for algo in [\"LogReg\", \"SGD\", \"XGBoost\"]:\n",
        "    metrics, shap_values, path = engine.train_model(df, algo, lr=0.1)\n",
        "    results[algo] = metrics\n",
        "    print(f\"\\n=== {algo} ===\")\n",
        "    print(\"Saved:\", path)\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"{k}: {v:.3f}\")\n",
        "\n",
        "# === Step 3: Train HybridBlend ===\n",
        "metrics, shap_values, path = engine.train_model(df, \"HybridBlend\")\n",
        "results[\"HybridBlend\"] = metrics\n",
        "print(f\"\\n=== HybridBlend ===\")\n",
        "print(\"Saved:\", path)\n",
        "for k,v in metrics.items():\n",
        "    print(f\"{k}: {v:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu843DdGwh0i",
        "outputId": "95db57b0-497a-4300-9e20-7200b8de97bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Synthetic dataset ready: (5000, 14)\n",
            "Columns: ['customer_id', 'customer_name', 'gov_id', 'age', 'gender', 'branch', 'product', 'product_weeks', 'loan_amount', 'loan_type', 'status', 'loan_health', 'debt_to_income', 'created_date']\n",
            "\n",
            "=== LogReg ===\n",
            "Saved: models/LogReg_model.pkl\n",
            "Accuracy: 0.845\n",
            "Precision: 0.000\n",
            "Recall: 0.000\n",
            "AUC: 0.526\n",
            "\n",
            "=== SGD ===\n",
            "Saved: models/SGD_model.pkl\n",
            "Accuracy: 0.845\n",
            "Precision: 0.000\n",
            "Recall: 0.000\n",
            "AUC: 0.500\n",
            "\n",
            "=== XGBoost ===\n",
            "Saved: models/XGBoost_model.pkl\n",
            "Accuracy: 0.845\n",
            "Precision: 0.000\n",
            "Recall: 0.000\n",
            "AUC: 0.732\n",
            "\n",
            "=== HybridBlend ===\n",
            "Saved: models/HybridBlend_model.pkl\n",
            "Accuracy: 0.845\n",
            "Precision: 0.000\n",
            "Recall: 0.000\n",
            "AUC: 0.723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [10:52:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "engine_code = r'''\n",
        "import os\n",
        "import joblib\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ======================================================\n",
        "# Utility\n",
        "# ======================================================\n",
        "\n",
        "def _prepare_xy(df: pd.DataFrame, target_col=\"default\", use_smote=False, seed=42):\n",
        "    \"\"\"Split dataframe into train/test with optional SMOTE rebalancing.\"\"\"\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # One-hot encode categoricals\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
        "    )\n",
        "\n",
        "    if use_smote:\n",
        "        sm = SMOTE(random_state=seed)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def _evaluate(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    try:\n",
        "        y_score = model.predict_proba(X_test)[:, 1]\n",
        "    except:\n",
        "        y_score = model.decision_function(X_test)\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"AUC\": roc_auc_score(y_test, y_score)\n",
        "    }, y_pred, y_score\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# Training\n",
        "# ======================================================\n",
        "\n",
        "def train_model(df, algo=\"LogReg\", lr=0.1, use_smote=True, seed=42):\n",
        "    \"\"\"Train a model (LogReg, SGD, XGBoost, HybridBlend).\"\"\"\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = _prepare_xy(df, use_smote=use_smote, seed=seed)\n",
        "\n",
        "    if algo == \"LogReg\":\n",
        "        pipe = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=False)),\n",
        "            (\"clf\", LogisticRegression(max_iter=1000))\n",
        "        ])\n",
        "        model = pipe.fit(X_train, y_train)\n",
        "\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(loss=\"log_loss\", max_iter=1000, random_state=seed)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = xgb.XGBClassifier(\n",
        "            n_estimators=200,\n",
        "            learning_rate=lr,\n",
        "            max_depth=6,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=seed,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric=\"auc\"\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    elif algo == \"HybridBlend\":\n",
        "        # Train 3 base models\n",
        "        logreg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "        sgd = SGDClassifier(loss=\"log_loss\", max_iter=1000, random_state=seed).fit(X_train, y_train)\n",
        "        xgbc = xgb.XGBClassifier(\n",
        "            n_estimators=200,\n",
        "            learning_rate=lr,\n",
        "            max_depth=6,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=seed,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric=\"auc\"\n",
        "        ).fit(X_train, y_train)\n",
        "\n",
        "        def predict_proba(X):\n",
        "            probs = (\n",
        "                logreg.predict_proba(X)[:,1] * 0.3 +\n",
        "                sgd.predict_proba(X)[:,1] * 0.2 +\n",
        "                xgbc.predict_proba(X)[:,1] * 0.5\n",
        "            )\n",
        "            return np.vstack([1-probs, probs]).T\n",
        "\n",
        "        class BlendWrapper:\n",
        "            def __init__(self, models): self.models = models\n",
        "            def predict(self, X): return (self.predict_proba(X)[:,1] > 0.5).astype(int)\n",
        "            def predict_proba(self, X): return predict_proba(X)\n",
        "\n",
        "        model = BlendWrapper([logreg, sgd, xgbc])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown algo: {algo}\")\n",
        "\n",
        "    metrics, y_pred, y_score = _evaluate(model, X_test, y_test)\n",
        "\n",
        "    # SHAP explainability (if tree-based)\n",
        "    try:\n",
        "        explainer = shap.Explainer(model, X_test)\n",
        "        shap_values = explainer(X_test)\n",
        "    except:\n",
        "        shap_values = None\n",
        "\n",
        "    out_path = f\"models/{algo}_model.pkl\"\n",
        "    joblib.dump(model, out_path)\n",
        "\n",
        "    return metrics, shap_values, out_path\n",
        "'''\n",
        "\n",
        "with open(\"modules/ml/engine.py\", \"w\") as f:\n",
        "    f.write(engine_code)\n",
        "\n",
        "print(\"✅ engine.py patched with SMOTE support\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRV1aMH3x-i7",
        "outputId": "316ab70a-02eb-4476-d976-d6aeafe16eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ engine.py patched with SMOTE support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Overwrite engine.py with SMOTE-enabled version ===\n",
        "engine_code = \"\"\"\n",
        "import os, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "\n",
        "# Optional SMOTE for imbalance handling\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    HAS_SMOTE = True\n",
        "except ImportError:\n",
        "    HAS_SMOTE = False\n",
        "\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "def prepare_data(df, target_col='loan_health', use_smote=False, seed=42):\n",
        "    y = (df[target_col] != \"performing\").astype(int)\n",
        "    X = df.drop(columns=['customer_id','customer_name','gov_id','created_date','loan_health'])\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "\n",
        "    if use_smote and HAS_SMOTE:\n",
        "        sm = SMOTE(random_state=seed)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, X.columns\n",
        "\n",
        "def train_model(df, algo=\"LogReg\", lr=0.1, use_smote=False, seed=42):\n",
        "    X_train, X_test, y_train, y_test, feat_names = prepare_data(df, use_smote=use_smote, seed=seed)\n",
        "\n",
        "    if algo == \"LogReg\":\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(loss=\"log_loss\", learning_rate=\"constant\", eta0=lr, max_iter=1000)\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=200, learning_rate=lr, max_depth=6,\n",
        "            subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\",\n",
        "            random_state=seed, use_label_encoder=False\n",
        "        )\n",
        "    elif algo == \"HybridBlend\":\n",
        "        # Train LogReg and XGBoost, blend predictions\n",
        "        log_metrics, _, _ = train_model(df, \"LogReg\", use_smote=use_smote, seed=seed)\n",
        "        xgb_metrics, _, _ = train_model(df, \"XGBoost\", use_smote=use_smote, seed=seed)\n",
        "        acc = (log_metrics[\"Accuracy\"] + xgb_metrics[\"Accuracy\"]) / 2\n",
        "        prec = (log_metrics[\"Precision\"] + xgb_metrics[\"Precision\"]) / 2\n",
        "        rec = (log_metrics[\"Recall\"] + xgb_metrics[\"Recall\"]) / 2\n",
        "        auc = (log_metrics[\"AUC\"] + xgb_metrics[\"AUC\"]) / 2\n",
        "        return {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"AUC\": auc}, None, None\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown algo: {algo}\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"Precision\": precision_score(y_test, preds, zero_division=0),\n",
        "        \"Recall\": recall_score(y_test, preds, zero_division=0),\n",
        "        \"AUC\": roc_auc_score(y_test, probs)\n",
        "    }\n",
        "\n",
        "    # SHAP explainability\n",
        "    explainer = shap.Explainer(model, X_train)\n",
        "    shap_values = explainer(X_test[:200])\n",
        "\n",
        "    model_path = os.path.join(MODEL_DIR, f\"{algo}_model.pkl\")\n",
        "    joblib.dump(model, model_path)\n",
        "\n",
        "    return metrics, shap_values, model_path\n",
        "\"\"\"\n",
        "\n",
        "with open(\"modules/ml/engine.py\", \"w\") as f:\n",
        "    f.write(engine_code)\n",
        "\n",
        "print(\"✅ engine.py overwritten with SMOTE-enabled version\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZhzbAljyuwK",
        "outputId": "c6b00ffd-56ae-478b-f1df-842a284acafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ engine.py overwritten with SMOTE-enabled version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib, modules.ml.engine as engine\n",
        "importlib.reload(engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m1lR2Tzy1im",
        "outputId": "66a51a8a-6d62-41a3-adcd-a8c2f9afa139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'modules.ml.engine' from '/content/modules/ml/engine.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/ml/engine.py\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import shap\n",
        "import xgboost as xgb\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "def preprocess(df: pd.DataFrame, target_col=\"default\"):\n",
        "    \"\"\"Split df into X, y and build preprocessing pipeline.\"\"\"\n",
        "    y = df[target_col].astype(int)\n",
        "    X = df.drop(columns=[target_col])\n",
        "\n",
        "    cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
        "\n",
        "    # Pipelines\n",
        "    cat_pipe = Pipeline([(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
        "    num_pipe = Pipeline([\n",
        "        (\"scaler\", StandardScaler())  # normalize numeric features\n",
        "    ])\n",
        "\n",
        "    preproc = ColumnTransformer([\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols)\n",
        "    ])\n",
        "\n",
        "    return X, y, preproc\n",
        "\n",
        "def train_model(df: pd.DataFrame, algo: str, use_smote=False, lr=0.1):\n",
        "    \"\"\"Train model with optional SMOTE and return metrics + SHAP values.\"\"\"\n",
        "    X, y, preproc = preprocess(df)\n",
        "\n",
        "    # Train/val split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    # Optionally apply SMOTE\n",
        "    if use_smote:\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train, y_train = sm.fit_resample(\n",
        "            pd.DataFrame(preproc.fit_transform(X_train).toarray()), y_train\n",
        "        )\n",
        "        preproc.fit(X)  # refit after resampling\n",
        "\n",
        "    # Models\n",
        "    if algo == \"LogReg\":\n",
        "        model = Pipeline([\n",
        "            (\"preproc\", preproc),\n",
        "            (\"clf\", LogisticRegression(\n",
        "                max_iter=2000,\n",
        "                class_weight=\"balanced\",\n",
        "                solver=\"lbfgs\"\n",
        "            ))\n",
        "        ])\n",
        "    elif algo == \"SGD\":\n",
        "        model = Pipeline([\n",
        "            (\"preproc\", preproc),\n",
        "            (\"clf\", SGDClassifier(\n",
        "                loss=\"log_loss\",\n",
        "                max_iter=2000,\n",
        "                class_weight=\"balanced\",\n",
        "                learning_rate=\"optimal\"\n",
        "            ))\n",
        "        ])\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = Pipeline([\n",
        "            (\"preproc\", preproc),\n",
        "            (\"clf\", xgb.XGBClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                eval_metric=\"auc\"\n",
        "            ))\n",
        "        ])\n",
        "    elif algo == \"HybridBlend\":\n",
        "        # Blend LogReg + XGB\n",
        "        logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "        xgbm = xgb.XGBClassifier(\n",
        "            n_estimators=200, max_depth=6, learning_rate=0.1,\n",
        "            subsample=0.8, colsample_bytree=0.8, random_state=42, eval_metric=\"auc\"\n",
        "        )\n",
        "        model = Pipeline([(\"preproc\", preproc), (\"clf\", logreg)])  # placeholder\n",
        "\n",
        "        # Train both separately\n",
        "        X_train_t = preproc.fit_transform(X_train)\n",
        "        X_test_t = preproc.transform(X_test)\n",
        "\n",
        "        logreg.fit(X_train_t, y_train)\n",
        "        xgbm.fit(X_train_t, y_train)\n",
        "\n",
        "        log_preds = logreg.predict_proba(X_test_t)[:,1]\n",
        "        xgb_preds = xgbm.predict_proba(X_test_t)[:,1]\n",
        "\n",
        "        blend = 0.5*log_preds + 0.5*xgb_preds\n",
        "        y_pred = (blend > 0.5).astype(int)\n",
        "\n",
        "        metrics = {\n",
        "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "            \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "            \"AUC\": roc_auc_score(y_test, blend)\n",
        "        }\n",
        "        path = f\"models/{algo}_model.pkl\"\n",
        "        pickle.dump({\"logreg\": logreg, \"xgb\": xgbm, \"preproc\": preproc}, open(path,\"wb\"))\n",
        "\n",
        "        # SHAP for blended (using xgb only)\n",
        "        shap_values = None\n",
        "        try:\n",
        "            X_for_shap = pd.DataFrame(X_test_t.toarray() if hasattr(X_test_t,\"toarray\") else X_test_t).astype(float)\n",
        "            explainer = shap.Explainer(xgbm, X_for_shap)\n",
        "            shap_values = explainer(X_for_shap[:200])\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ SHAP skipped (HybridBlend):\", e)\n",
        "        return metrics, shap_values, path\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown algo {algo}\")\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_scores = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"AUC\": roc_auc_score(y_test, y_scores)\n",
        "    }\n",
        "\n",
        "    # Save\n",
        "    path = f\"models/{algo}_model.pkl\"\n",
        "    pickle.dump(model, open(path,\"wb\"))\n",
        "\n",
        "    # SHAP\n",
        "    shap_values = None\n",
        "    try:\n",
        "        X_trans = model.named_steps[\"preproc\"].transform(X_test)\n",
        "        X_for_shap = pd.DataFrame(X_trans.toarray() if hasattr(X_trans,\"toarray\") else X_trans).astype(float)\n",
        "        explainer = shap.Explainer(model.named_steps[\"clf\"], X_for_shap)\n",
        "        shap_values = explainer(X_for_shap[:200])\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ SHAP skipped:\", e)\n",
        "\n",
        "    return metrics, shap_values, path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SifY0tszjTA",
        "outputId": "13cffc84-cb63-4360-c24d-400976f06d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/ml/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/ml/engine.py\n",
        "import os, time, pickle, sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "\n",
        "# === DB Setup ===\n",
        "os.makedirs(\"db\", exist_ok=True)\n",
        "DB_PATH = \"db/audit.db\"\n",
        "\n",
        "def init_audit_db():\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS model_audit (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        ts TEXT,\n",
        "        algo TEXT,\n",
        "        auc REAL,\n",
        "        acc REAL,\n",
        "        precision REAL,\n",
        "        recall REAL,\n",
        "        dataset TEXT,\n",
        "        model TEXT\n",
        "    )\"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_audit_db()\n",
        "\n",
        "# === Training Function ===\n",
        "def train_model(df: pd.DataFrame, algo: str=\"LogReg\", use_smote: bool=True):\n",
        "    \"\"\"Train model and log results to SQLite audit table.\"\"\"\n",
        "\n",
        "    # Ensure binary target\n",
        "    if \"default\" not in df.columns:\n",
        "        raise ValueError(\"Dataset must contain 'default' column (0/1).\")\n",
        "\n",
        "    # Split\n",
        "    X = df.drop(columns=[\"default\"])\n",
        "    X = pd.get_dummies(X, drop_first=True)   # encode categoricals\n",
        "    y = df[\"default\"].astype(int)\n",
        "\n",
        "    if use_smote:\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Select model\n",
        "    if algo == \"LogReg\":\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = xgb.XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)\n",
        "    elif algo == \"HybridBlend\":\n",
        "        # Blend: average predictions from LogReg + XGB\n",
        "        lr = LogisticRegression(max_iter=1000)\n",
        "        xgbm = xgb.XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)\n",
        "        lr.fit(X_train, y_train)\n",
        "        xgbm.fit(X_train, y_train)\n",
        "        preds_lr = lr.predict_proba(X_test)[:,1]\n",
        "        preds_xgb = xgbm.predict_proba(X_test)[:,1]\n",
        "        preds = (preds_lr + preds_xgb) / 2\n",
        "        y_pred = (preds > 0.5).astype(int)\n",
        "        auc = roc_auc_score(y_test, preds)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        metrics = {\"AUC\": auc, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec}\n",
        "        # Save hybrid model\n",
        "        ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        os.makedirs(\"models/versioned\", exist_ok=True)\n",
        "        model_path = f\"models/versioned/HybridBlend_{ts}.pkl\"\n",
        "        with open(model_path, \"wb\") as f:\n",
        "            pickle.dump((lr, xgbm), f)\n",
        "        shap_values = None\n",
        "        # Audit log\n",
        "        _log_audit(ts, algo, metrics, \"N/A\", model_path)\n",
        "        return metrics, shap_values, model_path\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown algo: {algo}\")\n",
        "\n",
        "    # Train + predict\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    metrics = {\"AUC\": auc, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec}\n",
        "\n",
        "    # Save model\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    os.makedirs(\"models/versioned\", exist_ok=True)\n",
        "    model_path = f\"models/versioned/{algo}_{ts}.pkl\"\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # SHAP values (only for tree/logreg)\n",
        "    shap_values = None\n",
        "    try:\n",
        "        explainer = shap.Explainer(model, X_test)\n",
        "        shap_values = explainer(X_test)\n",
        "    except Exception as e:\n",
        "        shap_values = None\n",
        "\n",
        "    # Audit log\n",
        "    dataset = f\"data/training_runs/trainset_{ts}.csv\"\n",
        "    _log_audit(ts, algo, metrics, dataset, model_path)\n",
        "\n",
        "    return metrics, shap_values, model_path\n",
        "\n",
        "\n",
        "def _log_audit(ts, algo, metrics, dataset_path, model_path):\n",
        "    \"\"\"Insert row into SQLite audit table.\"\"\"\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO model_audit (ts, algo, auc, acc, precision, recall, dataset, model)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        ts, algo,\n",
        "        metrics.get(\"AUC\", None),\n",
        "        metrics.get(\"Accuracy\", None),\n",
        "        metrics.get(\"Precision\", None),\n",
        "        metrics.get(\"Recall\", None),\n",
        "        dataset_path, model_path\n",
        "    ))\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvBKi80g1YTu",
        "outputId": "7203a245-5fd6-48b1-b9f6-1d847b4e0857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/ml/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/synth/generators.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "faker = Faker()\n",
        "\n",
        "# Kenyan branch sample (can expand)\n",
        "BRANCHES = [\n",
        "    \"Nairobi\", \"Mombasa\", \"Kisumu\", \"Nakuru\", \"Eldoret\",\n",
        "    \"Meru\", \"Nyeri\", \"Thika\", \"Machakos\", \"Embu\"\n",
        "]\n",
        "PRODUCTS = [\n",
        "    (\"Inuka\", 5), (\"Kuza\", 4), (\"Fadhili\", 6),\n",
        "    (\"Boresha\", 8), (\"Msingi\", 12)\n",
        "]\n",
        "\n",
        "def estimate_age_from_id(gov_id: str) -> int:\n",
        "    try:\n",
        "        prefix = int(str(gov_id)[:2])\n",
        "        if prefix < 20: return random.randint(60, 75)\n",
        "        elif prefix < 25: return random.randint(45, 59)\n",
        "        elif prefix < 30: return random.randint(35, 44)\n",
        "        elif prefix < 35: return random.randint(28, 34)\n",
        "        elif prefix < 40: return random.randint(21, 27)\n",
        "        else: return random.randint(18, 25)\n",
        "    except:\n",
        "        return random.randint(18, 60)\n",
        "\n",
        "def guess_gender_from_name(name: str) -> str:\n",
        "    return \"Female\" if name.endswith(\"a\") else \"Male\"\n",
        "\n",
        "def generate_clients_loans(n_rows:int=1000, seed:int|None=None,\n",
        "                           default_rate:float=0.1, gender_ratio:float=0.6) -> pd.DataFrame:\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        faker.seed_instance(seed)\n",
        "\n",
        "    rows = []\n",
        "    for i in range(n_rows):\n",
        "        name = faker.name()\n",
        "        gov_id = str(random.randint(2000000, 49999999))\n",
        "        age = estimate_age_from_id(gov_id)\n",
        "        gender = guess_gender_from_name(name)\n",
        "        branch = random.choice(BRANCHES)\n",
        "        product, weeks = random.choice(PRODUCTS)\n",
        "        amount = random.randint(5000, 200000)\n",
        "        loan_type = random.choice([\"Normal\", \"Top-Up\", \"Refinance\"])\n",
        "        status = random.choice([\"Active\", \"Pending Approval\"])\n",
        "        loan_health = \"defaulted\" if random.random() < default_rate else \"performing\"\n",
        "        debt_to_income = round(random.uniform(0.1, 0.8), 2)\n",
        "        created_date = faker.date_between(start_date=\"-2y\", end_date=\"today\")\n",
        "\n",
        "        rows.append({\n",
        "            \"customer_id\": f\"CUST{i+1:05d}\",\n",
        "            \"customer_name\": name,\n",
        "            \"gov_id\": gov_id,\n",
        "            \"age\": age,\n",
        "            \"gender\": gender,\n",
        "            \"branch\": branch,\n",
        "            \"product\": product,\n",
        "            \"product_weeks\": weeks,\n",
        "            \"loan_amount\": amount,\n",
        "            \"loan_type\": loan_type,\n",
        "            \"status\": status,\n",
        "            \"loan_health\": loan_health,\n",
        "            \"debt_to_income\": debt_to_income,\n",
        "            \"created_date\": created_date,\n",
        "            # 🔥 always include binary target\n",
        "            \"default\": 0 if loan_health == \"performing\" else 1\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fInX84Qi2ZvV",
        "outputId": "333250ad-9ba2-43ef-a5d6-1baaa5e58f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/synth/generators.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/synth/generators.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "faker = Faker()\n",
        "\n",
        "# Kenyan branch sample (expandable)\n",
        "BRANCHES = [\n",
        "    \"Nairobi\", \"Mombasa\", \"Kisumu\", \"Nakuru\", \"Eldoret\",\n",
        "    \"Meru\", \"Nyeri\", \"Thika\", \"Machakos\", \"Embu\"\n",
        "]\n",
        "PRODUCTS = [\n",
        "    (\"Inuka\", 5), (\"Kuza\", 4), (\"Fadhili\", 6),\n",
        "    (\"Boresha\", 8), (\"Msingi\", 12)\n",
        "]\n",
        "\n",
        "def estimate_age_from_id(gov_id: str) -> int:\n",
        "    try:\n",
        "        prefix = int(str(gov_id)[:2])\n",
        "        if prefix < 20: return random.randint(60, 75)\n",
        "        elif prefix < 25: return random.randint(45, 59)\n",
        "        elif prefix < 30: return random.randint(35, 44)\n",
        "        elif prefix < 35: return random.randint(28, 34)\n",
        "        elif prefix < 40: return random.randint(21, 27)\n",
        "        else: return random.randint(18, 25)\n",
        "    except:\n",
        "        return random.randint(18, 60)\n",
        "\n",
        "def guess_gender_from_name(name: str) -> str:\n",
        "    return \"Female\" if name.endswith(\"a\") else \"Male\"\n",
        "\n",
        "def generate_clients_loans(n_rows:int=1000, seed:int|None=None,\n",
        "                           default_rate:float=0.1, gender_ratio:float=0.6) -> pd.DataFrame:\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        faker.seed_instance(seed)\n",
        "\n",
        "    rows = []\n",
        "    for i in range(n_rows):\n",
        "        name = faker.name()\n",
        "        gov_id = str(random.randint(2000000, 49999999))\n",
        "        age = estimate_age_from_id(gov_id)\n",
        "        gender = guess_gender_from_name(name)\n",
        "        branch = random.choice(BRANCHES)\n",
        "        product, weeks = random.choice(PRODUCTS)\n",
        "        amount = random.randint(5000, 200000)\n",
        "        loan_type = random.choice([\"Normal\", \"Top-Up\", \"Refinance\"])\n",
        "        status = random.choice([\"Active\", \"Pending Approval\"])\n",
        "\n",
        "        # --- Dynamic default probability ---\n",
        "        p_default = default_rate\n",
        "\n",
        "        # Product risk (shorter terms riskier)\n",
        "        if weeks <= 5: p_default += 0.05\n",
        "        if weeks >= 12: p_default -= 0.02\n",
        "\n",
        "        # Loan type risk\n",
        "        if loan_type == \"Top-Up\": p_default += 0.03\n",
        "        elif loan_type == \"Refinance\": p_default += 0.05\n",
        "\n",
        "        # Branch risk differences\n",
        "        if branch in [\"Nairobi\", \"Mombasa\"]: p_default += 0.02\n",
        "        if branch in [\"Meru\", \"Nyeri\"]: p_default -= 0.01\n",
        "\n",
        "        # Age/gender adjustments\n",
        "        if age < 25: p_default += 0.04\n",
        "        if gender == \"Female\": p_default -= 0.02  # microfinance trend\n",
        "\n",
        "        p_default = max(0.01, min(0.8, p_default))  # keep sane bounds\n",
        "        loan_health = \"defaulted\" if random.random() < p_default else \"performing\"\n",
        "\n",
        "        debt_to_income = round(random.uniform(0.1, 0.8), 2)\n",
        "        created_date = faker.date_between(start_date=\"-2y\", end_date=\"today\")\n",
        "\n",
        "        rows.append({\n",
        "            \"customer_id\": f\"CUST{i+1:05d}\",\n",
        "            \"customer_name\": name,\n",
        "            \"gov_id\": gov_id,\n",
        "            \"age\": age,\n",
        "            \"gender\": gender,\n",
        "            \"branch\": branch,\n",
        "            \"product\": product,\n",
        "            \"product_weeks\": weeks,\n",
        "            \"loan_amount\": amount,\n",
        "            \"loan_type\": loan_type,\n",
        "            \"status\": status,\n",
        "            \"loan_health\": loan_health,\n",
        "            \"debt_to_income\": debt_to_income,\n",
        "            \"created_date\": created_date,\n",
        "            \"default\": 0 if loan_health == \"performing\" else 1\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "J7iz4Q3c3Cxe",
        "outputId": "575a9202-953f-47d5-b903-5f58321405ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modules/synth/generators.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import modules.synth.generators as g\n",
        "importlib.reload(g)   # make sure Colab reloads the newest generator\n",
        "\n",
        "# regenerate dataset (now with 'default')\n",
        "df = g.generate_clients_loans(n_rows=5000, seed=42, default_rate=0.15)\n",
        "print(\"✅ Dataset ready:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1VXGuqQ3-Yk",
        "outputId": "0a93037f-9ec1-4997-8b62-5cfaea32a200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset ready: (5000, 15)\n",
            "Columns: ['customer_id', 'customer_name', 'gov_id', 'age', 'gender', 'branch', 'product', 'product_weeks', 'loan_amount', 'loan_type', 'status', 'loan_health', 'debt_to_income', 'created_date', 'default']\n",
            "  customer_id   customer_name    gov_id  age gender   branch  product  \\\n",
            "0   CUST00001    Allison Hill  44911206   19   Male  Nairobi  Fadhili   \n",
            "1   CUST00002   Megan Mcclain  38598928   21   Male     Embu  Boresha   \n",
            "2   CUST00003  Allen Robinson   3780798   25   Male   Nakuru   Msingi   \n",
            "\n",
            "   product_weeks  loan_amount loan_type            status loan_health  \\\n",
            "0              6        69196    Normal            Active  performing   \n",
            "1              8        13331    Normal            Active  performing   \n",
            "2             12       114974    Normal  Pending Approval  performing   \n",
            "\n",
            "   debt_to_income created_date  default  \n",
            "0            0.57   2024-02-10        0  \n",
            "1            0.45   2023-11-02        0  \n",
            "2            0.67   2024-09-03        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# --- Utility: ensure target exists ---\n",
        "def ensure_target(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Ensure df contains a binary target column named 'default'.\"\"\"\n",
        "    if \"default\" not in df.columns:\n",
        "        raise ValueError(\"Dataset must contain 'default' column (0/1).\")\n",
        "    return df\n",
        "\n",
        "# --- Training Engine ---\n",
        "def train_model(df: pd.DataFrame, algo: str = \"LogReg\", use_smote: bool = True, lr: float = 0.01):\n",
        "    \"\"\"\n",
        "    Train a model with SMOTE option.\n",
        "    Supports: LogReg, SGD, XGBoost, HybridBlend\n",
        "    \"\"\"\n",
        "    df = ensure_target(df)\n",
        "\n",
        "    # Split\n",
        "    X = df.drop(columns=[\"default\"])\n",
        "    y = df[\"default\"]\n",
        "\n",
        "    # Encode categoricals\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Handle imbalance\n",
        "    if use_smote:\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Pick model\n",
        "    if algo == \"LogReg\":\n",
        "        model = LogisticRegression(max_iter=200, solver=\"lbfgs\")\n",
        "    elif algo == \"SGD\":\n",
        "        model = SGDClassifier(loss=\"log_loss\", max_iter=1000, tol=1e-3, random_state=42)\n",
        "    elif algo == \"XGBoost\":\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=200,\n",
        "            learning_rate=lr,\n",
        "            max_depth=5,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            eval_metric=\"logloss\",\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "    elif algo == \"HybridBlend\":\n",
        "        # Train individual sub-models first\n",
        "        m1 = LogisticRegression(max_iter=200, solver=\"lbfgs\")\n",
        "        m2 = XGBClassifier(\n",
        "            n_estimators=200,\n",
        "            learning_rate=lr,\n",
        "            max_depth=5,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            eval_metric=\"logloss\",\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "        m1.fit(X_train, y_train)\n",
        "        m2.fit(X_train, y_train)\n",
        "\n",
        "        def hybrid_predict(X_in):\n",
        "            p1 = m1.predict_proba(X_in)[:, 1]\n",
        "            p2 = m2.predict_proba(X_in)[:, 1]\n",
        "            return (p1 + p2) / 2\n",
        "\n",
        "        model = (m1, m2, hybrid_predict)  # store tuple for hybrid\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported algo: {algo}\")\n",
        "\n",
        "    # --- Fit model (skip for hybrid since we already fit inside) ---\n",
        "    if algo != \"HybridBlend\":\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    # --- Prediction handling ---\n",
        "    if algo == \"HybridBlend\":\n",
        "        m1, m2, hybrid_fn = model\n",
        "        y_pred = (hybrid_fn(X_test) > 0.5).astype(int)\n",
        "        y_scores = hybrid_fn(X_test)\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_scores = model.predict_proba(X_test)[:, 1]\n",
        "        elif hasattr(model, \"decision_function\"):\n",
        "            scores = model.decision_function(X_test)\n",
        "            y_scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
        "        else:\n",
        "            y_scores = y_pred  # fallback\n",
        "\n",
        "    # --- Metrics ---\n",
        "    metrics = {\n",
        "        \"Accuracy\": round(accuracy_score(y_test, y_pred), 3),\n",
        "        \"Precision\": round(precision_score(y_test, y_pred, zero_division=0), 3),\n",
        "        \"Recall\": round(recall_score(y_test, y_pred, zero_division=0), 3),\n",
        "        \"AUC\": round(roc_auc_score(y_test, y_scores), 3),\n",
        "    }\n",
        "\n",
        "    # --- SHAP values (tree-based only) ---\n",
        "    shap_values = None\n",
        "    try:\n",
        "        if algo == \"XGBoost\":\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_test)\n",
        "    except Exception:\n",
        "        shap_values = None\n",
        "\n",
        "    # --- Save model ---\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    path = f\"models/{algo}_model.pkl\"\n",
        "    joblib.dump(model, path)\n",
        "\n",
        "    return metrics, shap_values, path"
      ],
      "metadata": {
        "id": "BVivMh_j7-y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok -q"
      ],
      "metadata": {
        "id": "7INfD1l5Wo77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any tunnels from before\n",
        "ngrok.kill()\n",
        "\n",
        "# Auth\n",
        "NGROK_TOKEN = \"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\"\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Start tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🌍 Your app is live at:\", public_url)\n",
        "\n",
        "# Start Streamlit in background + log output\n",
        "!nohup streamlit run modules/streamlit_app/app.py --server.port 8501 > streamlit.log 2>&1 &\n",
        "\n",
        "# Wait a bit for startup\n",
        "time.sleep(5)\n",
        "\n",
        "# Show first 30 log lines\n",
        "!head -n 30 streamlit.log"
      ],
      "metadata": {
        "id": "wF8Teiz9Wwuf",
        "outputId": "a6e4aaf2-c6de-4ceb-950d-90465d15bebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌍 Your app is live at: NgrokTunnel: \"https://e05d4326736b.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
            "Try 'streamlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: modules/streamlit_app/app.py\n"
          ]
        }
      ]
    }
  ]
}