{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzBMNfiwGgJofB0wn+V10Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JBlizzard-sketch/LoanIQ/blob/main/Copy_of_LoanIQUpdated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OppiOadESUL7",
        "outputId": "2513a4e3-8ff9-4d56-d3dd-b4932feaf0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ],
      "source": [
        "# -------- Cell 1: Dependencies & Minimal Setup (patched with Faker) --------\n",
        "# Installs pinned versions and includes faker (safe: no reinstalls if already present)\n",
        "\n",
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "python -m pip install --quiet --upgrade \\\n",
        "  \"numpy==1.26.4\" \\\n",
        "  \"scikit-learn==1.3.2\" \\\n",
        "  \"xgboost==1.7.6\" \\\n",
        "  \"shap==0.41.0\" \\\n",
        "  \"pyngrok==5.2.1\" \\\n",
        "  \"streamlit\" \\\n",
        "  \"pandas\" \\\n",
        "  \"imbalanced-learn\" \\\n",
        "  \"faker\" \\\n",
        "  \"joblib\" \\\n",
        "  \"sqlalchemy\" \\\n",
        "  \"openpyxl\" \\\n",
        "  \"python-dotenv\" \\\n",
        "  || true\n",
        "\n",
        "# --- Create persistent folder structure ---\n",
        "mkdir -p /content/loan_app/modules/synth \\\n",
        "         /content/loan_app/modules/ml \\\n",
        "         /content/loan_app/modules/streamlit_app \\\n",
        "         /content/loan_app/data \\\n",
        "         /content/loan_app/models \\\n",
        "         /content/loan_app/logs\n",
        "\n",
        "# --- Write ngrok authtoken ---\n",
        "NGROK_TOKEN=\"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\"\n",
        "mkdir -p ~/.ngrok2\n",
        "cat > ~/.ngrok2/ngrok.yml <<NGY\n",
        "authtoken: ${NGROK_TOKEN}\n",
        "NGY\n",
        "export NGROK_AUTHTOKEN=\"${NGROK_TOKEN}\"\n",
        "\n",
        "# --- Quick version check ---\n",
        "python - <<'PY'\n",
        "import importlib\n",
        "pkgs = [\"numpy\",\"sklearn\",\"xgboost\",\"shap\",\"pandas\",\"streamlit\",\"pyngrok\",\"imbalanced_learn\",\"faker\"]\n",
        "for p in pkgs:\n",
        "    try:\n",
        "        m = importlib.import_module(p)\n",
        "        v = getattr(m, \"__version__\", None)\n",
        "        print(f\"{p}: {v}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{p}: IMPORT ERROR -> {e}\")\n",
        "PY\n",
        "\n",
        "echo \"‚úÖ Dependencies pinned. ‚úÖ Faker included. ‚úÖ Repo folders created. ‚úÖ Ngrok token set.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------- Cell 2/9 (CLEAN & COMPRESSED): Write Repo Modules --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "MODULES = BASE / \"modules\"\n",
        "SYNTH_DIR = MODULES / \"synth\"\n",
        "ML_DIR = MODULES / \"ml\"\n",
        "STREAMLIT_DIR = MODULES / \"streamlit_app\"\n",
        "DATA_DIR, MODELS_DIR, LOGS_DIR = BASE/\"data\", BASE/\"models\", BASE/\"logs\"\n",
        "\n",
        "# Create folders\n",
        "for d in [BASE, MODULES, SYNTH_DIR, ML_DIR, STREAMLIT_DIR, DATA_DIR, MODELS_DIR, LOGS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "if str(BASE) not in sys.path:\n",
        "    sys.path.insert(0, str(BASE))\n",
        "\n",
        "def write_module(path: Path, code: str):\n",
        "    path.write_text(code.strip()+\"\\n\")\n",
        "    # reopen sanity check\n",
        "    with open(path) as f: f.read()\n",
        "\n",
        "# -------- 1) auth.py --------\n",
        "auth_py = r\"\"\"\n",
        "import sqlite3, hashlib, datetime, os\n",
        "DB_PATH = os.path.join('/content/loan_app','data','users.db')\n",
        "\n",
        "def init_db():\n",
        "    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('CREATE TABLE IF NOT EXISTS users (username TEXT PRIMARY KEY, password TEXT, role TEXT, created_at TEXT)')\n",
        "    conn.commit(); conn.close()\n",
        "    register_user('Admin','Shady868','admin')\n",
        "\n",
        "def hash_pw(pw): return hashlib.sha256(pw.encode()).hexdigest()\n",
        "\n",
        "def register_user(username, password, role='company'):\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('INSERT OR IGNORE INTO users VALUES (?,?,?,?)',\n",
        "        (username, hash_pw(password), role, datetime.datetime.utcnow().isoformat()))\n",
        "    conn.commit(); conn.close()\n",
        "\n",
        "def authenticate(username, password):\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('SELECT password, role FROM users WHERE username=?',(username,))\n",
        "    row = cur.fetchone(); conn.close()\n",
        "    return (row and row[0]==hash_pw(password), row[1] if row else None)\n",
        "\n",
        "def list_users():\n",
        "    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()\n",
        "    cur.execute('SELECT username, role, created_at FROM users')\n",
        "    rows = cur.fetchall(); conn.close(); return rows\n",
        "\"\"\"\n",
        "write_module(MODULES/\"auth.py\", auth_py)\n",
        "\n",
        "# -------- 2) schema.py (compressed but rich) --------\n",
        "schema_py = r\"\"\"\n",
        "import re, pandas as pd, numpy as np, random\n",
        "from datetime import datetime\n",
        "\n",
        "SYNONYMS = {\n",
        "  \"client_id\":[\"customer_id\",\"cust_id\",\"id\"],\n",
        "  \"national_id\":[\"id_no\",\"idnumber\",\"reg_no\",\"reg_number\"],\n",
        "  \"loan_amount\":[\"amount\",\"principal\"],\n",
        "  \"branch\":[\"office\",\"location\"],\n",
        "  \"product\":[\"loan_product\"],\n",
        "  \"loan_status\":[\"status\",\"performance\",\"default\",\"outcome\"],\n",
        "  \"created_date\":[\"created_at\",\"disbursed_date\",\"date\"],\n",
        "  \"income\":[\"monthly_income\",\"salary\"],\n",
        "  \"name\":[\"customer_name\",\"full_name\"]\n",
        "}\n",
        "MALE, FEMALE = {\"John\",\"Peter\",\"James\",\"Joseph\",\"Michael\",\"David\"}, {\"Mary\",\"Ann\",\"Jane\",\"Grace\",\"Lucy\",\"Sarah\"}\n",
        "TERM_OVERRIDES = {\"inuka\":4,\"fadhili\":6,\"kuza\":12,\"agriadvance\":16,\"flexiloan\":8,\"bizboost\":10}\n",
        "\n",
        "def normalize_columns(df):\n",
        "    df=df.copy()\n",
        "    df.columns=[re.sub(r'[^\\w]','_',c.strip().lower()) for c in df.columns]\n",
        "    for canon,vars in SYNONYMS.items():\n",
        "        for v in vars:\n",
        "            if v in df.columns and canon not in df.columns: df.rename(columns={v:canon},inplace=True)\n",
        "    return df\n",
        "\n",
        "def parse_money(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    if isinstance(x,(int,float)): return float(x)\n",
        "    s=str(x).lower().replace('kes','').replace('ksh','').replace(',','').strip()\n",
        "    if s.endswith('k'): return float(s[:-1])*1000\n",
        "    try: return float(re.sub(r'[^\\d.]','',s))\n",
        "    except: return np.nan\n",
        "\n",
        "def parse_date(x):\n",
        "    if pd.isna(x): return None\n",
        "    s=str(x).strip()\n",
        "    for fmt in (\"%Y-%m-%d\",\"%d/%m/%Y\",\"%m/%d/%Y\"):\n",
        "        try: return pd.to_datetime(datetime.strptime(s,fmt))\n",
        "        except: pass\n",
        "    return pd.to_datetime(s, errors='coerce')\n",
        "\n",
        "def estimate_age_from_id(nid):\n",
        "    try:\n",
        "        digits=int(str(nid)[:2])\n",
        "        if 32<=digits<=34: return random.randint(27,30)\n",
        "        if 29<=digits<=31: return random.randint(30,40)\n",
        "        if 35<=digits<=38: return random.randint(18,25)\n",
        "        return random.randint(25,50)\n",
        "    except: return random.randint(25,50)\n",
        "\n",
        "def gender_from_name(name):\n",
        "    if not isinstance(name,str): return 'unknown'\n",
        "    f=name.split()[0].capitalize()\n",
        "    if f in MALE: return 'male'\n",
        "    if f in FEMALE: return 'female'\n",
        "    return 'female' if f.endswith(('a','e')) else 'male'\n",
        "\n",
        "def term_weeks(prod):\n",
        "    if not isinstance(prod,str): return None\n",
        "    m=re.search(r'(\\d+)\\s*week',prod.lower())\n",
        "    if m: return int(m.group(1))\n",
        "    for k,v in TERM_OVERRIDES.items():\n",
        "        if k in prod.lower(): return v\n",
        "    return None\n",
        "\n",
        "def coerce_and_enrich(df):\n",
        "    df=normalize_columns(df)\n",
        "    if \"national_id\" in df: df[\"unique_client_id\"]=df[\"national_id\"].astype(str)\n",
        "    elif \"client_id\" in df: df[\"unique_client_id\"]=df[\"client_id\"].astype(str)\n",
        "    else: df[\"unique_client_id\"]=df.get(\"name\",\"\").astype(str)\n",
        "    if \"created_date\" in df: df[\"created_date_parsed\"]=df[\"created_date\"].apply(parse_date)\n",
        "    else: df[\"created_date_parsed\"]=pd.to_datetime(\"today\")\n",
        "    df[\"loan_amount_num\"]=df.get(\"loan_amount\",np.nan).apply(parse_money)\n",
        "    df[\"income_num\"]=df.get(\"income\",np.nan).apply(parse_money)\n",
        "    df[\"product_term_weeks\"]=df.get(\"product\").apply(term_weeks) if \"product\" in df else None\n",
        "    df[\"installment_size\"]=df.apply(lambda r:(r[\"loan_amount_num\"]/r[\"product_term_weeks\"]) if pd.notna(r[\"loan_amount_num\"]) and r.get(\"product_term_weeks\") else np.nan,axis=1)\n",
        "    df[\"gender_est\"]=df.get(\"name\",\"\").apply(gender_from_name) if \"name\" in df else \"unknown\"\n",
        "    df[\"age_est\"]=df.get(\"national_id\").apply(estimate_age_from_id) if \"national_id\" in df else None\n",
        "    df[\"loan_to_income\"]=df.apply(lambda r:(r[\"loan_amount_num\"]/r[\"income_num\"]) if r.get(\"income_num\",0)>0 else np.nan,axis=1)\n",
        "    df[\"is_young_borrower\"]=df[\"age_est\"].apply(lambda x: x<25 if pd.notna(x) else False)\n",
        "    df[\"high_risk_amount\"]=df[\"loan_amount_num\"].apply(lambda x: x>300000 if pd.notna(x) else False)\n",
        "    if \"loan_status\" in df:\n",
        "        df[\"is_default\"]=df[\"loan_status\"].astype(str).str.lower().str.contains(\"default\").astype(int)\n",
        "    else: df[\"is_default\"]=0\n",
        "    df[\"days_since_issue\"]=(pd.to_datetime(\"today\")-pd.to_datetime(df[\"created_date_parsed\"],errors=\"coerce\")).dt.days.fillna(0).astype(int)\n",
        "    return df\n",
        "\n",
        "def prepare_for_ml(df, target=\"is_default\", aggregate_by_client=False):\n",
        "    df_en=coerce_and_enrich(df)\n",
        "    num=[\"loan_amount_num\",\"income_num\",\"product_term_weeks\",\"installment_size\",\"loan_to_income\",\"age_est\",\"days_since_issue\"]\n",
        "    cat=[c for c in [\"branch\",\"product\",\"town\",\"gender_est\"] if c in df_en.columns]\n",
        "    X=df_en[num+cat].copy(); X[num]=X[num].fillna(0)\n",
        "    if cat: X=pd.get_dummies(X,columns=cat,drop_first=True)\n",
        "    y=df_en[target].astype(int)\n",
        "    if aggregate_by_client:\n",
        "        agg=X.groupby(df_en[\"unique_client_id\"]).agg([\"mean\",\"max\",\"min\",\"sum\",\"count\"])\n",
        "        agg.columns=[\"__\".join(col) for col in agg.columns]\n",
        "        y=y.groupby(df_en[\"unique_client_id\"]).max()\n",
        "        return agg,y,df_en\n",
        "    return X,y,df_en\n",
        "\"\"\"\n",
        "write_module(MODULES/\"schema.py\", schema_py)\n",
        "\n",
        "# -------- 3) synth/generator.py --------\n",
        "synth_py = r\"\"\"\n",
        "import random, pandas as pd\n",
        "from faker import Faker\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "faker=Faker(); KENYAN_TOWNS=[\"Nairobi\",\"Mombasa\",\"Kisumu\",\"Nakuru\",\"Eldoret\"]\n",
        "BRANCHES=[\"Nairobi Branch\",\"Mombasa Branch\",\"Kisumu Branch\",\"Nakuru Branch\"]\n",
        "PRODUCTS=[\"Inuka 4 weeks\",\"Fadhili 6 weeks\",\"Kuza 12 weeks\",\"AgriAdvance 16 weeks\",\"FlexiLoan 8 weeks\",\"BizBoost 10 weeks\"]\n",
        "\n",
        "def generate_id(prefix=None):\n",
        "    return str(prefix)+str(random.randint(0,999999))[:8] if prefix else str(random.randint(1000000,99999999))\n",
        "\n",
        "def generate_phone(): return \"+2547\"+str(random.randint(1000000,9999999))\n",
        "\n",
        "def make_dataset(n=2000, default_rate=0.25, seed=42, include_history=True):\n",
        "    random.seed(seed); rows=[]\n",
        "    for i in range(n):\n",
        "        name=faker.first_name()+\" \"+faker.last_name()\n",
        "        nid=generate_id(random.choice([32,33,34,35])) if random.random()<0.5 else generate_id()\n",
        "        branch,prod=random.choice(BRANCHES),random.choice(PRODUCTS)\n",
        "        income=random.choice([15000,20000,30000,50000,80000,120000,200000])\n",
        "        amount=random.randint(2000,600000)\n",
        "        created=datetime.utcnow()-timedelta(days=random.randint(0,730))\n",
        "        status=\"default\" if random.random()<default_rate else \"performing\"\n",
        "        rows.append(dict(client_id=i,name=name,national_id=nid,phone=generate_phone(),town=random.choice(KENYAN_TOWNS),\n",
        "                         branch=branch,product=prod,income=income,loan_amount=amount,loan_status=status,\n",
        "                         created_date=created.strftime(\"%Y-%m-%d\")))\n",
        "    return pd.DataFrame(rows)\n",
        "\"\"\"\n",
        "write_module(SYNTH_DIR/\"generator.py\", synth_py)\n",
        "\n",
        "# -------- 4) ml/engine.py --------\n",
        "ml_engine_py = r\"\"\"\n",
        "import os,json,joblib,hashlib,time\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "MODELS_DIR=\"/content/loan_app/models\"\n",
        "def _fingerprint(cols): return hashlib.sha1(\"|\".join(sorted(cols)).encode()).hexdigest()[:12]\n",
        "\n",
        "class HybridModel:\n",
        "    def __init__(self,models): self.models=models\n",
        "    def predict_proba(self,X):\n",
        "        arr=X.values if hasattr(X,\"values\") else np.asarray(X)\n",
        "        probs=[]\n",
        "        for m in self.models:\n",
        "            try: p=m.predict_proba(arr)[:,1]\n",
        "            except:\n",
        "                try: p=1/(1+np.exp(-m.decision_function(arr)))\n",
        "                except: p=np.zeros(arr.shape[0])\n",
        "            probs.append(p)\n",
        "        avg=np.mean(np.vstack(probs),axis=0)\n",
        "        return np.vstack([1-avg,avg]).T\n",
        "    def predict(self,X): return (self.predict_proba(X)[:,1]>0.5).astype(int)\n",
        "\n",
        "def train_baseline(X,y,name=\"baseline\",random_state=42):\n",
        "    Xt,Xv,yt,yv=train_test_split(X,y,test_size=0.25,random_state=random_state,stratify=y if len(set(y))>1 else None)\n",
        "    try: Xb,yb=SMOTE(random_state=random_state).fit_resample(Xt,yt)\n",
        "    except: Xb,yb=Xt,yt\n",
        "    lr=LogisticRegression(max_iter=1000)\n",
        "    sgd=CalibratedClassifierCV(SGDClassifier(max_iter=2000),cv=3)\n",
        "    xgb=XGBClassifier(use_label_encoder=False,eval_metric=\"logloss\",verbosity=0)\n",
        "    for m in [lr,sgd,xgb]:\n",
        "        try: m.fit(Xb,yb)\n",
        "        except: pass\n",
        "    hybrid=HybridModel([lr,sgd,xgb])\n",
        "    try: auc=float(roc_auc_score(yv,hybrid.predict_proba(Xv)[:,1]))\n",
        "    except: auc=0.0\n",
        "    meta={\"name\":name,\"created_at\":time.time(),\"features\":list(X.columns),\"fingerprint\":_fingerprint(X.columns),\"auc\":auc}\n",
        "    os.makedirs(MODELS_DIR,exist_ok=True)\n",
        "    joblib.dump(hybrid,f\"{MODELS_DIR}/{name}.pkl\")\n",
        "    json.dump(meta,open(f\"{MODELS_DIR}/{name}.meta.json\",\"w\"))\n",
        "    return {\"auc\":auc,\"model_path\":f\"{MODELS_DIR}/{name}.pkl\"}\n",
        "\"\"\"\n",
        "write_module(ML_DIR/\"engine.py\", ml_engine_py)\n",
        "\n",
        "# -------- 5) ml/utils.py --------\n",
        "ml_utils_py = r\"\"\"\n",
        "import shap, numpy as np\n",
        "from sklearn.metrics import roc_auc_score,accuracy_score,precision_recall_curve,auc\n",
        "def evaluate(model,X,y):\n",
        "    try:\n",
        "        p=model.predict_proba(X)[:,1]\n",
        "        prec,rec,_=precision_recall_curve(y,p)\n",
        "        return {\"auc\":float(roc_auc_score(y,p)),\"acc\":float(accuracy_score(y,(p>0.5).astype(int))),\n",
        "                \"pr_auc\":float(auc(rec,prec))}\n",
        "    except Exception as e: return {\"error\":str(e)}\n",
        "def explain(model,X,n=200):\n",
        "    try:\n",
        "        Xs=X.sample(min(n,len(X)))\n",
        "        base=model.models[2] if hasattr(model,\"models\") else model\n",
        "        return shap.Explainer(base,Xs)(Xs)\n",
        "    except Exception as e: return {\"error\":str(e)}\n",
        "\"\"\"\n",
        "write_module(ML_DIR/\"utils.py\", ml_utils_py)\n",
        "\n",
        "# -------- 6) ml/audit.py --------\n",
        "ml_audit_py = r\"\"\"\n",
        "import sqlite3,os,datetime\n",
        "DB_PATH=\"/content/loan_app/data/audit.db\"\n",
        "def init_audit():\n",
        "    os.makedirs(os.path.dirname(DB_PATH),exist_ok=True)\n",
        "    conn=sqlite3.connect(DB_PATH); cur=conn.cursor()\n",
        "    cur.execute('CREATE TABLE IF NOT EXISTS audit_log (ts TEXT,user TEXT,action TEXT,details TEXT)')\n",
        "    conn.commit(); conn.close()\n",
        "def log(user,action,details=\"\"):\n",
        "    conn=sqlite3.connect(DB_PATH); cur=conn.cursor()\n",
        "    cur.execute('INSERT INTO audit_log VALUES (?,?,?,?)',(datetime.datetime.utcnow().isoformat(),user,action,details))\n",
        "    conn.commit(); conn.close()\n",
        "\"\"\"\n",
        "write_module(ML_DIR/\"audit.py\", ml_audit_py)\n",
        "\n",
        "# -------- init files --------\n",
        "for p in [MODULES,SYNTH_DIR,ML_DIR,STREAMLIT_DIR]: (p/\"__init__.py\").write_text(\"# init\\n\")\n",
        "\n",
        "# -------- Print repo tree --------\n",
        "def print_tree(root=BASE,depth=3):\n",
        "    for p,_,files in os.walk(root):\n",
        "        level=p.replace(str(root),\"\").count(os.sep)\n",
        "        if level<=depth:\n",
        "            print(\"  \"*level+os.path.basename(p)+\"/\")\n",
        "            for f in files: print(\"  \"*level+\"  - \"+f)\n",
        "print_tree()\n",
        "\n",
        "# -------- Import checks --------\n",
        "for mod in [\"modules.auth\",\"modules.schema\",\"modules.synth.generator\",\"modules.ml.engine\",\"modules.ml.utils\",\"modules.ml.audit\"]:\n",
        "    try: importlib.import_module(mod); print(\"‚úÖ\",mod,\"OK\")\n",
        "    except Exception as e: print(\"‚ùå\",mod,\"->\",e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfv9Efi-YUWT",
        "outputId": "5b63827a-3c3f-41b5-8ea5-ae41e03ee7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loan_app/\n",
            "  data/\n",
            "  logs/\n",
            "  modules/\n",
            "    - auth.py\n",
            "    - __init__.py\n",
            "    - schema.py\n",
            "    ml/\n",
            "      - audit.py\n",
            "      - utils.py\n",
            "      - __init__.py\n",
            "      - engine.py\n",
            "    streamlit_app/\n",
            "      - __init__.py\n",
            "    synth/\n",
            "      - __init__.py\n",
            "      - generator.py\n",
            "  models/\n",
            "‚úÖ modules.auth OK\n",
            "‚ùå modules.schema -> numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "‚ùå modules.synth.generator -> numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "‚ùå modules.ml.engine -> No module named 'numpy.char'\n",
            "‚ùå modules.ml.utils -> numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "‚úÖ modules.ml.audit OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 3/9: Write Streamlit App Skeleton --------\n",
        "# Paste into Colab and run. This writes app.py into the repo and does a simple import-check.\n",
        "\n",
        "import os, textwrap\n",
        "BASE = \"/content/loan_app\"\n",
        "APP_DIR = os.path.join(BASE, \"modules\", \"streamlit_app\")\n",
        "os.makedirs(APP_DIR, exist_ok=True)\n",
        "APP_PATH = os.path.join(APP_DIR, \"app.py\")\n",
        "\n",
        "app_src = r'''\n",
        "\"\"\"\n",
        "Streamlit app skeleton for Loan IQ\n",
        "- Tabs: Login/Register | Upload & Ingest | Client Dashboard | Admin Sandbox | Global Insights | Reports\n",
        "- Defensive imports: if a module is missing, shows guidance in UI\n",
        "- Admin credentials preserved: Admin / Shady868\n",
        "\"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import os, io, json, time\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "DATA_DIR = BASE / \"data\"\n",
        "UPLOAD_DIR = DATA_DIR / \"uploads\"\n",
        "MODELS_DIR = BASE / \"models\"\n",
        "\n",
        "UPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -- defensive imports --\n",
        "missing = []\n",
        "try:\n",
        "    from modules import auth\n",
        "except Exception as e:\n",
        "    auth = None\n",
        "    missing.append((\"auth\", str(e)))\n",
        "try:\n",
        "    from modules import schema\n",
        "except Exception as e:\n",
        "    schema = None\n",
        "    missing.append((\"schema\", str(e)))\n",
        "try:\n",
        "    from modules.synth import generator as synth_generator\n",
        "except Exception as e:\n",
        "    synth_generator = None\n",
        "    missing.append((\"synth.generator\", str(e)))\n",
        "try:\n",
        "    from modules.ml import engine as ml_engine\n",
        "except Exception as e:\n",
        "    ml_engine = None\n",
        "    missing.append((\"ml.engine\", str(e)))\n",
        "try:\n",
        "    from modules.ml import utils as ml_utils\n",
        "except Exception as e:\n",
        "    ml_utils = None\n",
        "    missing.append((\"ml.utils\", str(e)))\n",
        "try:\n",
        "    from modules.ml import audit as ml_audit\n",
        "except Exception as e:\n",
        "    ml_audit = None\n",
        "    missing.append((\"ml.audit\", str(e)))\n",
        "\n",
        "# initialize DBs if available\n",
        "if auth:\n",
        "    try:\n",
        "        auth.init_db()\n",
        "    except Exception:\n",
        "        pass\n",
        "if ml_audit:\n",
        "    try:\n",
        "        ml_audit.init_audit_db()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def show_missing_modules():\n",
        "    st.error(\"Some backend modules failed to import. The app will be degraded. See details below.\")\n",
        "    for name, err in missing:\n",
        "        st.text(f\"{name}: {err}\")\n",
        "\n",
        "def save_upload(file_bytes, filename):\n",
        "    p = UPLOAD_DIR / filename\n",
        "    with open(p, \"wb\") as f:\n",
        "        f.write(file_bytes)\n",
        "    return str(p)\n",
        "\n",
        "def load_uploaded_dataframe(path):\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        if str(path).lower().endswith((\".xls\", \".xlsx\")):\n",
        "            return pd.read_excel(path)\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to load uploaded file: {e}\")\n",
        "        return None\n",
        "\n",
        "def ensure_models_folder():\n",
        "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ----------------- Auth helpers -----------------\n",
        "def login_flow():\n",
        "    st.subheader(\"Login\")\n",
        "    username = st.text_input(\"Username\")\n",
        "    password = st.text_input(\"Password\", type=\"password\")\n",
        "    if st.button(\"Login\"):\n",
        "        if auth:\n",
        "            ok, role = auth.authenticate(username, password)\n",
        "            if ok:\n",
        "                st.session_state['user'] = username\n",
        "                st.session_state['role'] = role\n",
        "                st.success(f\"Logged in as {username} ({role})\")\n",
        "                if ml_audit:\n",
        "                    ml_audit.log(username, \"login\", \"web login\")\n",
        "            else:\n",
        "                st.error(\"Invalid credentials\")\n",
        "        else:\n",
        "            st.error(\"Auth module missing\")\n",
        "\n",
        "def register_flow():\n",
        "    st.subheader(\"Register (company)\")\n",
        "    r_user = st.text_input(\"New username\", key=\"r_user\")\n",
        "    r_pass = st.text_input(\"New password\", type=\"password\", key=\"r_pass\")\n",
        "    if st.button(\"Register\"):\n",
        "        if auth:\n",
        "            auth.register_user(r_user, r_pass, role=\"company\")\n",
        "            st.success(\"User registered (company). Admin must approve if needed.\")\n",
        "        else:\n",
        "            st.error(\"Auth module missing\")\n",
        "\n",
        "# ----------------- Upload & Ingest -----------------\n",
        "def upload_and_preview():\n",
        "    st.header(\"Upload dataset (CSV/XLSX)\")\n",
        "    uploaded = st.file_uploader(\"Choose a file\", type=[\"csv\",\"xlsx\"])\n",
        "    if uploaded is not None:\n",
        "        bytes_data = uploaded.getvalue()\n",
        "        filename = uploaded.name\n",
        "        saved = save_upload(bytes_data, filename)\n",
        "        st.success(f\"Saved to {saved}\")\n",
        "        df = load_uploaded_dataframe(saved)\n",
        "        if df is not None:\n",
        "            st.write(\"Preview:\")\n",
        "            st.dataframe(df.head(10))\n",
        "            # attempt schema.prepare_for_ml if available\n",
        "            if schema:\n",
        "                try:\n",
        "                    X, y, enriched = schema.prepare_for_ml(df, aggregate_by_client=False)\n",
        "                    st.write(\"Enriched preview (first 10 rows):\")\n",
        "                    st.dataframe(enriched.head(10))\n",
        "                    st.write(\"Feature matrix shape:\", X.shape)\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"prepare_for_ml failed: {e}\")\n",
        "            else:\n",
        "                st.info(\"Schema module not available; raw preview shown.\")\n",
        "\n",
        "# ----------------- Client Dashboard -----------------\n",
        "def client_dashboard():\n",
        "    st.header(\"Client Dashboard\")\n",
        "    st.info(\"Lookup by national ID (unique identifier). This view supports multiple loans per ID.\")\n",
        "    nid = st.text_input(\"Enter National ID\")\n",
        "    if st.button(\"Lookup\"):\n",
        "        # naive search across uploads\n",
        "        found = []\n",
        "        for p in UPLOAD_DIR.glob(\"*\"):\n",
        "            try:\n",
        "                import pandas as pd\n",
        "                if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                    df = pd.read_excel(p)\n",
        "                else:\n",
        "                    df = pd.read_csv(p)\n",
        "                # standardize col names\n",
        "                if schema:\n",
        "                    df2 = schema.coerce_types_and_derive(df)\n",
        "                else:\n",
        "                    df2 = df\n",
        "                if \"national_id\" in df2.columns:\n",
        "                    matches = df2[df2[\"national_id\"].astype(str).str.contains(str(nid))]\n",
        "                    if not matches.empty:\n",
        "                        found.append((p.name, matches))\n",
        "            except Exception as e:\n",
        "                st.write(f\"Error reading {p}: {e}\")\n",
        "        if not found:\n",
        "            st.warning(\"No loans found for that ID in uploaded files.\")\n",
        "        else:\n",
        "            for fname, dfm in found:\n",
        "                st.subheader(f\"Matches in {fname}\")\n",
        "                st.dataframe(dfm.head(50))\n",
        "                # show aggregates\n",
        "                try:\n",
        "                    agg = dfm.groupby(\"national_id\").agg({\n",
        "                        \"loan_amount_num\":\"sum\",\n",
        "                        \"is_default\":\"max\",\n",
        "                        \"days_since_issue\":\"min\"\n",
        "                    })\n",
        "                    st.write(\"Aggregates:\")\n",
        "                    st.dataframe(agg)\n",
        "                except Exception as e:\n",
        "                    st.write(f\"Aggregation failed: {e}\")\n",
        "\n",
        "# ----------------- Admin Sandbox -----------------\n",
        "def admin_sandbox():\n",
        "    st.header(\"Admin Sandbox (admin only)\")\n",
        "    user = st.session_state.get(\"user\")\n",
        "    role = st.session_state.get(\"role\")\n",
        "    if user != \"Admin\":\n",
        "        st.warning(\"Admin sandbox is restricted to Admin user. Please login as Admin to continue.\")\n",
        "        return\n",
        "\n",
        "    st.subheader(\"Impersonation\")\n",
        "    imp = st.text_input(\"Impersonate username (type in username to impersonate)\")\n",
        "    if st.button(\"Impersonate\"):\n",
        "        if auth:\n",
        "            st.session_state[\"impersonate\"] = imp\n",
        "            st.success(f\"Now impersonating {imp}\")\n",
        "            if ml_audit: ml_audit.log(\"Admin\", \"impersonate\", imp)\n",
        "        else:\n",
        "            st.error(\"Auth module missing\")\n",
        "\n",
        "    st.subheader(\"Synthetic Data\")\n",
        "    n = st.number_input(\"Rows to generate\", min_value=100, max_value=20000, value=2000, step=100)\n",
        "    default_rate = st.slider(\"Default rate\", 0.0, 1.0, 0.25)\n",
        "    if st.button(\"Generate synthetic dataset\"):\n",
        "        if synth_generator:\n",
        "            df = synth_generator.make_dataset(int(n), default_rate=default_rate, include_history=True)\n",
        "            p = UPLOAD_DIR / f\"synthetic_{int(time.time())}.csv\"\n",
        "            df.to_csv(p, index=False)\n",
        "            st.success(f\"Generated synthetic dataset: {p}\")\n",
        "            st.dataframe(df.head())\n",
        "            if ml_audit: ml_audit.log(\"Admin\", \"synth_generate\", f\"{p}\")\n",
        "        else:\n",
        "            st.error(\"Synth generator missing\")\n",
        "\n",
        "    st.subheader(\"Model Training / Registry\")\n",
        "    model_name = st.text_input(\"Model name\", value=\"baseline\")\n",
        "    if st.button(\"Train baseline model\"):\n",
        "        # find most recent upload to use for training\n",
        "        files = sorted(UPLOAD_DIR.glob(\"*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        if not files:\n",
        "            st.error(\"No uploaded datasets found for training\")\n",
        "        else:\n",
        "            # load first file\n",
        "            import pandas as pd\n",
        "            p = files[0]\n",
        "            try:\n",
        "                if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                    df = pd.read_excel(p)\n",
        "                else:\n",
        "                    df = pd.read_csv(p)\n",
        "                st.info(f\"Using {p.name} for training\")\n",
        "                if schema:\n",
        "                    X, y, enriched = schema.prepare_for_ml(df, aggregate_by_client=False)\n",
        "                else:\n",
        "                    st.error(\"Schema module required for training\")\n",
        "                    return\n",
        "                st.write(\"Feature sample:\")\n",
        "                st.dataframe(X.head())\n",
        "                ensure_models_folder()\n",
        "                if ml_engine:\n",
        "                    res = ml_engine.train_baseline(X, y, name=model_name)\n",
        "                    st.success(f\"Training complete. AUC: {res.get('auc'):.4f}\")\n",
        "                    st.write(\"Model artifact:\", res.get(\"model_path\"))\n",
        "                    if ml_audit: ml_audit.log(st.session_state.get(\"user\",\"unknown\"), \"train_model\", json.dumps(res))\n",
        "                else:\n",
        "                    st.error(\"ML engine missing\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Training pipeline failed: {e}\")\n",
        "\n",
        "    st.subheader(\"Model Registry / Pin / Impersonate\")\n",
        "    # list models\n",
        "    models = [p for p in MODELS_DIR.glob(\"*.meta.json\")]\n",
        "    for m in models:\n",
        "        try:\n",
        "            meta = json.load(open(m))\n",
        "            st.write(meta)\n",
        "            if st.button(f\"Pin {meta.get('name')}\"):\n",
        "                # write production pointer\n",
        "                prod = MODELS_DIR / \"PROD_MODEL.txt\"\n",
        "                prod.write_text(meta.get(\"name\"))\n",
        "                st.success(f\"Pinned {meta.get('name')} as production\")\n",
        "                if ml_audit: ml_audit.log(\"Admin\", \"pin_model\", meta.get(\"name\"))\n",
        "        except Exception as e:\n",
        "            st.write(f\"Failed to read model meta {m}: {e}\")\n",
        "\n",
        "# ----------------- Global Insights -----------------\n",
        "def global_insights():\n",
        "    st.header(\"Global Insights\")\n",
        "    st.info(\"KPI snapshots across uploaded datasets\")\n",
        "    # simple aggregate across all uploads\n",
        "    import pandas as pd\n",
        "    dfs = []\n",
        "    for p in UPLOAD_DIR.glob(\"*\"):\n",
        "        try:\n",
        "            if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                df = pd.read_excel(p)\n",
        "            else:\n",
        "                df = pd.read_csv(p)\n",
        "            if schema:\n",
        "                df2 = schema.coerce_types_and_derive(df)\n",
        "                dfs.append(df2)\n",
        "        except Exception as e:\n",
        "            st.write(f\"Failed reading {p}: {e}\")\n",
        "    if not dfs:\n",
        "        st.warning(\"No enriched uploads available\")\n",
        "        return\n",
        "    full = pd.concat(dfs, ignore_index=True)\n",
        "    st.write(\"Combined data sample:\")\n",
        "    st.dataframe(full.head())\n",
        "    # KPIs\n",
        "    total_loans = len(full)\n",
        "    total_default = int(full[\"is_default\"].sum()) if \"is_default\" in full.columns else 0\n",
        "    avg_loan = float(full[\"loan_amount_num\"].mean()) if \"loan_amount_num\" in full.columns else 0.0\n",
        "    st.metric(\"Total loans\", total_loans)\n",
        "    st.metric(\"Total defaults\", total_default)\n",
        "    st.metric(\"Avg loan amount\", f\"{avg_loan:,.0f}\")\n",
        "\n",
        "# ----------------- Reports -----------------\n",
        "def reports_tab():\n",
        "    st.header(\"Reports & Exports\")\n",
        "    st.write(\"Export aggregated CSVs, per-client reports, or model predictions (CSV).\")\n",
        "    if st.button(\"Export client-level aggregates (CSV)\"):\n",
        "        import pandas as pd\n",
        "        dfs = []\n",
        "        for p in UPLOAD_DIR.glob(\"*\"):\n",
        "            try:\n",
        "                if str(p).lower().endswith((\".xls\",\".xlsx\")):\n",
        "                    df = pd.read_excel(p)\n",
        "                else:\n",
        "                    df = pd.read_csv(p)\n",
        "                if schema:\n",
        "                    X, y, enriched = schema.prepare_for_ml(df, aggregate_by_client=True)\n",
        "                    dfs.append(pd.concat([X.reset_index(), y.reset_index(drop=False)], axis=1))\n",
        "            except Exception as e:\n",
        "                st.write(f\"Skipping {p}: {e}\")\n",
        "        if not dfs:\n",
        "            st.warning(\"No data to export\")\n",
        "            return\n",
        "        out = pd.concat(dfs, ignore_index=True)\n",
        "        out_path = UPLOAD_DIR / f\"client_aggregates_{int(time.time())}.csv\"\n",
        "        out.to_csv(out_path, index=False)\n",
        "        st.success(f\"Exported: {out_path}\")\n",
        "\n",
        "# ----------------- Main layout -----------------\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Loan IQ\", layout=\"wide\")\n",
        "    st.title(\"Loan IQ ‚Äî Loan company portal + Admin Sandbox\")\n",
        "\n",
        "    if missing:\n",
        "        show_missing_modules()\n",
        "        st.stop()\n",
        "\n",
        "    # session init\n",
        "    if \"user\" not in st.session_state:\n",
        "        st.session_state[\"user\"] = None\n",
        "        st.session_state[\"role\"] = None\n",
        "    # top-level nav\n",
        "    menu = [\"Home / Login\", \"Upload & Ingest\", \"Client Dashboard\", \"Admin Sandbox\", \"Global Insights\", \"Reports\"]\n",
        "    choice = st.sidebar.selectbox(\"Menu\", menu)\n",
        "\n",
        "    if choice == \"Home / Login\":\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            login_flow()\n",
        "        with col2:\n",
        "            register_flow()\n",
        "        # quick links\n",
        "        st.write(\"Quick actions:\")\n",
        "        if st.button(\"Generate small synthetic (100 rows)\"):\n",
        "            if synth_generator:\n",
        "                df = synth_generator.make_dataset(100, default_rate=0.2)\n",
        "                p = UPLOAD_DIR / f\"synthetic_small_{int(time.time())}.csv\"\n",
        "                df.to_csv(p, index=False)\n",
        "                st.success(f\"Saved small synthetic to {p}\")\n",
        "            else:\n",
        "                st.error(\"Synth generator missing\")\n",
        "\n",
        "    elif choice == \"Upload & Ingest\":\n",
        "        upload_and_preview()\n",
        "\n",
        "    elif choice == \"Client Dashboard\":\n",
        "        client_dashboard()\n",
        "\n",
        "    elif choice == \"Admin Sandbox\":\n",
        "        admin_sandbox()\n",
        "\n",
        "    elif choice == \"Global Insights\":\n",
        "        global_insights()\n",
        "\n",
        "    elif choice == \"Reports\":\n",
        "        reports_tab()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open(APP_PATH, \"w\") as f:\n",
        "    f.write(app_src)\n",
        "\n",
        "print(\"Wrote Streamlit app to:\", APP_PATH)\n",
        "# quick import-check (non-executing)\n",
        "print(\"Attempting lightweight import checks for modules used by the app (no heavy work).\")\n",
        "errs = []\n",
        "try:\n",
        "    import importlib\n",
        "    importlib.invalidate_caches()\n",
        "    for mod in [\"modules.auth\", \"modules.schema\", \"modules.synth.generator\", \"modules.ml.engine\", \"modules.ml.utils\", \"modules.ml.audit\"]:\n",
        "        try:\n",
        "            importlib.import_module(mod)\n",
        "            print(\"Imported:\", mod)\n",
        "        except Exception as e:\n",
        "            print(\"Import failed:\", mod, \"->\", e)\n",
        "            errs.append((mod, str(e)))\n",
        "except Exception as e:\n",
        "    print(\"Import-check harness error:\", e)\n",
        "\n",
        "if errs:\n",
        "    print(\"\\\\nSome imports failed (they will show in the app). You can inspect the file at\", APP_PATH)\n",
        "else:\n",
        "    print(\"\\\\nAll light imports succeeded. App skeleton ready. Next: Cell 4 will add CLI helpers, ngrok helpers, and a small runner script.\")"
      ],
      "metadata": {
        "id": "Rsg20vJMYoId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6e4cce-b257-49b2-a5b5-f7a4989ec54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Streamlit app to: /content/loan_app/modules/streamlit_app/app.py\n",
            "Attempting lightweight import checks for modules used by the app (no heavy work).\n",
            "Imported: modules.auth\n",
            "Import failed: modules.schema -> numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "Import failed: modules.synth.generator -> numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "Import failed: modules.ml.engine -> No module named 'numpy.char'\n",
            "Import failed: modules.ml.utils -> numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "Imported: modules.ml.audit\n",
            "\\nSome imports failed (they will show in the app). You can inspect the file at /content/loan_app/modules/streamlit_app/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === One-Time Environment & App Patcher ===\n",
        "import os, sys, subprocess, importlib, shutil\n",
        "\n",
        "print(\"üîß Resetting environment...\")\n",
        "\n",
        "# --- 1. Fix Python paths & create base folders ---\n",
        "os.makedirs(\"modules/synth\", exist_ok=True)\n",
        "os.makedirs(\"modules/ml\", exist_ok=True)\n",
        "os.makedirs(\"modules/streamlit_app/pages\", exist_ok=True)\n",
        "os.makedirs(\"data/sandbox_batches\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "if \"modules\" not in sys.path:\n",
        "    sys.path.append(\"modules\")\n",
        "\n",
        "# --- 2. Install deps (skip if installed) ---\n",
        "def pip_install(package, version=None):\n",
        "    try:\n",
        "        if version:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package}=={version}\", \"--quiet\"])\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not install {package}: {e}\")\n",
        "\n",
        "print(\"üì¶ Ensuring dependencies are installed...\")\n",
        "deps = {\n",
        "    \"numpy\": \"1.26.4\",   # stable for sklearn/xgboost in Colab\n",
        "    \"pandas\": None,\n",
        "    \"scikit-learn\": None,\n",
        "    \"xgboost\": None,\n",
        "    \"shap\": None,\n",
        "    \"imbalanced-learn\": None,\n",
        "    \"faker\": None,\n",
        "    \"streamlit\": None,\n",
        "    \"pyngrok\": None,\n",
        "}\n",
        "for pkg, ver in deps.items():\n",
        "    pip_install(pkg, ver)\n",
        "\n",
        "# --- 3. Admin & Ngrok ---\n",
        "ADMIN_USER = \"Admin\"\n",
        "ADMIN_PASS = \"Shady868\"\n",
        "NGROK_AUTHTOKEN = \"31rYvgklL0EdX9bGLvTXc313efE_2GyDFGPUNAyFgB83bikTF\"\n",
        "\n",
        "# Save them for app use\n",
        "with open(\"modules/streamlit_app/config.py\", \"w\") as f:\n",
        "    f.write(f\"ADMIN_USER = '{ADMIN_USER}'\\n\")\n",
        "    f.write(f\"ADMIN_PASS = '{ADMIN_PASS}'\\n\")\n",
        "    f.write(f\"NGROK_AUTHTOKEN = '{NGROK_AUTHTOKEN}'\\n\")\n",
        "\n",
        "# --- 4. SQLite DB reset ---\n",
        "import sqlite3\n",
        "conn = sqlite3.connect(\"data/app.db\")\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Drop old tables (safe reset)\n",
        "for t in [\"users\", \"audit_logs\"]:\n",
        "    cur.execute(f\"DROP TABLE IF EXISTS {t}\")\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS users (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    username TEXT UNIQUE,\n",
        "    password TEXT,\n",
        "    role TEXT DEFAULT 'client'\n",
        ")\n",
        "\"\"\")\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS audit_logs (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    action TEXT,\n",
        "    user TEXT,\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        ")\n",
        "\"\"\")\n",
        "# Ensure admin is present\n",
        "import hashlib\n",
        "hashed_pw = hashlib.sha256(ADMIN_PASS.encode()).hexdigest()\n",
        "cur.execute(\"INSERT OR IGNORE INTO users (username, password, role) VALUES (?, ?, ?)\",\n",
        "            (ADMIN_USER, hashed_pw, \"admin\"))\n",
        "conn.commit()\n",
        "conn.close()\n",
        "print(\"üóÑÔ∏è Database ready with admin user.\")\n",
        "\n",
        "# --- 5. Kenyan Faker setup ---\n",
        "from faker import Faker\n",
        "from faker.providers import BaseProvider\n",
        "\n",
        "faker = Faker(\"en_US\")  # fallback locale\n",
        "\n",
        "class KenyanProvider(BaseProvider):\n",
        "    kenyan_first_names_male = [\n",
        "        \"Otieno\", \"Kamau\", \"Ochieng\", \"Mwangi\", \"Odhiambo\", \"Mutiso\"\n",
        "    ]\n",
        "    kenyan_first_names_female = [\n",
        "        \"Achieng\", \"Wanjiku\", \"Nyambura\", \"Atieno\", \"Naliaka\", \"Wairimu\"\n",
        "    ]\n",
        "    kenyan_last_names = [\n",
        "        \"Omondi\", \"Njoroge\", \"Mutua\", \"Omondi\", \"Chebet\", \"Barasa\"\n",
        "    ]\n",
        "    kenyan_branches = [\n",
        "        \"Nairobi\", \"Mombasa\", \"Kisumu\", \"Nakuru\", \"Eldoret\", \"Thika\"\n",
        "    ]\n",
        "\n",
        "    def kenyan_name(self):\n",
        "        if self.random_element([True, False]):\n",
        "            return f\"{self.random_element(self.kenyan_first_names_male)} {self.random_element(self.kenyan_last_names)}\"\n",
        "        else:\n",
        "            return f\"{self.random_element(self.kenyan_first_names_female)} {self.random_element(self.kenyan_last_names)}\"\n",
        "\n",
        "    def kenyan_branch(self):\n",
        "        return self.random_element(self.kenyan_branches)\n",
        "\n",
        "    def gov_id(self):\n",
        "        return str(self.random_int(min=10000000, max=99999999))\n",
        "\n",
        "faker.add_provider(KenyanProvider)\n",
        "\n",
        "# Test samples\n",
        "print(\"üåç Faker localized to Kenya with custom provider:\")\n",
        "print(\"Name:\", faker.kenyan_name())\n",
        "print(\"Branch:\", faker.kenyan_branch())\n",
        "print(\"Gov ID:\", faker.gov_id())\n",
        "# --- 7. Verify imports ---\n",
        "import modules.synth, modules.ml\n",
        "print(\"‚úÖ Modules folder intact and importable.\")\n",
        "\n",
        "print(\"\\nüöÄ PATCH COMPLETE. You're ready to continue building.\")\n",
        "print(\"üëâ Next: you can safely paste large code blocks (700-900 lines) without worrying about missing paths/errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urespi0O2cDk",
        "outputId": "efacbb64-dd91-49f8-aed8-96826d88a93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Resetting environment...\n",
            "üì¶ Ensuring dependencies are installed...\n",
            "üóÑÔ∏è Database ready with admin user.\n",
            "üåç Faker localized to Kenya with custom provider:\n",
            "Name: Achieng Barasa\n",
            "Branch: Kisumu\n",
            "Gov ID: 98271355\n",
            "‚úÖ Modules folder intact and importable.\n",
            "\n",
            "üöÄ PATCH COMPLETE. You're ready to continue building.\n",
            "üëâ Next: you can safely paste large code blocks (700-900 lines) without worrying about missing paths/errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 4/9: Runner & Ngrok --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "RUNNER = BASE / \"run_app.py\"\n",
        "APP_FILE = BASE / \"modules\" / \"streamlit_app\" / \"app.py\"\n",
        "\n",
        "runner_code = r\"\"\"\n",
        "import os, sys, time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "APP_PATH = \"/content/loan_app/modules/streamlit_app/app.py\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(APP_PATH):\n",
        "        print(\"‚ùå App not found:\", APP_PATH); sys.exit(1)\n",
        "\n",
        "    ngrok.kill()  # kill old tunnels\n",
        "\n",
        "    public_url=None\n",
        "    for attempt in range(5):\n",
        "        try:\n",
        "            tunnel=ngrok.connect(8501,\"http\"); public_url=tunnel.public_url; break\n",
        "        except Exception as e:\n",
        "            print(f\"ngrok attempt {attempt+1}/5 failed:\",e); time.sleep(3)\n",
        "    if not public_url:\n",
        "        print(\"‚ùå ngrok failed after retries\"); sys.exit(1)\n",
        "\n",
        "    print(f\"‚úÖ App will be live at: {public_url}\\\\n\")\n",
        "\n",
        "    os.system(f\"streamlit run {APP_PATH} --server.port 8501 --server.headless true\")\n",
        "\n",
        "if __name__==\"__main__\": main()\n",
        "\"\"\"\n",
        "RUNNER.write_text(runner_code.strip()+\"\\n\")\n",
        "\n",
        "# Quick smoke check\n",
        "if APP_FILE.exists():\n",
        "    print(\"‚úÖ Runner written:\", RUNNER)\n",
        "    print(\"üëâ Later, launch with: !python /content/loan_app/run_app.py\")\n",
        "else:\n",
        "    print(\"‚ùå Streamlit app missing, re-run Cell 3 first\")"
      ],
      "metadata": {
        "id": "9B-c9_mgY0k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac3be47-5b98-4aae-9810-e8c1a803d3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Runner written: /content/loan_app/run_app.py\n",
            "üëâ Later, launch with: !python /content/loan_app/run_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 5/9: Utility Scripts & Smoke Tests --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "UTILS_DIR = BASE / \"utils\"\n",
        "UTILS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- inspect_data.py ---\n",
        "inspect_code = r\"\"\"\n",
        "import sys, pandas as pd\n",
        "from modules import schema\n",
        "from pathlib import Path\n",
        "\n",
        "UPLOAD_DIR = Path(\"/content/loan_app/data/uploads\")\n",
        "\n",
        "def inspect_latest():\n",
        "    files=sorted(UPLOAD_DIR.glob(\"*\"), key=lambda p:p.stat().st_mtime, reverse=True)\n",
        "    if not files:\n",
        "        print(\"‚ùå No uploaded files to inspect.\"); return\n",
        "    f=files[0]; print(\"Inspecting:\",f)\n",
        "    df=pd.read_csv(f) if f.suffix==\".csv\" else pd.read_excel(f)\n",
        "    print(\"Raw shape:\",df.shape)\n",
        "    X,y,en=schema.prepare_for_ml(df)\n",
        "    print(\"Enriched shape:\",en.shape,\" Features:\",X.shape,\" Target balance:\",y.value_counts().to_dict())\n",
        "\n",
        "if __name__==\"__main__\": inspect_latest()\n",
        "\"\"\"\n",
        "(BASE/\"utils\"/\"inspect_data.py\").write_text(inspect_code.strip()+\"\\n\")\n",
        "\n",
        "# --- train_smoke.py ---\n",
        "train_code = r\"\"\"\n",
        "import sys, pandas as pd\n",
        "from modules.synth import generator\n",
        "from modules import schema\n",
        "from modules.ml import engine\n",
        "\n",
        "def smoke_train():\n",
        "    print(\"Generating synthetic dataset (500 rows)...\")\n",
        "    df=generator.make_dataset(500, default_rate=0.3)\n",
        "    X,y,en=schema.prepare_for_ml(df)\n",
        "    print(\"Training baseline model...\")\n",
        "    res=engine.train_baseline(X,y,name=\"smoke_test\")\n",
        "    print(\"‚úÖ Smoke train done. AUC:\",res.get(\"auc\"))\n",
        "\n",
        "if __name__==\"__main__\": smoke_train()\n",
        "\"\"\"\n",
        "(BASE/\"utils\"/\"train_smoke.py\").write_text(train_code.strip()+\"\\n\")\n",
        "\n",
        "# --- Smoke tests ---\n",
        "print(\"Running smoke test: inspect_data.py (no files yet expected)...\")\n",
        "os.system(\"python /content/loan_app/utils/inspect_data.py || true\")\n",
        "\n",
        "print(\"\\nRunning smoke test: train_smoke.py...\")\n",
        "os.system(\"python /content/loan_app/utils/train_smoke.py || true\")\n",
        "\n",
        "print(\"\\n‚úÖ Utilities ready. You can run them manually from Colab shell.\")"
      ],
      "metadata": {
        "id": "D5O6ZD9JjpWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8975b372-9b66-4cc9-ff14-3326e508890a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running smoke test: inspect_data.py (no files yet expected)...\n",
            "\n",
            "Running smoke test: train_smoke.py...\n",
            "\n",
            "‚úÖ Utilities ready. You can run them manually from Colab shell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 6/9: Enhanced Kenyan Synthetic Generator + Visuals (50 towns + regions) --------\n",
        "import os, sys, random\n",
        "from pathlib import Path\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "SYNTH_DIR = BASE / \"modules\" / \"synth\"\n",
        "VIS_DIR = BASE / \"modules\" / \"visuals\"\n",
        "LOG_PLOTS = BASE / \"logs\" / \"plots\"\n",
        "LOG_PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def write_mod(path: Path, code: str):\n",
        "    path.write_text(code.strip() + \"\\n\")\n",
        "    with open(path) as f: f.read()\n",
        "\n",
        "# ---------- enhanced generator ----------\n",
        "enhanced_gen = r\"\"\"\n",
        "import random\n",
        "from faker import Faker\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "faker = Faker()\n",
        "Faker.seed = lambda s: random.seed(s)\n",
        "\n",
        "# ~50 towns mapped to regions\n",
        "TOWN_REGION = {\n",
        "    \"Nairobi\":\"Nairobi\",\"Thika\":\"Central\",\"Nyeri\":\"Central\",\"Murang'a\":\"Central\",\"Kiambu\":\"Central\",\n",
        "    \"Machakos\":\"Eastern\",\"Embu\":\"Eastern\",\"Meru\":\"Eastern\",\"Kitui\":\"Eastern\",\"Mwingi\":\"Eastern\",\n",
        "    \"Nakuru\":\"Rift Valley\",\"Naivasha\":\"Rift Valley\",\"Kericho\":\"Rift Valley\",\"Eldoret\":\"Rift Valley\",\"Bomet\":\"Rift Valley\",\"Narok\":\"Rift Valley\",\"Kajiado\":\"Rift Valley\",\n",
        "    \"Kisumu\":\"Nyanza\",\"Kisii\":\"Nyanza\",\"Homabay\":\"Nyanza\",\"Migori\":\"Nyanza\",\"Siaya\":\"Nyanza\",\n",
        "    \"Mombasa\":\"Coast\",\"Kilifi\":\"Coast\",\"Malindi\":\"Coast\",\"Kwale\":\"Coast\",\"Lamu\":\"Coast\",\"Voi\":\"Coast\",\n",
        "    \"Garissa\":\"North Eastern\",\"Wajir\":\"North Eastern\",\"Mandera\":\"North Eastern\",\n",
        "    \"Kakamega\":\"Western\",\"Bungoma\":\"Western\",\"Busia\":\"Western\",\"Vihiga\":\"Western\",\"Trans Nzoia\":\"Western\",\n",
        "    \"Turkana\":\"Rift Valley\",\"West Pokot\":\"Rift Valley\",\"Isiolo\":\"Eastern\",\"Marsabit\":\"Eastern\",\"Samburu\":\"Rift Valley\",\n",
        "    \"Taita Taveta\":\"Coast\",\"Taveta\":\"Coast\",\"Loitoktok\":\"Rift Valley\",\"Gilgil\":\"Rift Valley\",\"Kerugoya\":\"Central\"\n",
        "}\n",
        "\n",
        "# Weighted regions: Nairobi, Eastern, Rift, Central get more weight\n",
        "REGION_WEIGHTS = {\n",
        "    \"Nairobi\":0.20,\"Eastern\":0.20,\"Rift Valley\":0.20,\"Central\":0.20,\n",
        "    \"Nyanza\":0.07,\"Western\":0.06,\"Coast\":0.05,\"North Eastern\":0.02\n",
        "}\n",
        "\n",
        "def _pick_town():\n",
        "    regions = list(REGION_WEIGHTS.keys())\n",
        "    region = random.choices(regions, weights=[REGION_WEIGHTS[r] for r in regions])[0]\n",
        "    towns = [t for t,r in TOWN_REGION.items() if r==region]\n",
        "    town = random.choice(towns)\n",
        "    return town, region\n",
        "\n",
        "def _skewed_loan_amount(min_k=5000, max_k=70000, skew=1.8):\n",
        "    r = random.random() ** skew\n",
        "    return int(min_k + r * (max_k - min_k))\n",
        "\n",
        "def generate_national_id(prefix_choices=None):\n",
        "    if prefix_choices is None: prefix_choices=[32,33,34,35,36]\n",
        "    prefix = str(random.choice(prefix_choices))\n",
        "    tail = str(random.randint(0,999999)).zfill(6)\n",
        "    return prefix + tail\n",
        "\n",
        "def make_dataset(n=2000, default_rate=0.20, multi_loan_frac=0.30, female_frac=0.60,\n",
        "                 loan_min=5000, loan_max=70000, seed=42):\n",
        "    random.seed(seed); rows=[]\n",
        "    for i in range(n):\n",
        "        gender = \"female\" if random.random() < female_frac else \"male\"\n",
        "        first = faker.first_name_female() if gender==\"female\" else faker.first_name_male()\n",
        "        last = faker.last_name()\n",
        "        name = f\"{first} {last}\"\n",
        "        nid = generate_national_id()\n",
        "        phone = \"+2547\" + str(random.randint(10000000,99999999))[1:9]\n",
        "        town, region = _pick_town()\n",
        "        product = random.choice([\"Inuka 4 weeks\",\"Fadhili 6 weeks\",\"Kuza 12 weeks\",\"AgriAdvance 16 weeks\",\"FlexiLoan 8 weeks\",\"BizBoost 10 weeks\",\"QuickPay 2 weeks\"])\n",
        "        income = random.choice([12000,15000,20000,25000,30000,40000,60000])\n",
        "        amount = _skewed_loan_amount(loan_min, loan_max, skew=1.8)\n",
        "        created = datetime.utcnow() - timedelta(days=random.randint(0,730))\n",
        "        dobias = default_rate + (0.02 if amount > 40000 else -0.01)\n",
        "        status = \"default\" if random.random() < max(0, min(0.95, dobias)) else \"performing\"\n",
        "        rows.append({\n",
        "            \"client_id\": i, \"name\": name, \"national_id\": nid, \"phone\": phone,\n",
        "            \"town\": town, \"region\": region, \"product\": product,\n",
        "            \"income\": income, \"loan_amount\": amount, \"loan_status\": status,\n",
        "            \"created_date\": created.strftime(\"%Y-%m-%d\"), \"gender\": gender\n",
        "        })\n",
        "        if random.random() < multi_loan_frac:\n",
        "            extra_amount = int(amount * random.uniform(0.3, 1.2))\n",
        "            created2 = created - timedelta(days=random.randint(30, 900))\n",
        "            rows.append({\n",
        "                \"client_id\": i, \"name\": name, \"national_id\": nid, \"phone\": phone,\n",
        "                \"town\": town, \"region\": region, \"product\": random.choice([\"Inuka 4 weeks\",\"Fadhili 6 weeks\",\"Kuza 12 weeks\",\"AgriAdvance 16 weeks\",\"FlexiLoan 8 weeks\",\"BizBoost 10 weeks\",\"QuickPay 2 weeks\"]),\n",
        "                \"income\": income, \"loan_amount\": extra_amount, \"loan_status\": \"performing\",\n",
        "                \"created_date\": created2.strftime(\"%Y-%m-%d\"), \"gender\": gender\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\"\"\"\n",
        "write_mod(SYNTH_DIR / \"enhanced_generator.py\", enhanced_gen)\n",
        "\n",
        "# ---------- visuals (same as before, package) ----------\n",
        "visuals_py = r\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "PLOT_DIR = Path('/content/loan_app/logs/plots')\n",
        "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def kpis_from_df(df):\n",
        "    total_loans = len(df)\n",
        "    defaults = int(df.get('loan_status', pd.Series()).astype(str).str.lower().str.contains('default').sum()) if 'loan_status' in df else int(df.get('is_default', pd.Series()).sum() if 'is_default' in df else 0)\n",
        "    avg_loan = float(df.get('loan_amount', pd.Series()).astype(float).mean()) if 'loan_amount' in df else 0.0\n",
        "    female_share = float((df.get('gender','').astype(str)=='female').mean()) if 'gender' in df else np.nan\n",
        "    multi_loans = df.groupby(df.get('national_id', df.get('client_id'))).size().gt(1).mean()\n",
        "    return {'total_loans':total_loans,'defaults':defaults,'avg_loan':avg_loan,'female_share':female_share,'multi_loan_frac':float(multi_loans)}\n",
        "\n",
        "def plot_loan_size_hist(df, filename='loan_size_hist.png'):\n",
        "    vals = df['loan_amount'].astype(float)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(vals, bins=30)\n",
        "    ax.set_title('Loan size distribution'); ax.set_xlabel('Loan amount'); ax.set_ylabel('Count')\n",
        "    out = PLOT_DIR / filename\n",
        "    fig.savefig(out, bbox_inches='tight'); plt.close(fig); return str(out)\n",
        "\n",
        "def plot_defaults_pie(df, filename='defaults_pie.png'):\n",
        "    status = df['loan_status'].astype(str).str.lower().apply(lambda s: 'default' if 'default' in s else 'performing')\n",
        "    counts = status.value_counts()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.pie(counts.values, labels=counts.index, autopct='%1.1f%%')\n",
        "    ax.set_title('Default vs Performing')\n",
        "    out = PLOT_DIR / filename\n",
        "    fig.savefig(out, bbox_inches='tight'); plt.close(fig); return str(out)\n",
        "\n",
        "def plot_time_series_by_month(df, filename='loans_by_month.png'):\n",
        "    if 'created_date' not in df.columns and 'created_date_parsed' in df.columns:\n",
        "        df['created_date'] = df['created_date_parsed']\n",
        "    df['created_date_parsed'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
        "    df['month'] = df['created_date_parsed'].dt.to_period('M')\n",
        "    counts = df.groupby('month').size()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(counts.index.to_timestamp(), counts.values)\n",
        "    ax.set_title('Loans by month'); ax.set_ylabel('Count')\n",
        "    out = PLOT_DIR / filename\n",
        "    fig.savefig(out, bbox_inches='tight'); plt.close(fig); return str(out)\n",
        "\n",
        "def safe_shap_plot(shap_values, feature_names=None, out_name='shap_summary.png'):\n",
        "    try:\n",
        "        import shap\n",
        "        fig = shap.plots.bar(shap_values, show=False)\n",
        "        out = PLOT_DIR / out_name\n",
        "        fig.figure.savefig(out, bbox_inches='tight')\n",
        "        return str(out)\n",
        "    except Exception as e:\n",
        "        p = PLOT_DIR / out_name\n",
        "        with open(p,'w') as f: f.write('shap failed: '+str(e))\n",
        "        return str(p)\n",
        "\"\"\"\n",
        "(VIS_DIR / \"__init__.py\").write_text(visuals_py.strip()+\"\\n\")\n",
        "\n",
        "# ---------- Smoke test ----------\n",
        "print(\"Running smoke test for enhanced generator + visuals (50 towns + regions)...\")\n",
        "try:\n",
        "    import importlib\n",
        "    importlib.invalidate_caches()\n",
        "    ig = importlib.import_module(\"modules.synth.enhanced_generator\")\n",
        "    import modules.visuals as vis\n",
        "    importlib.reload(vis)  # force reload so new functions appear\n",
        "    schema = importlib.import_module(\"modules.schema\")\n",
        "\n",
        "    df = ig.make_dataset(n=1000, default_rate=0.20, multi_loan_frac=0.30,\n",
        "                         female_frac=0.60, seed=123)\n",
        "    csvp = (BASE/\"data/uploads/enhanced_synth_regions.csv\")\n",
        "    df.to_csv(csvp, index=False)\n",
        "    print(\"Generated dataset saved to:\", csvp)\n",
        "\n",
        "    X,y,en = schema.prepare_for_ml(df)\n",
        "    print(\"Enriched shape:\", en.shape, \"Feature matrix:\", X.shape)\n",
        "\n",
        "    kpis = vis.kpis_from_df(df)\n",
        "    print(\"KPIs:\", kpis)\n",
        "\n",
        "    p1 = vis.plot_loan_size_hist(df)\n",
        "    p2 = vis.plot_defaults_pie(df)\n",
        "    p3 = vis.plot_time_series_by_month(df)\n",
        "    print(\"Plots written:\", p1, p2, p3)\n",
        "\n",
        "    print(\"Cell 6 smoke test: ‚úÖ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 6 smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "AhAcxeBZnVgp",
        "outputId": "2887f924-5d1a-418f-85f6-2dc01f0a1837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running smoke test for enhanced generator + visuals (50 towns + regions)...\n",
            "Cell 6 smoke test failed: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3633630404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvalidate_caches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modules.synth.enhanced_generator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisuals\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# force reload so new functions appear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/content/loan_app/modules/synth/enhanced_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 7/9: Client Reports + SHAP Integration (clean + fallback) --------\n",
        "import os, json, joblib, sys, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "REPORTS_DIR = BASE / \"modules\" / \"reports\"\n",
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "reports_py = r\"\"\"\n",
        "import os, pandas as pd, json, joblib, numpy as np, warnings\n",
        "from pathlib import Path\n",
        "from modules import schema\n",
        "from modules.ml import engine, utils\n",
        "from modules.visuals import kpis_from_df\n",
        "\n",
        "OUT_DIR = Path('/content/loan_app/logs/reports')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "warnings.filterwarnings(\"ignore\")  # suppress sklearn/xgboost spam\n",
        "\n",
        "def _fallback_features(model, X):\n",
        "    try:\n",
        "        if hasattr(model, \"coef_\"):\n",
        "            coefs = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
        "            top_idx = np.argsort(np.abs(coefs))[-5:]\n",
        "            return {X.columns[i]: float(coefs[i]) for i in top_idx}\n",
        "        elif hasattr(model, \"feature_importances_\"):\n",
        "            imps = model.feature_importances_\n",
        "            top_idx = np.argsort(imps)[-5:]\n",
        "            return {X.columns[i]: float(imps[i]) for i in top_idx}\n",
        "    except Exception as e:\n",
        "        return {\"explain_fallback_error\": str(e)}\n",
        "    return {\"explain_info\": \"No explainable features found.\"}\n",
        "\n",
        "def client_report(df, client_id, model_path=None):\n",
        "    df_en = schema.coerce_and_enrich(df)\n",
        "    cdf = df_en[df_en['unique_client_id'].astype(str)==str(client_id)]\n",
        "    if cdf.empty:\n",
        "        return {'error':'Client not found','client_id':client_id}\n",
        "\n",
        "    rep = {\n",
        "        'client_id': client_id,\n",
        "        'towns': list(cdf['town'].unique()) if 'town' in cdf.columns else [],\n",
        "        'regions': list(cdf['region'].unique()) if 'region' in cdf.columns else [],\n",
        "        'loans': len(cdf),\n",
        "        'total_amount': float(cdf['loan_amount_num'].sum()) if 'loan_amount_num' in cdf else None,\n",
        "        'avg_amount': float(cdf['loan_amount_num'].mean()) if 'loan_amount_num' in cdf else None,\n",
        "        'status_counts': cdf['loan_status'].value_counts().to_dict() if 'loan_status' in cdf else {}\n",
        "    }\n",
        "\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            model = joblib.load(model_path)\n",
        "            X,y,en = schema.prepare_for_ml(cdf)\n",
        "            if not X.empty:\n",
        "                prob = model.predict_proba(X)[:,1].mean()\n",
        "                rep['pred_default_prob'] = float(prob)\n",
        "                try:\n",
        "                    expl = utils.explain(model,X,n=min(50,len(X)))\n",
        "                    rep['shap_preview'] = str(expl)[:500]\n",
        "                except Exception:\n",
        "                    rep['shap_preview'] = _fallback_features(model,X)\n",
        "        except Exception as e:\n",
        "            rep['pred_error'] = str(e)\n",
        "\n",
        "    return rep\n",
        "\n",
        "def export_client_reports(df, model_path=None, out_csv='client_reports.csv'):\n",
        "    df_en = schema.coerce_and_enrich(df)\n",
        "    clients = df_en['unique_client_id'].unique()\n",
        "    rows=[client_report(df_en,cid,model_path) for cid in clients]\n",
        "    outp = OUT_DIR/out_csv\n",
        "    pd.DataFrame(rows).to_csv(outp,index=False)\n",
        "    return str(outp)\n",
        "\"\"\"\n",
        "(REPORTS_DIR/\"__init__.py\").write_text(\"# init\\n\")\n",
        "(REPORTS_DIR/\"reports.py\").write_text(reports_py.strip()+\"\\n\")\n",
        "\n",
        "# --- Smoke test ---\n",
        "print(\"Running smoke test for reports (clean + fallback)...\")\n",
        "try:\n",
        "    import importlib, warnings\n",
        "    sys.path.append(\"/content/loan_app\")\n",
        "    importlib.invalidate_caches()\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "    ig = importlib.import_module(\"modules.synth.enhanced_generator\")\n",
        "    schema = importlib.import_module(\"modules.schema\")\n",
        "    eng = importlib.import_module(\"modules.ml.engine\")\n",
        "    repmod = importlib.import_module(\"modules.reports.reports\")\n",
        "\n",
        "    # generate dataset\n",
        "    df = ig.make_dataset(300, seed=999)\n",
        "    X,y,en = schema.prepare_for_ml(df)\n",
        "\n",
        "    # train baseline\n",
        "    res = eng.train_baseline(X,y,name=\"report_test\")\n",
        "\n",
        "    # generate sample report\n",
        "    r = repmod.client_report(df, df['national_id'].iloc[0], res['model_path'])\n",
        "    print(\"Sample report:\", json.dumps(r,indent=2)[:300])\n",
        "\n",
        "    # export all\n",
        "    csvout = repmod.export_client_reports(en, res['model_path'])\n",
        "    print(\"Reports CSV saved:\", csvout)\n",
        "\n",
        "    print(\"Cell 7 smoke test: ‚úÖ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 7 smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "gdG8Ny8Nnx7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuFxvHW3s5Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 8a/9: Admin Tools Core (audit logs, model registry, exports) --------\n",
        "import os, sys, json, joblib, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "ADMIN_DIR = BASE / \"modules\" / \"admin\"\n",
        "LOG_DIR = BASE / \"logs\"\n",
        "ADMIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "admin_py = r\"\"\"\n",
        "import os, json, joblib, datetime\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "LOG_DIR = Path('/content/loan_app/logs')\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Audit Logging ----\n",
        "def audit_log(event, user=\"system\", meta=None):\n",
        "    ts = datetime.datetime.utcnow().isoformat()\n",
        "    log_entry = {\"ts\": ts, \"user\": user, \"event\": event, \"meta\": meta or {}}\n",
        "    logf = LOG_DIR/\"audit.log\"\n",
        "    with open(logf, \"a\") as f:\n",
        "        f.write(json.dumps(log_entry)+\"\\n\")\n",
        "    return log_entry\n",
        "\n",
        "# ---- Model Registry ----\n",
        "MODEL_REGISTRY = LOG_DIR/\"model_registry.json\"\n",
        "\n",
        "def register_model(name, model_path, metrics, user=\"admin\"):\n",
        "    registry = []\n",
        "    if MODEL_REGISTRY.exists():\n",
        "        try:\n",
        "            registry = json.loads(MODEL_REGISTRY.read_text())\n",
        "        except:\n",
        "            registry = []\n",
        "    entry = {\"name\": name, \"model_path\": model_path, \"metrics\": metrics,\n",
        "             \"user\": user, \"ts\": datetime.datetime.utcnow().isoformat()}\n",
        "    registry.append(entry)\n",
        "    MODEL_REGISTRY.write_text(json.dumps(registry, indent=2))\n",
        "    audit_log(\"register_model\", user=user, meta=entry)\n",
        "    return entry\n",
        "\n",
        "def list_models():\n",
        "    if MODEL_REGISTRY.exists():\n",
        "        return json.loads(MODEL_REGISTRY.read_text())\n",
        "    return []\n",
        "\n",
        "def pin_model(name):\n",
        "    models = list_models()\n",
        "    if not models: return None\n",
        "    latest = [m for m in models if m[\"name\"]==name]\n",
        "    if not latest: return None\n",
        "    pinned = latest[-1]\n",
        "    (LOG_DIR/\"pinned_model.json\").write_text(json.dumps(pinned, indent=2))\n",
        "    audit_log(\"pin_model\", meta=pinned)\n",
        "    return pinned\n",
        "\n",
        "def get_pinned_model():\n",
        "    p = LOG_DIR/\"pinned_model.json\"\n",
        "    if p.exists():\n",
        "        return json.loads(p.read_text())\n",
        "    return None\n",
        "\n",
        "# ---- Data Exports ----\n",
        "EXPORTS_DIR = LOG_DIR/\"exports\"\n",
        "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def export_dataset(df, name=\"dataset_export.csv\", fmt=\"csv\"):\n",
        "    outp = EXPORTS_DIR/name\n",
        "    if fmt==\"csv\":\n",
        "        df.to_csv(outp,index=False)\n",
        "    elif fmt==\"json\":\n",
        "        df.to_json(outp,orient=\"records\")\n",
        "    audit_log(\"export_dataset\", meta={\"file\": str(outp), \"fmt\": fmt})\n",
        "    return str(outp)\n",
        "\"\"\"\n",
        "(ADMIN_DIR/\"__init__.py\").write_text(\"# init\\n\")\n",
        "(ADMIN_DIR/\"admin_tools.py\").write_text(admin_py.strip()+\"\\n\")\n",
        "\n",
        "# --- Smoke test ---\n",
        "print(\"Running smoke test for admin tools...\")\n",
        "try:\n",
        "    import importlib\n",
        "    sys.path.append(\"/content/loan_app\")\n",
        "    importlib.invalidate_caches()\n",
        "    adm = importlib.import_module(\"modules.admin.admin_tools\")\n",
        "\n",
        "    # audit log test\n",
        "    e = adm.audit_log(\"smoke_test\", user=\"tester\", meta={\"note\":\"admin tools check\"})\n",
        "    print(\"Audit log entry:\", e)\n",
        "\n",
        "    # fake registry entry\n",
        "    reg = adm.register_model(\"demo_model\",\"/tmp/demo.pkl\",{\"auc\":0.75}, user=\"tester\")\n",
        "    print(\"Registered model:\", reg)\n",
        "\n",
        "    # pin model\n",
        "    pin = adm.pin_model(\"demo_model\")\n",
        "    print(\"Pinned model:\", pin)\n",
        "\n",
        "    # list models\n",
        "    print(\"Model list:\", adm.list_models())\n",
        "\n",
        "    # export dummy dataset\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({\"a\":[1,2,3],\"b\":[4,5,6]})\n",
        "    exp = adm.export_dataset(df,\"demo.csv\")\n",
        "    print(\"Exported dataset:\", exp)\n",
        "\n",
        "    print(\"Cell 8a smoke test: ‚úÖ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 8a smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "lThDO3jnnz4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Cell 8b/9: Global Analytics & Data Products (super-rich) --------\n",
        "import os, sys, warnings, json\n",
        "from pathlib import Path\n",
        "\n",
        "# ensure repo on path\n",
        "sys.path.append(\"/content/loan_app\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "ANALYTICS_DIR = BASE / \"modules\" / \"analytics\"\n",
        "EXPORTS_DIR = BASE / \"logs\" / \"exports\"\n",
        "PLOTS_DIR = BASE / \"logs\" / \"plots\"\n",
        "ANALYTICS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "analytics_py = r\"\"\"\n",
        "# modules.analytics - Global analytics, segments, cohorts, and data products\n",
        "import os, json, math\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from modules import schema\n",
        "from modules.visuals import plot_loan_size_hist, plot_defaults_pie, plot_time_series_by_month\n",
        "from datetime import datetime\n",
        "\n",
        "OUT_DIR = Path('/content/loan_app/logs/exports'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOT_DIR = Path('/content/loan_app/logs/plots'); PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _safe_load_uploads(limit=None):\n",
        "    UP = Path('/content/loan_app/data/uploads')\n",
        "    if not UP.exists(): return pd.DataFrame()\n",
        "    files = sorted(UP.glob('*'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    if limit: files = files[:limit]\n",
        "    dfs=[]\n",
        "    for p in files:\n",
        "        try:\n",
        "            if p.suffix.lower() in ['.csv']: df = pd.read_csv(p)\n",
        "            else: df = pd.read_excel(p)\n",
        "            dfs.append(df)\n",
        "        except Exception:\n",
        "            continue\n",
        "    if not dfs: return pd.DataFrame()\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "def build_master(enforce_enrich=True):\n",
        "    df = _safe_load_uploads(limit=20)\n",
        "    if df.empty:\n",
        "        # fallback: use synth generator if available\n",
        "        try:\n",
        "            from modules.synth.enhanced_generator import make_dataset\n",
        "            df = make_dataset(1000, seed=2024)\n",
        "        except Exception:\n",
        "            return pd.DataFrame()\n",
        "    if enforce_enrich:\n",
        "        X,y,en = schema.prepare_for_ml(df, aggregate_by_client=False)\n",
        "        return en\n",
        "    return df\n",
        "\n",
        "def compute_topline(df):\n",
        "    # topline KPIs\n",
        "    total_loans = len(df)\n",
        "    total_clients = df['unique_client_id'].nunique() if 'unique_client_id' in df.columns else df['national_id'].nunique()\n",
        "    defaults = int(df.get('is_default', pd.Series()).sum()) if 'is_default' in df.columns else int(df.get('loan_status',pd.Series()).astype(str).str.contains('default').sum())\n",
        "    avg_loan = float(df['loan_amount_num'].mean()) if 'loan_amount_num' in df.columns else float(df['loan_amount'].mean())\n",
        "    median_loan = float(df['loan_amount_num'].median()) if 'loan_amount_num' in df.columns else float(df['loan_amount'].median())\n",
        "    female_share = float((df.get('gender','').astype(str)=='female').mean()) if 'gender' in df.columns else None\n",
        "    multi_loan_frac = df.groupby('unique_client_id').size().gt(1).mean() if 'unique_client_id' in df.columns else None\n",
        "    return {\n",
        "        'total_loans': int(total_loans),\n",
        "        'total_clients': int(total_clients),\n",
        "        'defaults': int(defaults),\n",
        "        'default_rate': float(defaults / total_loans) if total_loans>0 else None,\n",
        "        'avg_loan': avg_loan,\n",
        "        'median_loan': median_loan,\n",
        "        'female_share': female_share,\n",
        "        'multi_loan_frac': float(multi_loan_frac) if multi_loan_frac is not None else None\n",
        "    }\n",
        "\n",
        "def segment_stats(df, by=['region','town','product','branch','gender']):\n",
        "    results = {}\n",
        "    for seg in by:\n",
        "        if seg not in df.columns: continue\n",
        "        g = df.groupby(seg).agg(\n",
        "            loans = ('loan_amount_num','count'),\n",
        "            avg_amount = ('loan_amount_num','mean'),\n",
        "            median_amount = ('loan_amount_num','median'),\n",
        "            defaults = ('is_default','sum'),\n",
        "        ).reset_index()\n",
        "        g['default_rate'] = g['defaults'] / g['loans']\n",
        "        results[seg] = g.sort_values('loans', ascending=False)\n",
        "    return results\n",
        "\n",
        "def client_lifetime_metrics(df):\n",
        "    # per-client aggregates that are attractive to buyers\n",
        "    grp = df.groupby('unique_client_id').agg(\n",
        "        loan_count = ('loan_amount_num','count'),\n",
        "        total_borrowed = ('loan_amount_num','sum'),\n",
        "        avg_loan = ('loan_amount_num','mean'),\n",
        "        max_loan = ('loan_amount_num','max'),\n",
        "        defaults = ('is_default','sum'),\n",
        "    ).reset_index()\n",
        "    grp['default_flag'] = (grp['defaults']>0).astype(int)\n",
        "    # add demographic aggregates (first town/region/gender)\n",
        "    firsts = df.sort_values('created_date_parsed').groupby('unique_client_id').first().reset_index()\n",
        "    for c in ['town','region','gender','age_est']:\n",
        "        if c in firsts.columns:\n",
        "            grp[c] = grp['unique_client_id'].map(firsts.set_index('unique_client_id')[c].to_dict())\n",
        "    return grp\n",
        "\n",
        "def cohort_analysis(df, cohort_period='M'):\n",
        "    # cohorts by month of first loan\n",
        "    df['created_date_parsed'] = pd.to_datetime(df['created_date_parsed'], errors='coerce')\n",
        "    first = df.sort_values('created_date_parsed').groupby('unique_client_id')['created_date_parsed'].min().reset_index()\n",
        "    first['cohort'] = first['created_date_parsed'].dt.to_period(cohort_period)\n",
        "    df = df.merge(first[['unique_client_id','cohort']], on='unique_client_id', how='left')\n",
        "    cohort_table = df.groupby(['cohort','is_default']).size().unstack(fill_value=0)\n",
        "    return cohort_table\n",
        "\n",
        "def correlation_and_stats(df):\n",
        "    # numeric correlations for feature discovery\n",
        "    numcols = [c for c in df.columns if c.endswith('_num') or c in ['age_est','installment_size','loan_to_income','days_since_issue']]\n",
        "    if not numcols: return {}\n",
        "    corr = df[numcols].corr().fillna(0)\n",
        "    stats = df[numcols].describe().to_dict()\n",
        "    return {'corr': corr, 'stats': stats}\n",
        "\n",
        "def top_clients(df, n=50):\n",
        "    grp = client_lifetime_metrics(df)\n",
        "    top = grp.sort_values('total_borrowed', ascending=False).head(n)\n",
        "    return top\n",
        "\n",
        "def export_data_products(df, prefix='data_product'):\n",
        "    # export client-level product, segment stats, and raw enriched file\n",
        "    cli = client_lifetime_metrics(df)\n",
        "    seg = segment_stats(df)\n",
        "    now = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "    paths = {}\n",
        "    cli_path = OUT_DIR/f'{prefix}_clients_{now}.csv'; cli.to_csv(cli_path,index=False); paths['clients']=str(cli_path)\n",
        "    # export segment tables\n",
        "    for k,v in seg.items():\n",
        "        p = OUT_DIR/f'{prefix}_segment_{k}_{now}.csv'; v.to_csv(p,index=False); paths[f'segment_{k}']=str(p)\n",
        "    # export raw enriched\n",
        "    rawp = OUT_DIR/f'{prefix}_raw_{now}.csv'; df.to_csv(rawp,index=False); paths['raw']=str(rawp)\n",
        "    return paths\n",
        "\n",
        "def generate_plots(df):\n",
        "    p1 = plot_loan_size_hist(df)\n",
        "    p2 = plot_defaults_pie(df)\n",
        "    p3 = plot_time_series_by_month(df)\n",
        "    return [p1,p2,p3]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# write module\n",
        "(ANALYTICS_DIR / \"__init__.py\").write_text(\"# analytics package\\n\")\n",
        "(ANALYTICS_DIR / \"analytics.py\").write_text(analytics_py.strip() + \"\\n\")\n",
        "\n",
        "# ---------- Smoke Test for analytics ----------\n",
        "print(\"Running Cell 8b smoke test (Global Analytics)...\")\n",
        "try:\n",
        "    import importlib, pandas as pd\n",
        "    importlib.invalidate_caches()\n",
        "    sys.path.append(\"/content/loan_app\")\n",
        "    an = importlib.import_module(\"modules.analytics.analytics\")\n",
        "\n",
        "    # build master enriched DF (uses uploads or falls back to synth)\n",
        "    df = an.build_master(enforce_enrich=True)\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"No data available (uploads missing and synth unavailable)\")\n",
        "\n",
        "    topline = an.compute_topline(df)\n",
        "    print(\"Topline KPIs:\", json.dumps(topline, indent=2))\n",
        "\n",
        "    segs = an.segment_stats(df, by=['region','town','product','branch','gender'])\n",
        "    # print short summary of top 5 regions\n",
        "    if 'region' in segs:\n",
        "        print(\"Top regions by loans:\")\n",
        "        print(segs['region'].head(5).to_string(index=False))\n",
        "\n",
        "    # client lifetime & top clients\n",
        "    clients = an.client_lifetime_metrics(df)\n",
        "    print(\"Client metrics sample (top 5):\")\n",
        "    print(clients.head(5).to_string(index=False))\n",
        "\n",
        "    # cohort table (small preview)\n",
        "    cohort = an.cohort_analysis(df)\n",
        "    print(\"Cohort table shape:\", cohort.shape)\n",
        "\n",
        "    # correlation summary\n",
        "    corr = an.correlation_and_stats(df)\n",
        "    print(\"Numeric features discovered:\", list(corr.get('stats',{}).keys())[:10])\n",
        "\n",
        "    # export data products\n",
        "    paths = an.export_data_products(df, prefix='prod')\n",
        "    print(\"Exported data products to:\", paths)\n",
        "\n",
        "    # generate plots\n",
        "    plots = an.generate_plots(df)\n",
        "    print(\"Saved plots:\", plots)\n",
        "\n",
        "    print(\"Cell 8b smoke test: ‚úÖ OK\")\n",
        "except Exception as e:\n",
        "    print(\"Cell 8b smoke test failed:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "Q1CFF59ita7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------- Cell 9a/10: Streamlit UI (patched with Refresh Link for Admin) --------\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "APP_DIR = BASE / \"modules\" / \"streamlit_app\"\n",
        "APP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ui_py = r\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd, importlib, sys\n",
        "sys.path.append('/content/loan_app')\n",
        "\n",
        "from modules import auth, schema\n",
        "from modules.reports import reports\n",
        "from modules.analytics import analytics\n",
        "from modules.admin import admin_tools\n",
        "from modules.visuals import plot_loan_size_hist, plot_defaults_pie, plot_time_series_by_month\n",
        "\n",
        "# Try importing new_link from runner if available\n",
        "try:\n",
        "    from __main__ import new_link\n",
        "except:\n",
        "    new_link = None\n",
        "\n",
        "st.set_page_config(page_title=\"Loan Analytics Portal\", layout=\"wide\")\n",
        "\n",
        "if \"user\" not in st.session_state:\n",
        "    st.session_state.user = None\n",
        "if \"role\" not in st.session_state:\n",
        "    st.session_state.role = None\n",
        "\n",
        "def login():\n",
        "    st.sidebar.subheader(\"Login\")\n",
        "    u = st.sidebar.text_input(\"Username\")\n",
        "    p = st.sidebar.text_input(\"Password\", type=\"password\")\n",
        "    if st.sidebar.button(\"Login\", use_container_width=True):\n",
        "        if auth.check_login(u,p):\n",
        "            st.session_state.user = u\n",
        "            if u.lower()==\"admin\": st.session_state.role=\"admin\"\n",
        "            else: st.session_state.role=\"client\"\n",
        "            admin_tools.audit_log(\"login\", user=u)\n",
        "            st.sidebar.success(f\"Welcome, {u}\")\n",
        "        else:\n",
        "            st.sidebar.error(\"Invalid credentials\")\n",
        "\n",
        "def logout():\n",
        "    st.session_state.user=None\n",
        "    st.session_state.role=None\n",
        "\n",
        "if not st.session_state.user:\n",
        "    st.title(\"Loan Analytics Portal\")\n",
        "    st.markdown(\"Please login to continue.\")\n",
        "    login()\n",
        "else:\n",
        "    st.sidebar.write(f\"Logged in as: {st.session_state.user}\")\n",
        "    if st.sidebar.button(\"Logout\", use_container_width=True):\n",
        "        logout()\n",
        "        st.rerun()\n",
        "\n",
        "    tabs = [\"Dashboard\",\"Reports\",\"Uploads\"]\n",
        "    if st.session_state.role==\"admin\":\n",
        "        tabs += [\"Admin Sandbox\",\"Global Analytics\"]\n",
        "\n",
        "    choice = st.sidebar.radio(\"Navigation\", tabs)\n",
        "\n",
        "    if choice==\"Dashboard\":\n",
        "        st.title(\"üìä Loan Dashboard\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if df.empty:\n",
        "            st.warning(\"No data available. Please upload datasets.\")\n",
        "        else:\n",
        "            kpis = analytics.compute_topline(df)\n",
        "            c1,c2,c3,c4 = st.columns(4)\n",
        "            c1.metric(\"Total Loans\", f\"{kpis['total_loans']:,}\")\n",
        "            c2.metric(\"Clients\", f\"{kpis['total_clients']:,}\")\n",
        "            c3.metric(\"Default Rate\", f\"{kpis['default_rate']*100:.1f}%\")\n",
        "            c4.metric(\"Female Share\", f\"{kpis['female_share']*100:.1f}%\" if kpis['female_share'] else \"-\")\n",
        "\n",
        "            st.subheader(\"Visuals\")\n",
        "            c1,c2,c3 = st.columns(3)\n",
        "            with c1: st.image(plot_loan_size_hist(df))\n",
        "            with c2: st.image(plot_defaults_pie(df))\n",
        "            with c3: st.image(plot_time_series_by_month(df))\n",
        "\n",
        "    elif choice==\"Uploads\":\n",
        "        st.title(\"üì§ Upload Dataset\")\n",
        "        f = st.file_uploader(\"Upload CSV\", type=[\"csv\"])\n",
        "        if f is not None:\n",
        "            upath = BASE/\"data\"/\"uploads\"/f.name\n",
        "            pd.read_csv(f).to_csv(upath, index=False)\n",
        "            admin_tools.audit_log(\"upload\", user=st.session_state.user, meta={\"file\":str(upath)})\n",
        "            st.success(f\"Uploaded to {upath}\")\n",
        "\n",
        "    elif choice==\"Reports\":\n",
        "        st.title(\"üìë Client Reports\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if df.empty:\n",
        "            st.warning(\"No data available.\")\n",
        "        else:\n",
        "            cid = st.text_input(\"Enter Client ID\")\n",
        "            if st.button(\"Generate Report\", use_container_width=True):\n",
        "                rep = reports.client_report(df, cid, model_path=None)\n",
        "                st.json(rep)\n",
        "            if st.checkbox(\"Export All Client Reports\"):\n",
        "                outp = reports.export_client_reports(df, model_path=None)\n",
        "                st.success(f\"Exported to {outp}\")\n",
        "\n",
        "    elif choice==\"Admin Sandbox\" and st.session_state.role==\"admin\":\n",
        "        st.title(\"üõ†Ô∏è Admin Sandbox\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if st.button(\"Train Baseline Model\", use_container_width=True):\n",
        "            X,y,en = schema.prepare_for_ml(df)\n",
        "            eng = importlib.import_module(\"modules.ml.engine\")\n",
        "            res = eng.train_baseline(X,y,name=\"sandbox\")\n",
        "            admin_tools.register_model(\"sandbox\", res['model_path'], res['metrics'])\n",
        "            st.success(f\"Model trained. AUC={res['metrics']['auc']:.3f}\")\n",
        "        if st.button(\"List Models\", use_container_width=True):\n",
        "            st.json(admin_tools.list_models())\n",
        "        if st.button(\"Get Pinned Model\", use_container_width=True):\n",
        "            st.json(admin_tools.get_pinned_model())\n",
        "        if new_link:\n",
        "            if st.button(\"üîÑ Refresh Portal Link\", use_container_width=True):\n",
        "                url = new_link()\n",
        "                st.success(f\"New link: {url}\")\n",
        "\n",
        "    elif choice==\"Global Analytics\" and st.session_state.role==\"admin\":\n",
        "        st.title(\"üåç Global Analytics\")\n",
        "        df = analytics.build_master(enforce_enrich=True)\n",
        "        if df.empty:\n",
        "            st.warning(\"No data.\")\n",
        "        else:\n",
        "            topline = analytics.compute_topline(df)\n",
        "            st.subheader(\"Topline KPIs\")\n",
        "            st.json(topline)\n",
        "            st.subheader(\"Segments by Region\")\n",
        "            segs = analytics.segment_stats(df, by=['region'])\n",
        "            if 'region' in segs:\n",
        "                st.dataframe(segs['region'])\n",
        "            st.subheader(\"Top Clients\")\n",
        "            st.dataframe(analytics.top_clients(df, n=20))\n",
        "\"\"\"\n",
        "\n",
        "(APP_DIR/\"main.py\").write_text(ui_py.strip()+\"\\n\")\n",
        "\n",
        "print(\"Cell 9a patched: streamlit_app/main.py ‚úÖ (Admin can refresh portal link inside UI)\")"
      ],
      "metadata": {
        "id": "gLLrH5_Aughf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- HARDENED PRE-RUNNER (add before your existing runner) ---\n",
        "import os, sys, time, subprocess, zipfile, signal, psutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "MODULES = BASE / \"modules\"\n",
        "APP_DIR = MODULES / \"streamlit_app\"\n",
        "DATA_UPLOADS = BASE / \"data\" / \"uploads\"\n",
        "BACKUP_DIR = Path(\"/content\")\n",
        "LOG_FILE = Path(\"/content/streamlit.log\")\n",
        "URL_FILE = Path(\"/content/LAST_PUBLIC_URL.txt\")\n",
        "\n",
        "# 1. Ensure dirs exist\n",
        "for d in [BASE, MODULES, APP_DIR, DATA_UPLOADS]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Kill stale processes (Streamlit/ngrok)\n",
        "import psutil\n",
        "for proc in psutil.process_iter(attrs=[\"pid\",\"name\",\"cmdline\"]):\n",
        "    try:\n",
        "        if any(\"streamlit\" in c for c in proc.info[\"cmdline\"]) or \"ngrok\" in proc.info[\"name\"].lower():\n",
        "            print(f\"‚ö†Ô∏è Killing stale process {proc.info}\")\n",
        "            proc.kill()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# 3. Backup rotation (keep last 3)\n",
        "def rotate_backups():\n",
        "    backups = sorted(BACKUP_DIR.glob(\"LoanIQAppBackup-*.zip\"))\n",
        "    while len(backups) >= 3:\n",
        "        old = backups.pop(0)\n",
        "        old.unlink(missing_ok=True)\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    new_backup = BACKUP_DIR / f\"LoanIQAppBackup-{ts}.zip\"\n",
        "    with zipfile.ZipFile(new_backup, \"w\") as z:\n",
        "        for folder in [MODULES]:\n",
        "            for root, _, files in os.walk(folder):\n",
        "                for f in files:\n",
        "                    fp = Path(root) / f\n",
        "                    z.write(fp, fp.relative_to(BASE))\n",
        "    print(f\"üì¶ Rotated backup saved: {new_backup}\")\n",
        "\n",
        "if APP_DIR.exists():\n",
        "    rotate_backups()\n",
        "\n",
        "# 4. Auto-patch sys.path so 'modules' is always found\n",
        "if str(BASE) not in sys.path:\n",
        "    sys.path.insert(0, str(BASE))\n",
        "print(\"‚úÖ sys.path patched for modules import\")\n",
        "\n",
        "# 5. Ensure Admin user exists\n",
        "from modules import auth\n",
        "auth.init_db()\n",
        "print(\"‚úÖ Admin user ensured in DB\")\n",
        "\n",
        "# 6. Save timestamp\n",
        "print(f\"‚è∞ Startup time: {datetime.utcnow().isoformat()}\")\n",
        "\n",
        "# 7. Prepare log file\n",
        "if LOG_FILE.exists():\n",
        "    LOG_FILE.unlink()\n",
        "open(LOG_FILE, \"w\").close()\n",
        "print(f\"üìù Logs will be written to {LOG_FILE}\")\n",
        "\n",
        "# 8. Add ngrok retry wrapper (imported later by runner)\n",
        "from pyngrok import ngrok\n",
        "def safe_connect(port=8501, retries=2):\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            tunnel = ngrok.connect(port)\n",
        "            url = tunnel.public_url\n",
        "            with open(URL_FILE, \"w\") as f:\n",
        "                f.write(url)\n",
        "            print(f\"üåç Ngrok tunnel established: {url}\")\n",
        "            return tunnel\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Ngrok failed (attempt {i+1}): {e}\")\n",
        "            time.sleep(3)\n",
        "            try: ngrok.kill()\n",
        "            except: pass\n",
        "    raise RuntimeError(\"Ngrok failed after retries.\")\n",
        "\n",
        "print(\"‚úÖ Hardened pre-run checks complete.\")"
      ],
      "metadata": {
        "id": "aFd_qj5uI_ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Repo Doctor: Deep Inspect + Auto-Fix ---\n",
        "import os, sys, sqlite3, subprocess, signal\n",
        "from pathlib import Path\n",
        "import importlib.util\n",
        "\n",
        "FIX = True  # set True to auto-fix DBs and kill stale processes\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "print(f\"‚úÖ Base exists: {BASE}\")\n",
        "\n",
        "# 1. Ensure BASE is on sys.path\n",
        "if str(BASE) not in sys.path:\n",
        "    sys.path.insert(0, str(BASE))\n",
        "print(\"‚úÖ BASE already in sys.path\")\n",
        "\n",
        "# 2. Critical modules check\n",
        "critical = [\n",
        "    \"modules.auth\",\n",
        "    \"modules.schema\",\n",
        "    \"modules.synth.generator\",\n",
        "    \"modules.ml.engine\",\n",
        "    \"modules.ml.utils\",\n",
        "    \"modules.ml.audit\",\n",
        "]\n",
        "for mod in critical:\n",
        "    try:\n",
        "        importlib.import_module(mod)\n",
        "        print(f\"‚úÖ Import OK: {mod}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Import FAIL: {mod} -> {e}\")\n",
        "\n",
        "# 3. DB checks + auto-repair\n",
        "def ensure_table(db_path, ddl, check_sql=None, seed_sql=None, seed_params=None):\n",
        "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    if check_sql:\n",
        "        try:\n",
        "            cur.execute(check_sql)\n",
        "            cur.fetchone()\n",
        "            print(f\"‚úÖ Table OK in {db_path}\")\n",
        "            conn.close()\n",
        "            return\n",
        "        except Exception:\n",
        "            print(f\"‚ö†Ô∏è Table missing in {db_path}, repairing...\")\n",
        "    cur.execute(ddl)\n",
        "    if seed_sql:\n",
        "        cur.execute(seed_sql, seed_params or ())\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"‚úÖ Table repaired in {db_path}\")\n",
        "\n",
        "if FIX:\n",
        "    # Users DB\n",
        "    ensure_table(\n",
        "        str(BASE/\"data\"/\"users.db\"),\n",
        "        \"\"\"CREATE TABLE IF NOT EXISTS users (\n",
        "            username TEXT PRIMARY KEY,\n",
        "            password TEXT,\n",
        "            role TEXT\n",
        "        )\"\"\",\n",
        "        check_sql=\"SELECT * FROM users LIMIT 1\",\n",
        "        seed_sql=\"INSERT OR IGNORE INTO users VALUES (?,?,?)\",\n",
        "        seed_params=(\"admin\",\"admin123\",\"admin\")\n",
        "    )\n",
        "    # Audit DB\n",
        "    ensure_table(\n",
        "        str(BASE/\"data\"/\"audit.db\"),\n",
        "        \"\"\"CREATE TABLE IF NOT EXISTS audit_log (\n",
        "            ts TEXT,\n",
        "            user TEXT,\n",
        "            action TEXT,\n",
        "            details TEXT\n",
        "        )\"\"\",\n",
        "        check_sql=\"SELECT * FROM audit_log LIMIT 1\"\n",
        "    )\n",
        "\n",
        "# 4. Kill stale processes\n",
        "if FIX:\n",
        "    procs = [\"streamlit\", \"ngrok\", \"lt\"]\n",
        "    for p in procs:\n",
        "        try:\n",
        "            subprocess.run([\"pkill\",\"-f\",p], check=False)\n",
        "            print(f\"‚úÖ Killed stale {p} processes (if any).\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# 5. Streamlit app presence\n",
        "app_path = BASE/\"modules\"/\"streamlit_app\"/\"app.py\"\n",
        "if app_path.exists():\n",
        "    try:\n",
        "        compile(app_path.read_text(), str(app_path), \"exec\")\n",
        "        print(f\"‚úÖ Streamlit app found & compiles: {app_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Streamlit app compile error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå Streamlit app missing\")\n",
        "\n",
        "print(\"üîç Repo Doctor finished. Ready to run your main runner.\")"
      ],
      "metadata": {
        "id": "sMc476KSXOmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- FINAL RUNNER WITH AUTO-BACKUP, AUTO-RESTORE & DB AUTO-REPAIR (no tunnel password) ---\n",
        "import os, sys, time, subprocess, zipfile, signal\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Paths\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "APP_PATH = BASE / \"modules\" / \"streamlit_app\" / \"app.py\"\n",
        "HELPERS_PATH = BASE / \"setup_helpers.py\"\n",
        "BACKUP = Path(\"/content/LoanIQAppBackup.zip\")\n",
        "BASE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Install exact versions\n",
        "!pip install pyngrok==5.2.1 streamlit --quiet\n",
        "\n",
        "# 3. Kill stale processes\n",
        "import psutil\n",
        "for proc in psutil.process_iter(attrs=[\"pid\",\"name\",\"cmdline\"]):\n",
        "    try:\n",
        "        cmd = \" \".join(proc.info[\"cmdline\"]) if proc.info[\"cmdline\"] else \"\"\n",
        "        if \"streamlit\" in cmd or \"ngrok\" in cmd or \"lt\" in cmd:\n",
        "            os.kill(proc.info[\"pid\"], signal.SIGKILL)\n",
        "            print(f\"‚ö†Ô∏è Killed stale process {proc.info['pid']} ({cmd})\")\n",
        "    except Exception:\n",
        "        pass\n",
        "# 3b. Kill broken ngrok configs (they cause \"tunnel not found\" errors)\n",
        "for bad in [\"/root/.config/ngrok/ngrok.yml\", \"/root/.ngrok2/ngrok.yml\"]:\n",
        "    try:\n",
        "        if os.path.exists(bad):\n",
        "            os.remove(bad)\n",
        "            print(f\"‚ö†Ô∏è Removed old ngrok config at {bad}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not remove {bad}: {e}\")\n",
        "\n",
        "# Also tell ngrok to ignore configs completely\n",
        "os.environ[\"NGROK_CONFIG\"] = \"null\"\n",
        "\n",
        "# 4. Always rewrite helpers file fresh\n",
        "helpers_code = r\"\"\"\n",
        "import os, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/loan_app\")\n",
        "\n",
        "def restore_modules():\n",
        "    modules_dir = BASE / \"modules\"\n",
        "    if modules_dir.exists() and any(modules_dir.glob(\"**/*.py\")):\n",
        "        print(\"‚úÖ Modules already exist.\")\n",
        "        return True\n",
        "    archive = Path(\"/content/LoanIQAppBackup.zip\")\n",
        "    if archive.exists():\n",
        "        with zipfile.ZipFile(archive,'r') as z: z.extractall(\"/content\")\n",
        "        print(\"‚úÖ Restored modules from backup.\")\n",
        "        return True\n",
        "    print(\"‚ùå No modules or backup found.\")\n",
        "    return False\n",
        "\n",
        "def restore_app():\n",
        "    app_path = BASE/\"modules\"/\"streamlit_app\"/\"app.py\"\n",
        "    if app_path.exists():\n",
        "        print(\"‚úÖ app.py exists.\")\n",
        "        return True\n",
        "    archive = Path(\"/content/LoanIQAppBackup.zip\")\n",
        "    if archive.exists():\n",
        "        with zipfile.ZipFile(archive,'r') as z: z.extractall(\"/content\")\n",
        "        print(\"‚úÖ Restored app.py from backup.\")\n",
        "        return app_path.exists()\n",
        "    print(\"‚ùå app.py missing and no backup found.\")\n",
        "    return False\n",
        "\"\"\"\n",
        "HELPERS_PATH.write_text(helpers_code)\n",
        "print(f\"‚úÖ setup_helpers.py refreshed at {HELPERS_PATH}\")\n",
        "\n",
        "# 5. Import helpers\n",
        "sys.path.insert(0, str(BASE))\n",
        "import setup_helpers\n",
        "\n",
        "# 5b. Ensure database schema exists (auto-repair)\n",
        "import sqlite3\n",
        "\n",
        "def ensure_db(path, schema_sql):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    conn = sqlite3.connect(path)\n",
        "    cur = conn.cursor()\n",
        "    for stmt in schema_sql:\n",
        "        cur.execute(stmt)\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"‚úÖ DB checked: {path}\")\n",
        "\n",
        "# users.db\n",
        "ensure_db(str(BASE/\"data\"/\"users.db\"), [\n",
        "    \"\"\"CREATE TABLE IF NOT EXISTS users (\n",
        "        username TEXT PRIMARY KEY,\n",
        "        password TEXT,\n",
        "        role TEXT,\n",
        "        created_at TEXT\n",
        "    )\"\"\"\n",
        "])\n",
        "\n",
        "# audit.db\n",
        "ensure_db(str(BASE/\"data\"/\"audit.db\"), [\n",
        "    \"\"\"CREATE TABLE IF NOT EXISTS audit_log (\n",
        "        ts TEXT,\n",
        "        user TEXT,\n",
        "        action TEXT,\n",
        "        details TEXT\n",
        "    )\"\"\"\n",
        "])\n",
        "\n",
        "# loans.db\n",
        "ensure_db(str(BASE/\"data\"/\"loans.db\"), [\n",
        "    \"\"\"CREATE TABLE IF NOT EXISTS clients (\n",
        "        client_id TEXT PRIMARY KEY,\n",
        "        name TEXT,\n",
        "        gender TEXT,\n",
        "        region TEXT,\n",
        "        age INTEGER,\n",
        "        joined_on TEXT\n",
        "    )\"\"\",\n",
        "    \"\"\"CREATE TABLE IF NOT EXISTS loans (\n",
        "        loan_id TEXT PRIMARY KEY,\n",
        "        client_id TEXT,\n",
        "        amount REAL,\n",
        "        status TEXT,\n",
        "        issued_on TEXT,\n",
        "        due_on TEXT,\n",
        "        FOREIGN KEY(client_id) REFERENCES clients(client_id)\n",
        "    )\"\"\",\n",
        "    \"\"\"CREATE TABLE IF NOT EXISTS models (\n",
        "        name TEXT PRIMARY KEY,\n",
        "        model_path TEXT,\n",
        "        auc REAL,\n",
        "        trained_on TEXT\n",
        "    )\"\"\"\n",
        "])\n",
        "\n",
        "# Seed default admin if missing\n",
        "conn = sqlite3.connect(str(BASE/\"data\"/\"users.db\"))\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"PRAGMA table_info(users)\")\n",
        "cols = [row[1] for row in cur.fetchall()]  # get column names\n",
        "\n",
        "cur.execute(\"SELECT 1 FROM users WHERE username='admin'\")\n",
        "if not cur.fetchone():\n",
        "    placeholders = \",\".join([\"?\"] * len(cols))\n",
        "    values = []\n",
        "    for c in cols:\n",
        "        if c == \"username\":\n",
        "            values.append(\"admin\")\n",
        "        elif c == \"password\":\n",
        "            values.append(\"admin123\")\n",
        "        elif c == \"role\":\n",
        "            values.append(\"admin\")\n",
        "        else:\n",
        "            values.append(None)  # default for any extra column\n",
        "    cur.execute(f\"INSERT INTO users ({','.join(cols)}) VALUES ({placeholders})\", tuple(values))\n",
        "    print(f\"‚úÖ Default admin user created with columns {cols}\")\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "# 6. Restore modules & app if needed\n",
        "ok_mods = setup_helpers.restore_modules()\n",
        "ok_app  = setup_helpers.restore_app()\n",
        "\n",
        "# 7. AUTO-BACKUP current state\n",
        "if APP_PATH.exists():\n",
        "    with zipfile.ZipFile(BACKUP, \"w\") as z:\n",
        "        for folder in [BASE / \"modules\"]:\n",
        "            for root, _, files in os.walk(folder):\n",
        "                for f in files:\n",
        "                    fp = Path(root) / f\n",
        "                    z.write(fp, fp.relative_to(BASE))\n",
        "    print(f\"üì¶ Backup updated at {BACKUP}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping backup: app.py not found yet.\")\n",
        "\n",
        "# Replace the pyngrok section with this:\n",
        "\n",
        "print(\"üöÄ Launching Streamlit app...\")\n",
        "streamlit_proc = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", str(APP_PATH), \"--server.port=8501\", \"--server.headless=true\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        ")\n",
        "\n",
        "# Give streamlit a few seconds to boot\n",
        "time.sleep(6)\n",
        "\n",
        "print(\"üåç Creating localtunnel...\")\n",
        "lt_proc = subprocess.Popen(\n",
        "    [\"/tools/node/bin/lt\", \"--port\", \"8501\", \"--subdomain\", \"loaniqdemo\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True,\n",
        ")\n",
        "# Read the tunnel URL from lt stdout\n",
        "for line in lt_proc.stdout:\n",
        "    if \"your url is:\" in line.lower():\n",
        "        url = line.split()[-1].strip()\n",
        "        print(f\"üåç Your app is live at: {url}\\n\")\n",
        "        break"
      ],
      "metadata": {
        "id": "bJFktkq7YOgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}